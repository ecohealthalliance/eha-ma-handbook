[["index.html", "EHA Modeling &amp; Analytics Handbook Introduction", " EHA Modeling &amp; Analytics Handbook “These aren’t rules, just some things that we figured out.” – Michael Reno Harrel Last edit 2022-01-31 by GitHub Introduction This handbook describes best practices and guidelines for project management, organization, modeling and and programming we aim for our on the EHA Modeling &amp; Analytics team. This is a living document. To make changes, just click the edit button () at the top of the page. It will take you to the source editor for the chapter on GitHub, where you can make edits and submit your changes. Be sure to commit major contributions to a new branch and open a pull request. In general, we aim to produce sound analyses that are: Self-contained Well-documented and communicated Easily reproducible Sharable Version controlled Together, these attributes help assure that our work is correct, can be built off of and extended, meets requirements for sharing and publication, and can be continued through staff turnover. The tools we use to accomplish this are mostly, but not exclusively, based around the R programming language and git version control. Other teams at EHA use other tools (e.g., the technology team mostly uses Python and Javascript, much of our work with partners is MS Office based). The guidelines in the document represent an ideal we aim for but to not always attain. Remember: Best practices are always evolving. Don’t let the perfect be the enemy of the good. Other teams and external partners have different workflows and we adjust as neccessary to collaborate. Our goal is to do good science to advance conservation and human health, not be slick programmers. You can find some slides from a previous presentation on this topic here. The philosophy and guidelines in this document owe an enormous amount to the work of the Software and Data Carpentry and rOpenSci organizations, and the work of Hadley Wickham and Jenny Bryan. You’ll find many links to their work in this handbook. "],["1-Contributing.html", "1 Contributing 1.1 Anatomy of a chapter 1.2 Workflow Summary 1.3 Git based workflows 1.4 Working with rmarkdown in the bookdown framework 1.5 Review 1.6 Modifying chapters 1.7 Additional resources", " 1 Contributing How do I add or modify a chapter in the EHA M&amp;A Handbook? Identify a gap in the handbook, create a branch, write the chapter and then create a pull request. This handbook is created using the bookdown package. Bookdown allows you to publish a set of Rmarkdown documents into a book. Each document constitutes a chapter in the book. This chapter assumes an introductory level understanding of git. See the chapter on git to learn more. 1.1 Anatomy of a chapter Motivation - What question is the chapter trying to answer? Summary - Short paragraph on the topic and/or bulleted list of workflow Main Content - Longer form answer to the question Additional Resources - Where can someone go to learn more about the topic? 1.2 Workflow Summary 1.2.1 For quick edits Click the edit button () at the top of the page Modify the chapter Commit the changes to master These changes will be immediately reflected in the handbook!. If you would like someone else to review them, commit the changes to a branch then open a pull request. 1.2.2 Adding a chapter Clone the repo Create a branch off of main/master for your chapter Start a new Rmarkdown document with rmarkdown::draft(\"my-chapter.Rmd\",template = \"EHA-MA-Chapter\",package = \"ehastyle\") Add content to RMD Add chapter to _bookdown.yml Preview chapter with bookdown::preview_chapter(\"my-chapter.Rmd\") Create a pull request into main/master Revise and resubmit 1.2.3 Major edits Clone the repo Create a branch off of main/master for your edits Edit Rmarkdown document Preview chapter with bookdown::preview_chapter(\"my-chapter.Rmd\") Create a pull request into main/master Revise and resubmit 1.3 Git based workflows This handbook is generated via github actions. Whenever a change is made to the master branch, the book is re-compiled and published to this webpage. Because of that, its important that we are creating new chapters using branches in git as any changes to master will be added before review. To create a branch use git checkout -b feature/my-chapter then use git push -u origin feature/my-chapter to track it on github. This will create a branch off of master and push it to the repository. 1.4 Working with rmarkdown in the bookdown framework The format of bookdown chapters and the workflow for previewing them is a little different from typical rmarkdown documents. Unlike typical rmarkdown documents, bookdown chapters do not start with a yaml section. Also, the file name for the chapter must be added to the _bookdown.yaml for it to knit as expected. Otherwise, the workflow is more or less the same. The ehaStyle package has a template for handbook chapters. remotes::install_github(&quot;ecohealthalliance/ehastyle&quot;) # if you are installing the package you may need to close and reopen # Rstudio to be able to access this template rmarkdown::draft(&quot;my-chapter.Rmd&quot;,template = &quot;EHA-MA-Chapter&quot;,package = &quot;ehastyle&quot;) This will generate a markdown document with a rough outline of a chapter structure and an overview of the workflow. After you have created the rmd, you can add it to the _bookdown.yaml. The chapters are rendered in order based on the _bookdown.yaml file so try to place your chapter near related content. You can now use bookdown::preview_chapter(\"my-chapter.Rmd\") to make sure that your chapter has rendered properly. By specifying your chapter, you avoid rendering the entire book. 1.5 Review When you’re happy with your chapter, open up a pull request and assign a reviewer. The reviewer will read the chapter and check that the contents are in-line with the overall context of the handbook and sufficient to get someone started on the topic. When your pull request is accepted and merged into master, the github actions workflow will kick in and rebuild the book with your new material. 1.6 Modifying chapters If you are making more than minor edits to the handbook, it is best to follow a branch based workflow as described above. When editing, you do not need to create a new rmd file or add the file name to _bookdown.yaml. 1.7 Additional resources Bookdown - Documentation for bookdown Rmarkdown: The definitive guide - Higher level overview of bookdown ROpenSci Bookdown materials - articles from ROpenSci regarding bookdown Bookdown cheatsheet - Quick reference from happy git with R for bookdown Happy Git With R: Branches - Overview of how to work with branches in GIT Git strategies - an overview of trunk based git workflows "],["2-quickstart-for-computing.html", "2 Quickstart for computing 2.1 Optional", " 2 Quickstart for computing In addition to the standard computer setup provided by office admin (Email, MS Office, etc), M&amp;A members should do the following: Download and install Slack. If you don’t alread have an invite, get one from Noam. Join the #data-sci-discuss and #journal-club channels any others your supervisor suggests or are associated with projects you are assigned to. (Don’t forget #random and #out-of-office-fun!) If you don’t already have one, sign up for an account on GitHub. Provide Noam with your username so as to add you to the EcoHealth Alliance team. If you already have a GitHub account using a personal email, add your EHA email to your account and set up custom routing to direct EHA organization notifications to your EHA email Install R and then RStudio. Links and instructions can be found here Install git and link it to RStudio and your GitHub account. Instructions for all of this are found here, in Sections 7-15 of Happy Git with R. If you get stuck along the way don’t hesitate to ask for help (via the #data-sci-discuss channel on Slack!). Install Dropbox on your computer with your EHA account (note you can have separate personal and EHA Dropbox folders) Check that your EHA email gives you access to Google Drive. If you prefer it, or your supervisor specifies it, install it locally on your computer 2.1 Optional In addition, the following programs may be helpful to install (all for Mac users): Homebrew for general package management iTerm2 for a better shell interface. (brew cask install iterm2 or brew install --cask iterm2 using Homebrew in the shell) mosh for connecting to our high-performance servers (brew install mosh using Homebrew in the shell) hub for interacting with GitHub from the shell (brew install hub) "],["3-projects.html", "3 Project Management 3.1 Teams and Work Cycles 3.2 Setup and materials Organization", " 3 Project Management 3.1 Teams and Work Cycles At EHA projects are typically months-to-years-long workstreams centered around a main analytical or research problem. A program, grant or contract may have multiple projects, and a project may have multiple outputs such as reports, scientific publications, or apps. A project typically has a small (2-5 person team) with a project lead and possibly a project administrative point of contact (APOC). We organize projects into work cycles of 4-8 weeks. For each cycle, a team should define day-to-week scale tasks, assign tasks to members, determine the percentage of time team members will put towards the project in that cycle, given other workloads, and plan travel, reporting, collaboration, or other deadlines. Teams report out their progress at the end (and start) of each work cycle at our weekly M&amp;A meetings. Report-outs should include Progress on tasks assigned and completed in previous cycle Substantive report-out of results and products Draft plan for tasks and goals for the coming cycle Team assignments for the next cycle and level of involvement (high (&gt;50%), medium (25%-50%, low (&lt;25%)) of team members over the cycle. Any additional deadlines or reporting anticipated in that time frame, including plans for other internal presentations or feedback sessions. During report-outs, the M&amp;A group will provide feedback for the upcoming cycle and set a date for the next one. Teams track work cycle progress through various mechanisms based on team preferences. One option is GitHub Milestones (Example). Others use Google Spreadsheets, Air tables, or other systems. Teams may choose what they prefer as long as their system Shows current tasks, deadlines, and assignments Tracks past tasks, deadlines, and assignments Includes top-level summaries for a reporting period Is available in “real time” online rather than stored on individual machines and e-mailed Can be made accessible to other staff via a URL but kept private within EHA note: historically teams had used Asana 3.2 Setup and materials Organization An M&amp;A project lasting more than one work cycle should typically have a Slack channel for communication, a GitHub repo for data and analysis code, or Dropbox or Google Drive folder for documents or materials not appropriate for git-based version control. In addition, it may have a Paperpile folder for references. In general, one URL (often the GitHub README) should be the starting point from which one can reach all project materials. 3.2.1 Code organization In general, one should aim to set up the analysis portion of a project in a self-contained way, with clear separation between raw data, processed data, exploratory analyses, and final products. In organizing a project folder, ask If I copied this whole folder onto someone else’s computer, could they pick up the project? Are the folder organization and file naming clear? There are some exceptions for large data sets or rapidly changing data sets. In these cases, data can be organized as a separate folder or project, and large data sets can be stored in an Amazon Web Services S3 bucket. In many cases it is actually best for data to be organized as a separate resource from analysis. This allows multiple analysis projects to rely on the same upstream data project, avoiding multiple versions of data. Data may also not be best stored in a git repository but in a project database to be pulled for analyses. See EHA guidance on setting up data resources for a project here 3.2.1.1 RStudio Projects In general we also prefer that R analyses be set up as RStudio projects. Here’s a Software Carpentry Lesson on RStudio projects 3.2.1.2 targets We recommend that analysis projects be set up using the targets framework to define steps in the code. targets is a package for defining R project workflow and tracks your functions and objects to ensure everything is up-to-date. Here are some resources for getting started with targets An internal presentation on using targets at EHA: (Video (via password at internal AirTable), Slides, ster Example Repository on GitHub) The targets manual, including a walkthrough A good intro talk(~90 mins with questions) by Miles McBain on using targets and getting a good workflow A great introductory video series (5 30-minute lessons as a YouTubePlaylist) 3.2.1.3 Package management with renv We strongly recommend that projects use the renv package to manage versions of R packages so that the project does not break when packages update or when run on machines that have different package versions installed. The renv getting started guide tells you most of what you need to know. Here is a short presentation on using renv at EHA: (Video (via password at internal AirTable), Slides, Example Repository on GitHub) "],["4-r.html", "4 R and Reproducible Analysis 4.1 Install 4.2 Learn 4.3 Additional Resources", " 4 R and Reproducible Analysis Can everything be re-done easily if I change one data point in the inputs? At EHA R is our primary, though not exclusive, tool for analysis and modeling work. R is not just a piece of software for statistics and data manipulation but a computer language, meaning that our analyses are scripted. This means they thus can be automated, run again, built upon and extended. 4.1 Install R itself RStudio, the leading R development environment. 4.2 Learn Learning R is beyond the scope of this document, and you likely already have some experience in it, but some good starting points are: The Software Carpentry Lessons Swirl, a set of interactive lessons run right in R. The JHU Coursera Series R for Data Science by Hadley Wickam is a beginner/intermediate text that we highly recommend for getting up to speed with the particular workflows we recommend and the most recent packages that support them. Advanced R (Wickham) is very good for understanding how the language works. Efficient R by Colin Gillespie and Robin Lovelace is helpful for imporving workflows and speeding up code. R Packages (Wickham) is good for package development. Cheatsheets from RStudio are a useful references for a number of things. DataCamp courses are also potentially useful. If you feel they would match your learning style and needs, discuss EHA purchasing a subscription for you with your supervisor. These resources are largely about the mechanics of programming in R, rather than using it for statistical analyses. This is a far larger subject, but see the Statistical Methods section for a jumping-off point. 4.3 Additional Resources Trouble shooting your code: Getting Help. User groups/communities of practice: R Meetups Specific domains: Training Plans "],["5-eha-team-communication.html", "5 EHA Team Communication 5.1 Install", " 5 EHA Team Communication How do we work together and keep a useful record of our interactions? Slack is our office chat tool and is good for day-to-day communication. Slack does not have to be an instant communication tool - some people prefer to check it a few times a day. Check with your supervisor about your project/team preferences. Slack’s main purpose is to organize our communication by channels specific to a topic or project. It is good for keeping information from one project together in a way that can be referenced later by new team members, rather than being lost in various e-mail inboxes. A channel can be linked to many other tools (Dropbox/Google Drive Folder, GitHub Repository), so as to have a central hub for project management. E-mails can be forwarded to a channel. Slack also has voice-calling and, critically, screen-sharing capabilities that are useful for pair-debugging while programming. GitHub (see below) has a good issue-tracking system that accompanies each project and can be used for task management and general communication. This ties messages to a specific project and keeps a good long-term record, and can be connected to a slack channel or integrated with e-mail Remember that your Slack and GitHub communications are part of your project and are likely to be seen by both internal and external collaborators. 5.1 Install Download and install Slack. If you don’t alread have an invite, get one from Noam. Join the #data-sci-discuss and #journal-club channels and any others your supervisor suggests or are associated with projects you are assigned to. There’s also a mobile Slack app for iOS and Android, which may be helpful if you are traveling. "],["6-documentation-and-outputs.html", "6 Documentation and Outputs 6.1 Learn 6.2 Install 6.3 Library", " 6 Documentation and Outputs Will someone understand this thing when I hand it over? Documentation is essential to collaboration and continuity for our work. Your project should contain documentation to allow a project to be picked up by another user. Documentation includes the following: A README document in the top level of the project folder with a high-level explanation of project organization, purpose, and and contact info. Metadata for your data set showing its origin and the type of data in each field in a table. Comments in your computer code Written descriptions of your analyses. The primary medium for this should be R Markdown documents, which allow you to combine code, results, and descriptive text in an easy to update and modify form. Shorter ephermal results can be posted as plots to your project Slack rooms. 6.1 Learn R Markdown is pretty straightforward to learn. You can create your first document and get the basics by going to File &gt; New File &gt; R Markdown in the RStudio menu. When you have time, dive in a bit more with this great lesson on it with accompanying video. Here’s an RStudio Reference Sheet for R Markdown. 6.2 Install (Very optional unless you are asked): ehastyle is our internal EHA R package with R Markdown templates for some reports we produce. 6.3 Library Once your project has been published, you should include the output in our EHA Library. Simply submit through this google form. The library database, along with full texts of both media and science publications, is stored on DropBox. The purpose of the EHA Library is to store our work in a searchable, systematic database. A centralized location of EHA work is of use to our science and development teams. Additionaly, standardized analyses (stored in the eha-library github repository) create easy reporting to our board, our funding bodies, and the public. We have found this database particularly useful in the on-boarding of new staff. The current scope of the Library includes: Scientific publications Conference presentations Layman publications by EHA staff EHA-mentored academic work Media articles that highlight our work or our scientists "],["7-data-management.html", "7 Data Management 7.1 Learn 7.2 Install", " 7 Data Management Can the data be shared and published, and easily re-used in other analyses? Store data in simple, cross-compatible formats such as CSV files. Microsoft Excel can be a useful tool for data entry and organization, but limit its use to that, and organize your data in a way that can be easily exported. Metadata! Metadata! Document your data. For relational datasets you can create linked data on Airtable For data sets that cross multiple projects, create data-only project folders for the master version. When these data sets are finalized, they can be deposited in public or private data repositories such as figshare and zenodo. In some cases it makes sense for us to create data-only R packages for easily distributing data internally and externally. We aim to generally work in a tidy data framework. This approach to structuring data makes iteroperability between tools easier. 7.1 Learn Read Hadley Wickham’s tidy data paper for the general concept. Note the packages in this paper are out of date, but the structures and concepts apply. R For Data Science is a great online book to read and reference for working in this framework, and gives guidance for the most up-to-date packages (tidyr being the latest analogue of reshape and reshape2). Data Carpentry has a Lesson on spreadsheet organization for when you need to do some work in Excel but make it compatible with R. Nine simple ways to make it easier to (re)use your data rounds some things out in terms of data sharing. This post is nice, too. 7.2 Install Get the tidyverse package for R using install.packages(\"tidyverse\"). This will install several other relevant packages. "],["8-versioning.html", "8 Version Control, Git and Github 8.1 Learn 8.2 Install", " 8 Version Control, Git and Github Can I go back to before I made that mistake? Can others see changes others have made to the project and can I see theirs? Version control is essential to long-term project management and collaboration. We primarily use git for this - we recommend it for any project with more than one file of code. It has a steep learning curve but is very powerful. GitHub is a web service for sharing git-versioned projects that has many great tools for collaboration. We have an organizational GitHub account so we can have private repositories and work in teams with shared projects. For projects with little code-based work, there are other options, as well: Google Docs/Word Track Changes are limited to single documents Dropbox can track all files in a shared project/folder Allows one to view/revert to any previous version of a file in the folder Easily sharable Does not travel well - history is lost when project moves elsewhere File histories are independent - does not track interrelated changes. Avoid filename-based version control: 8.1 Learn Git has a steep learning curve and we recommend you spend some time learning rather than only trying to pick it up as you go along. Work through the online book Happy Git with R, which will help you get git and GitHub setup and working with R and RStudio, and teach you some basic workflows. Take the Data Camp course Introduction to Git for Data Science (free) to reinforce some key concepts and learn how work with versions on a day-to-day basis. Here is a useful git cheat sheet 8.2 Install Go through the installation steps Happy Git with R’s “Installation” and “Connect” chapters and Appendix B Note when setting up your GitHub account that one account can have multiple e-mail addresses associated with it, so you can split your work and personal stuff without needing multiple accounts (see here). Give Noam (Modeling &amp; Analytics) or Robert Young (Tech), your GitHub username so they can make you a member of the organizational EHA account and be given access to the appropriate teams. If you are using a GitHub account you previously created with another e-mail, be sure to add your EHA e-mail under Email Settings and set “Custom Routing” under your notification settings so that notifications related to the EHA organization go to your EHA e-mail. Install Dropbox on your computer with your EHA account (note you can have separate personal and EHA Dropbox folders) Check that your EHA email gives you access to Google Drive. If you prefer it, or your supervisor specifies it, install it locally on your computer "],["9-reviewing-analyses-and-code.html", "9 Reviewing Analyses and Code 9.1 Learn", " 9 Reviewing Analyses and Code Has my work recieved feedback? Has a second set of eyes checked it for correctness? Have I learned from my colleagues’ work? Just like any piece of writing that you do, your analysis code should be reviewed by a peer or supervisor. There are generally two types of code reviews we engage in: Unit reviews are reviews of discrete, small parts of a project. This might be an analysis that you took a few days or a couple of weeks to complete, and consists of 1-2 files or a few dozen to hundred lines of code. When you complete such a discrete unit, you should solicit feedback. Project reviews are reviews of a whole project as it wraps up, such as prior to the submission of a manuscript. These reviews aim to check that the project is complete, understandable and reproducible. Reviews can be either In person reviews where you go over your code with your team or at our informal science meetings. ScreenHero can also be used for this. Written reviews where a peers place comments in your code or use the commenting and reviewing features on GitHub. or both. 9.1 Learn Check out Fernando Perez’s tips for code review in the lab. Read the Mozilla Guide to Code Review in the Lab Check out some rOpenSci package review examples to look at one kind of code review in action. Best practices for this are evolving. Check out a recent conversation among scientists on Twitter on the topic "],["10-testing.html", "10 Testing 10.1 Learn 10.2 Install", " 10 Testing Is this code doing what I think its doing? Is this data correct? Most code should be accompanied by some form of testing. This scales with the size and type of project. Your work should generally accompanied with testing code or outputs that show that your models behave appropriately, are statisically sound, that your code is running as you expect and your data is checked for quality. 10.1 Learn Test driven data analysis is a neat blog on this subject. There’s a testing chapter in the R Packages book. The vingettes and README files of the packages below are useful. 10.2 Install R packages: assertr or validate for testing that data meets criteria visdat for visually inspecting tabular data. (though there are many ways to plot your data for inspection). testthat for functions and R packages. "],["11-high-performance-servers.html", "11 High-Performance Servers", " 11 High-Performance Servers How can I make this giant beast of a model run faster? EHA maintains internal high-performance servers for Modeling and Analytics work. For security reasons, details on these can now be found in this private repository. "],["12-cloud-computing-services.html", "12 Cloud Computing Services 12.1 Setting up Amazon Credentials 12.2 Using Amazon S3 Storage from R", " 12 Cloud Computing Services What if my data files are too big to hold in Github? Where is the latest IUCN/MODIS/LANDSAT shapefile? What if I need more CPUs than even our servers have? EHA has an organizational Amazon Web Services and Microsoft Azure accounts that may be useful for some projects. AWS S3 storage or Azure Blog Storage are useful for hosting large data files, especially if they are shared across projects. AWS EC2 cloud computers or Azure Virtual Machines may be appropriate for a hosting a web app or database, automating regular processes, or other analytical projects. If you think you need cloud resources for your project, contact Noam or Robert Young for access to the AWS account, to discuss what services or other providers may be useful. If you are using AWS, remember: All resources used on AWS need to be tagged with a project:PROJECTNAME tag in order to assign costs to the appropriate EHA projects. Be judicious with AWS service usage. It is easy to run up costs. 12.1 Setting up Amazon Credentials To connect to AWS via program like R, you will net to set up an access access credientials. Once you’ve gotten your credentials, log on to ehatek.signin.aws.amazon.com/console/. ehatek should be already entered in the “Account ID or alias” field. You will be asked to change your password after the first time you log on. 12.1.1 Generate an access key Navigate to your IAM (Identity and Access Management) page to generate an access key. You should see a ✅ next to “rotate your access keys.” Click “Manage User Access Keys” after expanding the access key subheading. (If you see an ❌, ask whoever created your account if you have the appropriate S3 privileges.) Click the “Security credentials” panel and scroll down to the “create access key” button. Your access key will have a key ID - a long string of letters and numbers - and a *~*secret*~* access key - a longer string of letters and numbers. Press “show” and hang onto it somewhere until you are sure your ~/.aws/credentials file is working as intended. Don’t share your secret key with anyone. If you lose it before you use it, you can delete your and make another one. Deleting a key is different from making a key inactive - you might reach your access key limit pretty quickly, so you’ll probably have to do the former if you have keys that you don’t use. 12.1.2 Create a credentials file Open a blank text file in a text editor and paste your key ID and secret key as follows. It will look like this: [default] aws_access_key_id = your_key_id aws_secret_access_key = your_secret_access_key aws_default_region = us-east-1&quot; Replace your_key_id and your_secret_access_key with the values you see in the AWS browser. Create a folder called ~/.aws, and put the credentials file in it. (YES, currently, you need that trailing \" after us-east-1 if you want to include the AWS_DEFAULT_REGION in your credentials file, because the aws.signature is using a strange parser.) If you get errors related to the AWS default region, you can delete that line and set it locally inside your R environment like this: Sys.setenv(&quot;AWS_DEFAULT_REGION&quot; = &quot;us-east-1&quot;) 12.1.3 Test your Key Open an R session to make sure your credentials file works. # install the AWS packages if you need them if (!require(aws.s3)) devtools::install_github(&quot;cloudyr/aws.s3&quot;) if (!require(aws.signature)) devtools::install_github(&quot;cloudyr/aws.signature&quot;) library(aws.s3) # load credentials from your credentials file aws.signature::use_credentials() # check list of aws S3 buckets head(aws.s3::bucketlist()) If you get a ‘403 Forbidden’ error, your credentials aren’t working. 12.1.4 Alternate credentialing method If you have Done Your Googles and are still having trouble setting your AWS credentials, you can set them in your .Renviron file. The advantage of using an ~/.aws/credentials file is that your key will then work across all platforms - R, Python, the command line, etc. However, if you’re only connecting to AWS through R, having your key in your .Renviron will be enough. # If you don&#39;t know how to find your .Renviron file, do the following to open it: if(!require(usethis)) install.packages(&quot;usethis&quot;) usethis::edit_r_environ() # add the following to your .Renviron, replacing your_key_id and your_secret_access_key with their actual values. Save your .Renviron and restart R. &quot;AWS_ACCESS_KEY_ID&quot; = your_key_id &quot;AWS_SECRET_ACCESS_KEY&quot; = your_secret_access_key &quot;AWS_DEFAULT_REGION&quot; = us-east-1 After you restart R, test that your credentials are working with aws.s3::bucketlist(). 12.2 Using Amazon S3 Storage from R Above, you used bucketlist above to list all the buckets associated with your account. To look at everything inside a specific bucket, use get_bucket. Note that Amazon charges per Gigabyte for downloading and uploading files. Use things as needed for your project, but don’t get crazy and put a file download inside a for loop that you’re running a thousand times a day. To test privacy privileges are working are intended, we’ll use a very small private bucket with ~4 KB in it - Two small subsets of the Social Security Administration’s baby names dataset from the babynames package. # check list of aws S3 buckets bucketlist() # aws.s3::bucketlist() pb &lt;- &quot;eha-ma-practice-bucket&quot; b &lt;- get_bucket(pb) 12.2.1 Save objects from S3 to your machine One of the main reasons we use Amazon S3 is to make sure everyone is working with the same large data files. Usually, you’ll download the large files you need one time and keep them on your local machine for analyses. This can be done with save_object. Wrapping save_object in an if() clause checks to see if the dataset exists first before downloading. # make a data folder if this is the first dataset you&#39;re downloading into this project if(!dir.exists(&quot;data&quot;)) { dir.create(&quot;data&quot;) } # save the babynames_subset.csv object (b[[1]]) into local memory if(!file.exists(&quot;data/babynames_subset.csv&quot;)) { save_object(object = b[[1]], bucket = pb, file = &quot;data/babynames_subset.csv&quot;) } dir(&quot;data&quot;) babynames_subset.csv is now in your data directory.You can then read it into memory as you would normally. 12.2.2 Upload objects from your machine to S3 To put a file from your computer back into the bucket, use put_object. This will be less common than downloading and saving objects from S3 - as a general rule, we want everyone working with the same versions of large datasets. If you’ve made a large rasterfile that your colleagues will use, you can upload it, In this example, we’ll make a change to the babynames_subset and save it as a new file (babynames_subset2). babynames_subset2 &lt;- babynames_subset %&gt;% group_by(name) %&gt;% summarize(n = sum(n)) %&gt;% filter(n &lt; 100) # save to csv write.csv(babynames_subset2, file = &quot;data/babynames_subset2.csv&quot;, row.names = FALSE) # check whether an object with this name is in the bucket, and if it isn&#39;t, put it in there if(!(&quot;babynames_subset2.csv&quot; %in% dplyr::pull(get_bucket_df(pb), &quot;Key&quot;) ) { put_object(file = &quot;data/babynames_subset2.csv&quot;, object = &quot;babynames_subset2.csv&quot;, bucket = pb, acl = &quot;private&quot;) } So now we have a data frame in memory called babynames_subset2 and a .csv in the AWS eha-ma-practice-bucket called babynames_subset2.csv. We’ll pull the AWS csv back into memory to make sure they’re the same. To do this, we have to first copy over babynames_subset2 to a new object in memory and remove it so that when we load the version from AWS, it doesn’t overwrite it. babynames_subset_R &lt;- babynames_subset2 rm(babynames_subset2) # save the .csv object using &quot;save_object&quot; tmp = tempdir() file = paste0(tmp, &quot;/babynames_subset2.csv&quot;) save_object(&quot;babynames_subset2.csv&quot;, bucket = pb, file = file) # read the csv file into memory babynames_subset2 &lt;- read_csv(file) Finally, we can check that the object in memory and the .csv in AWS are the same. assertthat::are_equal(babynames_subset_R, babynames_subset2) "],["13-google-authorization-and-r.html", "13 Google Authorization and R 13.1 The basic overview: 13.2 Key Terms 13.3 Before we start: 13.4 Setup encryption tools on your machine 13.5 Enable git-crypt on your repository 13.6 Setting up non-interactive authentication for Google sheets 13.7 General approach to securely managing keys 13.8 Additional Resources", " 13 Google Authorization and R IMPORTANT DO NOT ADD SENSITIVE FILES TO A GITHUB REPO UNTIL THEY ARE ENCRYPTED! EcoHealth Alliance sometimes uses Google Drive, and Google Sheets in particular, to store and collaborate on data. Working with Google Drive-based files in R is relatively painless thanks to the googledrive, googlesheets4, and gargle packages. What is less straightforward is working with drive based files without having to manually authenticate your identity. In this chapter, we will walk through the process of creating credentials with API access that can be used in your R project or package. Ultimately, this could allow you to fully automate your Google-centric data pipelines. 13.1 The basic overview: Create or store something (sheet, csv, doc, etc.) in Google drive Create a Google Cloud project to manage Google services Enable the appropriate APIs for the project so it can access things like Drive and Sheets. Create a service account so that you can access the APIs via credentials from R Encrypt credentials then add them to your R project so that you can still use git-based workflows without leaking access to your service account Share Google-based resources with the service account and check that credentials work as expected from R Add an encryption key to RStudio Connect or Github as an environment variable so that R can access the resources in automated workflows 13.2 Key Terms Authentication - Confirms the identity of an entity Authorization - Permits an entity to do something Auth - shorthand for authentication or authorization Key or Token- Computer-generated credentials that allow for authorization and authentication. In the R-Google universe key and token are synonyms, though not all services use this way, at at times okens and keys are communicated over the web using different method. You will see these terms used interchangeably in tutorials. Symmetric encryption - A type of encryption that uses a single key to encrypt and decrypt an object Asymmetric encryption - A type of encryption that uses public and private keys to encrypt and decrypt an object Environment variable - A value stored in a computer’s system environment. In R, this generally means values stored in the .Renviron file, which can be brought into your project using Sys.getenv(\"Variable_Name\") and are very useful for storing sensitive information like tokens and keys. Service - Functionality provided by another system i.e. serving data via an API. GCP - Google cloud platform. Web services from Google. 13.3 Before we start: Note: The preferred method for adding encryption to projects is via git-crypt. See chapter 14 for more on git-crypt. Credit: This chapter largely follows the non-interactive auth vignette from the Gargle R package, but diverges for package and non-package focused projects. What about Billing?: Good question. This is not an issue for Google Sheets or Drive APIs but you do need a linked billing account for BigQuery and Maps APIs. If you’re new to GCP as of 29 Sept 2021 you get $300 of credits in the Free Tier. If you use the $300 in credits GCP will ask for consent before billing. Check with the Data Librarian about using and billing arrangements beyond this. 13.4 Setup encryption tools on your machine See chapter 14. This process will take ~30 mins and involves using the command line. 13.5 Enable git-crypt on your repository Enabling git-crypt requires your code to be stored in a git-backed repository. See chapter 8 for setting up git repositories. Enabling git-crypt happens from the command line. You can access the command line directly in Rstudio. If you use rstudio projects and the command line in Rstudio, then the terminal should open in the repo you want to encrypt. In the command line run: # check that you are in the directory with the repo pwd ## /path/to/repo-i-want-to-encrypt # if you&#39;re in the wrong directory, use cd to navigate to the correct repo # cd /path/to/repo-i-want-to-encrypt git-crypt init Next you want to tell git-crypt which files should be encrypted. To do this, create a file in the top level directory of your repo called .gitattributes. Here you will list the files and folders you would like to encrypt. Each item should be placed a on separate line. To learn more about pattern matching in the .gitattributes file, see the git-crypt read.me and gitignore manual Your .gitattributes file might look something like this: .env filter=git-crypt diff=git-crypt auth/** filter=git-crypt diff=git-crypt .gitattributes !filter !diff The .env file will be used to store environment variables and the auth/ folder will be used to store keys. Do NOT encrypt the .gitattributes file. It maybe a good idea to add your google auth key explicitly to the .gitattributes file using this format **/mySecretKey.json so that the key is encrypted independent of the directory it is in. Next add yourself and other users who will require access to the encrypted files to the repo. The encryption chapter 14 in the handbook details how to add contributors to the repo. Finally, you will have to set up a symmetric key for github actions to use. This key can always be regenerated so there is no reason to store it. First, add the git-crypt key to your .gitignore so you don’t accidentally commit it to the repo. .Rproj.user .Rhistory .Rdata .httr-oauth .DS_Store inst/.DS_Store git_crypt_key.key In the command line run: # create the symmetic key git-crypt export-key git_crypt_key.key # convert it from binary to bas64 so github can use it # the file&#39;s contents can now be pasted into a github secret # environment variable cat git_crypt_key.key | base64 | pbcopy Paste the key into Github’s secret environment variable field as GIT_CRYPT_KEY64. Now delete the key file: # doesn&#39;t have to be done in terminal but you&#39;ve already got it open rm -i git_crypt_key.key You have now setup asymmetric encryption for human users and symmetric encryption for automations. The next steps involve getting the files you want to encrypt. 13.6 Setting up non-interactive authentication for Google sheets In this section, we will walk through setting up credentials that can be used in R to access Google sheets without manual authentication. To achieve non-interactive authorization, we want to either provide a token directly to a service or make a token discoverable for a service. A token is essentially a long password, designed to be exchanged by machines but too long and complexly formatted to be used by people, and often time-limited. Remember that tokens, secrets, and API keys should be stored in a secure fashion (NOT stored in the text of your code or in unencrypted files). We are going to follow the recommended (as of 29 September 2021) strategy of providing a service account key directly to handle authorization. A newer approach called “workload identity federation” exists as of writing but is not fully implemented in the gargle package. 13.6.1 Create a Google cloud platform project “Google Cloud projects form the basis for creating, enabling, and using all Google Cloud services including managing APIs, enabling billing, adding and removing collaborators, and managing permissions for Google Cloud resources.” - GCP Docs We will use a GCP project to access the Google sheets API via a service account. You do not need a profound understanding of GCP projects to setup a service account. Setup/view your Google cloud account Create a project on Google cloud to hold your credentials The GCP console is your destination for monitoring and modifying your projects 13.6.2 Enable APIs GCP Projects are centered on the idea that a single project will contain a single application. In our case, the application we are creating relies on the Google Sheets API. You can enable API’s for our application to access via the APIs &amp; Services menu item. In the left side menu, navigate to APIs &amp; Services &gt; Library Choose your api of interest. For this example it is Google sheets. Enable the api of interest. If you need to enable more API’s later you can always come back. 13.6.3 Create a Service Account Service accounts allow applications, like the GCP project we make, to access certain resources they need via authorized API calls. The service account’s access can be limited such that it can only access specific resources in a certain way. Importantly, service accounts are not part of the EHA workspace domain. You have to manually share resources like Google sheets with a service account even if you have provided domain-level access. GCP Service Account Docs Navigate back to your project homepage In the left sidebar go to IAM &amp; Admin &gt; Service Accounts Click create service account Give it a good name and description For Google sheets, we do not need to assign our account service a role Roles can be established to perform tests and otherwise manage the service but are not necessary Also not necessary to grant user access for this example You may have a need for this with more complicated services It may also be a good idea to get some redundancy in your workflow 13.6.4 Create a Key for your service account Keys for Google service accounts are stored in JSON files. Remember that this key will hold very sensitive information and we should treat it like a username and password combo. Click on the appropriate item in the service accounts table. Notice that it says no key. Click on the keys tab, then click on ADD KEY Select create new key and download the JSON file. WARNING: Do not store this unencrypted key in a shared location (Dropbox, Google Drive, folder connected to a git repository). If you are using the git-crypt workflow, add the file to the “./auth” folder in your project’s working directory. You should now see that there is an active key associated with your service account in the GCP project. 13.6.5 Share the Google drive resources of interest with the service account Make sure your sheet is shared with the service account Remember that service accounts do not belong the EHA domain so there will be extra prompts when sharing resources. 13.7 General approach to securely managing keys This workflow uses the relatively simple approach of symmetric encryption to securely store files on shared resources like a github repository. Symmetric encryption uses a single key to encrypt and decrypt files. In this workflow, the key is generated from a passphrase. Unencrypted files storing keys for the Google service account should NOT be stored in shared or public locations (Drive, Dropbox, Github Repo) If possible store in an encrypted volume. Keybase, Bitwarden, and other credential management storage systems generally allow you to store files in an encrypted manner. Files storing keys for the Google service account only ever enter the project working directory after being encrypted or after git-crypt has been initiated and the file is included in the .gitattributes file or stored in an encrypted folder like ./auth. For the sodium based work flow, encryption keys or the passwords used to generate keys are stored as environment variables and retrieved from .Renviron, never hard-coded into scripts. Skip ahead to Securely managing your keys for packages if you are using the key in a project that produces a package. 13.7.1 Provide a service account key for projects Now you have a secret key for the service account and need to securely access it in an R project. If you setup git-crypt, make sure the file is saved in an appropriate location (e.g. ./auth) and that all the user who need to use the encrypted file are added to the repo. If you used git-crypt, you can skip ahead to Run your code using encrypted keys. Alternatively, we can use the sodium and gargle packages to encrypt the JSON file that stores our key, safely store the encrypted file in the R project, and securely store the encryption key as an environment variable. This workflow is retained in the handbook but is not preferred and will be removed once the project that used it is fully transitioned to a git-crypt workflow. This section was inspired by the workflow described in ROpenSci:Security with additional information from the sodium vignette and gargle vignette. 13.7.1.1 Create and store encryption password Here we will create an environment variable to store our encryption password. We will store it in the user-level .Renviron file that is outside of your project repository so it is not accidentally committed to your project. Creating your password encrypting the key should only need to be done once, then we will write code that makes use of your encrypted key every time it runs. library(sodium) library(gargle) library(usethis) library(readr) ## Name of Environment variable pw_name &lt;- gargle:::secret_pw_name(&quot;service_account&quot;) #&gt; SERVICE_ACCOUNT_PASSWORD ## Value of Environment variable pw &lt;- gargle:::secret_pw_gen() #&gt; someSecretComplicatedPassword sprintf(&quot;%s=%s&quot;,pw_name, pw) #&gt; SERVICE_ACCOUNT_PASSWORD=someSecretComplicatedPassword usethis::edit_r_environ(scope = &quot;user&quot;) #### Copy and paste pw_name=pw into .Renviron file ### # 1 SERVICE_ACCOUNT_PASSWORD=someSecretComplicatedPassword # 2 # always leave an empty line at the end of the .Renviron file 13.7.1.2 Create encryption key make_sodium_key &lt;- function(env_var_name = &quot;SERVICE_ACCOUNT_PASSWORD&quot;){ pw &lt;- Sys.getenv(env_name) # we use scrypt as our hashing function because it makes keys difficult # to brute force. k &lt;- sodium::scrypt(charToRaw(pw)) return(k) } key &lt;- make_sodium_key() # The key we just made is binary so it has to be converted to character in order # to be stored in .Renviron sodium::bin2hex(key) usethis::edit_r_environ(scope = &quot;user&quot;) ########### .Renviron file #################### # 1 SERVICE_ACCOUNT_PASSWORD=someSecretComplicatedPassword # 2 SERVICE_ACCOUNT_NA_KEY=output_from_bin2hex_function # 3 # always leave an empty line at the end of the .Renviron file 13.7.1.3 Symmetrically encrypt your file Now that we have a sodium key stored as an environment variable, we can symmetrically encrypt our JSON file that stores the service account key. The encrypted file will live in inst/tokens within your working directory. After the file has been encrypted it is safe to push it to a remote git repository. # get your sodium key from the environment variable get_sodium_key &lt;- function(env_var_name = &quot;SERVICE_ACCOUNT_NA_KEY&quot;){ k &lt;- sodium::hex2bin(Sys.getenv(env_var_name)) return(k) } key &lt;- get_sodium_key() # Our encrypted file will live here dir.create(&quot;./inst/tokens&quot;,recursive = T) binJSON &lt;- readr::read_file_raw(&quot;My/directory/outside/the/project/service_account_key.json&quot;) cipher &lt;- data_encrypt(msg = binJSON, key = key2,nonce = random(24)) # The encrypted file in inst/tokens is now safe to push to a remote repository saveRDS(cipher, &quot;./inst/tokens/service_account_key.rds&quot;) 13.7.1.4 Run your code using encrypted keys For git-crypt users, the file will already be decrypted on your machine. If it isn’t, run git-crypt unlock in the terminal. If that does not work double check that your public key has been added to the repo. jsonChar &lt;- readr::read_file(&quot;./auth/myKey.json&quot;) googlesheets4::gs4_auth(path = jsonChar) For sodium users, decrypt the credentials then pass them to Google using the gs4_auth function: library(googlesheets4) key &lt;- get_sodium_key() service_account_token &lt;- readRDS(&quot;./inst/tokens/service_account_key.rds&quot;) jsonRaw &lt;- sodium:::data_decrypt(service_account_token,key = key) jsonChar &lt;- rawToChar(jsonRaw) googlesheets4::gs4_auth(path = jsonChar) 13.7.1.5 Read in your sheet Make sure your sheet is shared with the service account if it hasn’t been already googlesheets4::read_sheet(&quot;https://docs.Google.com/spreadsheets/that/i/definitely/shared/with/the/service-account&quot;) 13.7.1.6 Add Environment Variable to other services Non-interactive authentication allows us to automate workflows that involve Google sheets on services like Rstudio connect or Github Actions. In either case, it is relatively straight forward to securely add our SERVICE_ACCOUNT_PASSWORD environment variables to those services. See documentation here: Add env variables to RSConnect Add secrets to Github Actions 13.7.2 Securely managing your keys for Packages You can make your secret service account key accessible to your R package. We will use the sodium package as well as functions in gargle to encrypt the JSON file that stores the key and securely store the encryption key as an environment variable. Notice that we are using the ::: operator in function calls. See this article for more details. 13.7.2.1 Create and store encryption password Here we will create an environment variable to store our encryption password. We will store it in the user-level .Renviron file that is outside of your project repository so it is not accidentally committed to your project. Creating your password encrypting the key should only need to be done once. The package can then use code that makes use of your encrypted key every time it runs. library(sodium) library(gargle) library(usethis) ## Name of Environment variable pw_name &lt;- gargle:::secret_pw_name(&quot;gargle&quot;) #&gt; GARGLE_PASSWORD ## Value of Environment variable pw &lt;- gargle:::secret_pw_gen() #&gt; someSecretComplicatedPassword sprintf(&quot;%s=%s&quot;,pw_name, pw) #&gt; GARGLE_PASSWORD=someSecretComplicatedPassword usethis::edit_r_environ(scope = &quot;user&quot;) #### in .Renviron file ### # 1 GARGLE_PASSWORD=someSecretComplicatedPassword # 2 # always leave an empty line at the end of the .Renviron file 13.7.2.2 Encrypt the Secret File gargle:::secret_write( package = &quot;myCoolPackage&quot;, name = &quot;sheets-demo-key.json&quot;, input = &quot;My/directory/outside/the/project/longComplicatedFile.json&quot; ) 13.7.2.3 Test that you can decrypt if(gargle:::secret_can_decrypt(&quot;myCoolPackage&quot;)){ json&lt;- gargle:::secret_read(&quot;sheets-demo-key.json&quot;) googlesheets4::gs4_auth(path = rawToChar(json)) } 13.7.2.4 Give the env variable to services (Optional) If the package will be used in services like RStudio Connect or Github Actions, it is relatively straight forward to securely add our GARGLE_PASSWORD environment variables to those services. See documentation here: Add env variables to RSConnect Add secrets to Github Actions 13.8 Additional Resources 13.8.1 Google Drive The googledrive package allows you to interact with files stored in Google drive from R. You can download, share, delete, copy, publish and otherwise manipulate files on your drive using this package. Package vignettes can be found here 13.8.2 Google Sheets The googlesheets4 package allows you to directly interact with Google sheets in R. You can read, write, reformat, create formulas, and otherwise manipulate the sheet of interest. Package vignettes can be found here 13.8.3 Gargle The gargle package focuses entirely on managing credentials for the Google api. Vignettes for the package can be found here. "],["14-encryption.html", "14 Encryption 14.1 Set up Keybase 14.2 Install gpg and git-crypt 14.3 Create your Keybase keys 14.4 Import your keys to your local keychain 14.5 Configure gpg 14.6 Use git-crypt to unlock a repository 14.7 Managing which users can decrypt files in a repository", " 14 Encryption How do we share data in git repositories that needs to be secure? Sometimes we need to store and share secure information, such as passwords or API keys, to online service accounts. One of our methods of choice for this is to keep these files stored in git/GitHub repositories, but to encrypt them. We do this using PGP (Pretty Good Privacy) encryption, implemented by the program git-crypt. It takes a bit to set up but once activated makes sharing secure and seamless. The PGP encryption scheme involves making a public key that you share and a private key that you use to decrypt data encrypted with your public key. We also use Keybase, a service that helps you publish and verify a public key for this purpose. Instructions for setting this up are below. 14.1 Set up Keybase Sign up for Keybase, and follow the instructions for installing it on your computer. 14.1.1 Installing Keybase on Linux For installing on linux, first identify the distribution by entering the following command into a terminal and noting down the Distributor ID. lsb_release -a Next identify the architecture via, arch Follow the instructions to install Keybase on Linux, available here, making sure to use the section relevant to the architecture and distribution information identified above. 14.1.2 Installing Keybase on macOS and Windows For installing on Windows and macOS, the easiest way is to download and install the graphical user interface available from the Keybase download page. This will also install the necessary command line tools. You may also use homebrew to install keybase on macOS 14.2 Install gpg and git-crypt gpg is the program that implements encryption, and git-crypt sets up git repos for encrypted sharing using gpg. These are already installed on EHA servers. 14.2.1 Install gpg and git-crypt on macOS Use homebrew to install gpg, git-crypt. You should also install pinentry, which is a helper program for entering passwords securely. Run the following in the terminal: brew install gpg brew install pinentry brew install git-crypt Homebrew automatically updates when you run it so if you haven’t used it in a while there may be a somewhat lengthy update 14.2.2 Install gpg and git-crypt on Windows For Windows there are two alternative approaches. 14.2.2.1 Using Windows Subsystem for Linux (WSL) The first approach is to utilize the Windows Subsystem for Linux (WSL). This method requires Windows 10 or higher. You must first install WSL. Do so following this guide: https://docs.microsoft.com/en-us/windows/wsl/install. Once WSL is set up, the necessary packages can be installed through the WSL command line shell. Run the following in the shell: sudo apt update sudo apt install keybase gpg git-crypt 14.2.2.2 Using Windows: install binaries The second method is to install pre-compiled Windows binaries for GPG and git-crypt. First download a Windows-compatible binary for GPG which can be found (here)https://gnupg.org/download/. The ‘Simple installer for the current GnuPG’ binary on that page is the recommended choice. Then install git-crypt by via the following steps: Downloading git-crypt-windows.zip from https://github.com/ecohealthalliance/git-crypt/releases/tag/win-release. This may generate the warning “git-crypt-windows.zip is not commonly downloaded and may be dangerous”. Click the up arrow next to ‘Discard’ and select ‘Keep’. Even after this the download may fail with the message ‘Failed - Virus detected’. Do not worry this is a false positive. If this occurs, search for ‘Virus &amp; thread protection’ in the task bar and click on ‘Manage settings’ under ‘Virus &amp; thread protection settings’. Once there, turn off “real-time protection” and try downloading again. Please make sure to turn it back on again when done. Once downloaded, the unzip the zip file. Open file explorer, navigate to the downloaded git-crypt-windows.zip file and right-click it once. In the menu above choose ‘Extract all’. This will create a new folder containing gpg-crypt.exe. Move the resulting gpg-crypt.exe into a folder recognized by the Windows PATH environment variable. A convenient location is C:\\Program Files\\Git\\cmd\\. Once Keybase and GPG are installed, the terminal commands related to exporting keys from Keybase into GPG are the same regardless of operating system. 14.3 Create your Keybase keys If you are just starting to use Keybase, you can generate new keys for use on your computer using this guide: https://github.com/pstadler/keybase-gpg-github. That guide also helps set up using your key to sign GitHub commits, which you should do for added security. Create a password associated with your keys when asked. You can store this in your password manager such as LastPass or BitWarden. Once your keys are created, visit your Keybase account at and verify your keys via as many other services, devices, or online identities as you want. We suggest at least three. It is also a good idea to generate a physical ‘paper key’ and store it in a secure location. 14.4 Import your keys to your local keychain If you have a Keybase account set and keys already generated, you can now import your Keybase keys to use. Instruction are found at https://blog.scottlowe.org/2017/09/06/using-keybase-gpg-macos/. When followng those instructions, set your keys to maximum trust level. 14.5 Configure gpg 14.5.1 Configure gpg on macOS (and Linux) A common source of errors for macOS (and Linux) users is that the text entry for gpg isn’t set properly. This means that gpg and your terminal aren’t speaking the same language. You can fix this by setting the GPG_TTY environment variable in your shell configuration. export GPG_TTY=$(tty) Adding this to your .profile, .bashrc, .zshrc or other settings files prevents having to run the command when you use git-crypt or sign commits. U Use a text editor to modify the settings file for your shell. These are set in one of the files ~/.profile, ~/.bashrc, ~/.zshrc. For most macOS users, it is ~/.zshrc. Here are instructions using nano, an editor available on most machines. In the terminal, run: nano ~/.zshrc Then, in the nano text editor that comes up, add the following line to the file: export GPG_TTY=$(tty) In the nano editor, press Ctrl-O to write (“write-Out”) to save your changes, then press Ctrl-X to exit. Alternatively, run the following line in the terminal to change your `.zshrc file without using nano or any other editor: echo &quot;export GPG_TTY=$(tty)&quot; &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc 14.5.2 Configure gpg on Windows Windows needs to inform git of the location of the gpg executable. This can be done by opening cmd or PowerShell and entering the following command: git config --global gpg.program &quot;C:\\Program Files (x86)\\GnuPG\\bin\\gpg.exe&quot;` Note that if GnuPG is installed in a different location the command should be altered to reflect this change. 14.6 Use git-crypt to unlock a repository The git-crypt README outlines the basics of using git-crypt to encrypted and decrypt files in a git repository. First be sure your public key has been added to the repository. To do this, check the .git-crypt/keys/default/0 folder in the github repo for your public key. If your key is present pull or clone the repo then run, git-crypt unlock from the terminal or command line in the repository folder. If all goes well, congrats! Encryption and decryption for pushing and pulling should now happen automatically. 14.6.1 Troubleshooting If git-crypt unlock fails, try the following steps: Verify that GPG has successfully imported your private key from Keybase by opening a shell and entering gpg --list-secret-keys. If no keys are found, follow the guide for importing keys. This might fail for mac and linux users if export GPG_TTY=$(tty) is not in your .bashrc or .zshrc. Make sure that your GPG private key has actually been added to the repository. Navigate to the ~/.git-crypt/keys/default/0/ directory on github and look for a file that matches your public key. Once your key has been added, pull from the remote to make sure the key is available to your local repository. Windows users need to let git know where to find the gpg executable. Double check that the gpg.exe really is in the folder you specified above. If not find it’s location or try re-installing gpg. 14.7 Managing which users can decrypt files in a repository If you’ve made it this far and you only need to unlock a repository that has already been set up you’re done! The instructions below will help you go further by outlining how to grant access to encrypted files on a repository, add your key to the EHA servers, and to initialize a new repository to use git-crypt. 14.7.1 Set up encryption for a repo that did not previously use git-crypt. This will initialize the repository and add the default gpg key to git-crypt. Note: adding encryption to a repository will only encrypt files going forward. Any previous versions in the commit history will still be un-encrypted. Best practice is to set up git-crypt first, add relevant file names to the .gitattribues file, and only then add any sensitive files to the repo. ## In the repo base directory open the terminal or command line and enter: git-crypt init ## To verify that files are being encrypted run: git-crypt status 14.7.2 Allow contributors access to encrypted files: First add their public key to your keychain. Visit their Keybase profiles (e.g., https://keybase.io/noamross) and click on the key - it will show several ways to import the keys. Two methods are shown below, # curl + gpg pro tip: import noamross&#39;s keys curl https://keybase.io/noamross/pgp_keys.asc | gpg --import # the Keybase app can push to gpg keychain, too keybase pgp pull noamross Next edit the key so that it has sufficient trust levels # in terminal gpg --list-keys ## Copy the key that matches the individual you want to allow access to ecnrypted files. ## Edit that key so that it has sufficient trust levels as described in the above [guide](https://blog.scottlowe.org/2017/09/06/using-keybase-gpg-macos/). Add the key to your git repo (make sure you’re in the right directory) ## this will automatically commit changes. Note: replace [key] below with the appropriate gpg key. git crypt add-gpg-user [key] push changes to remote (github) git push remind the added individual to pull these changes down before they try to unlock the repo. 14.7.3 Import your keys to the EHA server You will probably want the ability to decrypt files when working remotely, so place them on the EHA server. You can do this by copying over your whole gpg keys directory to the server, like so: scp -rp ~/.gnupg url.of.server: Note that the main EHA analysis servers share user file systems so you only have to do this on one of them. You may have to edit the file ~/.gnupg/gpg-agent.conf on the server. If its first line is pinentry-program /usr/local/bin/pinentry-mac, change it to pinentry-program /usr/bin/pinentry. Note that it may not exist at all, which is fine - it means the program is just using default behavior. 14.7.4 Extra: Use a symmetric key for automated processes If you are using continuous integration on a repository with encrypted files, you’ll need to provide a way for the CI system to unlock them. An easy, but not most secure way is to provide a symmetric key. You can generate this by running this in your project directory. This key can always be regenerated so do NOT commit it to your repository. git-crypt export-key git_crypt_key.key git_crypt_key.key can now be used to decrypt the repository, and you can provide it to the CI system as an environment variable. However, since it is binary data, you’ll need to convert it to base64 first. So run something like: cat git_crypt_key.key | base64 | pbcopy to convert this file to base64 data, then paste it in your CI system’s environment variable field as something like GIT_CRYPT_KEY64. The key can now be removed from your system. rm git_crypt_key.key To use the key later, you’ll need (1) git-crypt and gpg installed in the CI system image, and (2) to run these commands after the CI clones your repository: echo $GIT_ENCRYPT_KEY64 &gt; git_crypt_key.key64 &amp;&amp; base64 -d git_crypt_key.key64 &gt; git_crypt_key.key &amp;&amp; git-crypt unlock git_crypt_key.key "],["15-dependencies.html", "15 Dependency Management", " 15 Dependency Management How do I make sure that all my software and configurations needed for a project are portable? Packrat or checkpoint to fix R package versions. Docker for everything A lesson in user Docker for an R project Makefiles can automate a complex, multipart project. Here’s a lesson on them from Software Carpentry R packages can be a useful project output. We have some in-house R packages to provide access to internal data and generate reports, and may be developing more for external audiences. Hadley Wickham’s R Packages Book provides guidance for these, and we expect our packages to be up to rOpenSci standards. "],["16-statistical-methods.html", "16 Statistical Methods 16.1 Multivariate Regression 16.2 Networks 16.3 Bioinformatics and Sequence Analysis 16.4 Species Distribution Modeling 16.5 Epidemic Simulation and Fitting 16.6 Phylogenetics", " 16 Statistical Methods Statistical methods are a far larger subject than can be covered in this handbook. Our team uses a wide variety of methods that differ by project and change via advances in the field. Nonetheless, here are some methods that we favor when they are appropriate. 16.1 Multivariate Regression We like Generalized Linear Models. Many of us approach these from a Bayesian perspective, and build and fit these models using Stan. A great introduction to this type of modeling is found in Statistical Rethinking, by Richard McElreath which we have a couple of copies of. Richard also has accompanying lectures for this book online as YouTube videos. For nonlinear models, we like Generalized Additive Models, for which we use the mgcv R package. Noam has a bunch of materials on this, including slides and exercises from a workshop he co-teaches, and the seminal text is Generalized Additive Models in R, by Simon Wood. (Example - Host-Pathogen Phylogeny Project: Paper, GitHub) When a multivariate analysis is primarily about prediction, and less about variable inference, machine-learning methods such as boosted regression trees are useful. We make use of these through the dismo or xgboost packages. (Example - Hotspots 2: Paper, GitHub) 16.2 Networks We like networks for both descriptive visualizations and quantitative analyses. R is full of packages to achieve both of these goals, check out ggnetwork and bipartite. We assess individual components within networks looking for high-impact individuals due to their high degree, centrality, or ability to bridge different communities. For a tutorial on basic metrics check out Randi Griffin’s tutorial on primate social networks which utilizes the igraph package. Often, our networks are bipartite, meaning the networks shows interactions between two type of nodes. For us, these are usually viruses and hosts. You can look at example networks in two PREDICT publications: Figure 4 in Anthony et al. 2017 and Figure 3 in Willoughby et al. 2017. To assess overall network structure and identify communities, we measure the modularity. The modularity of a network model is a type of network partitioning into subgroups or modules. This is an alternative to clustering alogrithms. For bipartite networks, we like the Barber’s modularity (Q), calculated through the lpbrim package. Alternatives modularity algorithms include NetCarto which uses a simulated annealing (SA) algorithm or using a map equation (ME) algorithm. 16.3 Bioinformatics and Sequence Analysis Bioconductor is a comprehensive ecosystem of R packages focused on sequence analyses. Typing your general topic of interest into their searchable software table should at least provide an introduction to relevant software. In general, best practices in bioinformatics are changing rapidly, so it is difficult to recommend particular procedures. However, the journal Nature Protocols covers cutting-edge methods, ranging from the lab to the laptop, in great depth. Many articles include step-by-step instructions to complete a given analysis. For a scattershot introduction to high-quality bioinformatics software and relevant applications, it might be worth checking out the Bedford, Pachter, and Patro lab websites. Note that many contemporary bioinformatics tools are accessed through the command line rather than R. For a general overview of RNA sequencing analysis (other bioinformatics pipelines have strong similarities), the Simple Fool’s Guide from the Palumbi lab is a great learning resource. 16.4 Species Distribution Modeling 16.5 Epidemic Simulation and Fitting 16.6 Phylogenetics "],["17-data-sources.html", "17 Data Sources", " 17 Data Sources Here are some key data sources we have built and use across many projects: awesome-parasite is our curated list of sources for host/parasite/pathogen associations and associated taxonomies. HP3Extended in an internal EHA database of host-virus associations. It is hosted on AirTable and requires an EHA login. lemis and cites are R packages that host data on wildlife trade. EIDR is our emerging infectious disease repository and hosts data on first emergence effects of disease. FLIRT, the Flight Risk Tracker, contains information on airline travel patterns as well as simulations of human travel using those patterns. EIDITH, the Emerging Infectious Diseases Technology Hub, is the shared database for the PREDICT project. The eidith R package is how we interface with it for analysis. "],["18-references.html", "18 References 18.1 Converting Paperpile Citations in Google Docs to Endnote Citations in MS Word", " 18 References Paperpile is an excellent tool for sharing and keeping track of research papers, and it is fully integrated into Google Docs – which means you can easily add citations on shared documents. Many teams and projects use Paperpile to organize and share references. If you need a paperpile account, request a key from Megan Walsh. Be sure to associate the Paperpile with your EHA Google account, not a personal one. Many people in the office use EndNote which can be offline on your desktop or online. A nice feature of Endnote is the traveling libraries export from word documents. You will need a license for this, so contact Megan Walsh. 18.1 Converting Paperpile Citations in Google Docs to Endnote Citations in MS Word There are some situations where you may need to convert a Google Doc with Paperpile citations into a Microsoft Word Document with Endnote citations. Luckily, Paperpile has a detailed guide to walk you through the process. But first, you must properly install the Paperpile Google Docs Add-on! Note: You may already have the Google Chrome Paperpile Extension installed and be using it to create citations in your Google Doc, but you need the Google Docs Add-on for proper exporting. To install this addon, follow the links from Paperpile’s website dedicated to Google Docs. Now, when you open a document in Google Docs, you should see both menus for Addons and Paperpile. In order to begin Step 1 of Paperpile’s Endnote Guide, you need to access the add-on sidebar: Go to Addons \\(\\rightarrow\\) Paperpile \\(\\rightarrow\\) Manage Citations and the Paperpile side-bar will appear Don’t try to export or mess around with the Paperpile menu option (or P symbol) – they won’t help you! Once you have the add-on sidebar open, you can follow Paperpile’s detailed guide. Note: depending on your operating system and your edition of Microsoft Word, some of the Word menus may appear slightly different than the ones depicted in Paperpile’s guide. However, the Paperpile images included in the guide should look exactly the same. "],["19-training-resources-and-plans.html", "19 Training Resources and Plans 19.1 Training plan components 19.2 Training Plans", " 19 Training Resources and Plans This section under revision as we consider alternate resources. If you are planning on spending significant time improving your data science and modeling skills, you will want to create a training plan. A training plan is required if you would like to use EHA’s Data Camp account. Data Camp provides excellent modular courses comprised of both video and interactive, automatically graded exercises to learn data science and coding. 19.1 Training plan components A description of your goals for the training plans and to which EHA projects and activities you will apply your skills A list of courses/tutorials your plan to complete The total time the courses will take to complete. We find the estimated times of Data Camp courses are roughly accurrate. The time frame over which you expect to complete them The name of a peer learner. You should have a peer learner at EcoHealth who will be a partner over the course of your training. This may be someone working on a similar training plan or someone with knowledge of the material already. They should play some or all of these roles: Accountability: Your peer learner should know about your training plan and its time frame, and check in on how you are doing. Co-learning: Your learning peer and you may want to schedule times to watch course videos and complete exercises together Review: Especially for materials without automating like Data Camp, your peer learner should be able to look at your work and provide feedback Motivation: Your peer learner should make your training fun and tell you that you rock. When your supervisor signs off on your training plan, contact Megan Walsh to provide you with a Data Camp subscription for the period you need it. 19.2 Training Plans These are some suggestions for assembling resources and courses into training plans. This is of course a small fraction of the many learning and teaching resources available, including many more Data Camp courses. Consult your peers, supervisor, and the #data-sci-discuss Slack channel to find courses or resources on the topics you require. If you use a new resource or course, please add to to this page so others can learn from your impressions of it! 19.2.1 Better Managing Data DataCamp: Introduction to the Tidyverse (4 hours) will get you started immediately working with data in R, and puts the following items in context. It’s a good introduction for everyone with little to no experience. If you have no experience, including no EHA workshop, take Introduction to R, as well (4 hours, though there will be some redundancy). If you are mostly working in spreadhseets but collaborating with R or Python users, or just trying to organize a lot of spreadsheet data for your projects, work through the Data Carpentry lesson on spreadsheet organization (~2 hours) and read Hadley Wickham’s paper on tidy data (~1 hour) Both DataCamp: Importing Data in R (Part 1), (3 hours) and DataCamp: Cleaning Data in R (4 hours) will be useful if you are doing a database aggregation or building project. 19.2.2 Version Control and Git Read through and work through the examples in Happy Git with R. There is a Data Camp course Introduction to Git for Data Science (4 hours). This is a more detailed introduction that teaches you more about how git works and features we use less in our workflow. It may be appropriate once you have jumped into a project that uses git more elaborately. 19.2.3 Reproducible reporting Reporting with R Markdown (3 hours) covers making the reproducible reports some teams like for rapid iteration. Take this if you are writing an Emerging Disease Insights report, and if you team is using R Markdown for communication. 19.2.4 Improving Your Statistical Fundamentals DataCamp: Introduction to Data (4 hours) is a good starter course you have not much or any training in statistics in school and want to get familiar with the language and concepts. DataCamp: Foundations of Probability in R (4 hours) covers many of the important concepts that those doing Bayesian model work at EHA discuss. Take this if you are just getting started interacting with or reading this work and you don’t have a probability background. Statistical Rethinking: A Bayesian Course. This is an excellent book and video lecture series that gives builds great foundations for doing many types of modeling. Prerequisites are Intermediate R, some experience with linear regression and probability (for which the DataCamp course above is a good basis). This course has about 19 hours of video. We recommend 2-3 months for going through the book, video, and exercises. 19.2.5 Improving your data visualization Data Visualization with ggplot2 (Parts 1 + 2) (5 + 5 hours). These will take you much further than the basic visualization you learn in Intro to the Tidyverse. Take them if you aren’t able to make the plots you need based on the knowledge you learn in that. Part 3 covers topics not generally needed. Fundamentals of Data Visualization by Claus Wilke is an excellent guide to making high-quality figures, focusing more on design than mechanics of programming. R code is available for all of its examples. If you feel you have a solid grasp of ggplot2 but want to improve the quality of your figures, we recommend reading this e-book, and using the accompanying code in its GitHub repository to reproduce figures. 19.2.6 R Programming DataCamp: Introduction to R is a good place to back up a bit to fill in concepts if you are missing things to help understand other course sequences. DataCamp: Intermediate R (6 hours) or DataCamp: Writing Functions in R/ (4 hours) are both good courses to improve your skills if you find yourself managing a more complex workflow or working on an R package with someone at EHA. 19.2.7 Map-making and geospatial analysis in R Geocomputation in R is a comprehensive guide for understanding geographic data, mapping, and conducting spatial analysis in R. Likely, the most relevant chapters for your purposes are 1-8, 10-11. A chapter might take you 1-3 hours to work through, depending on how in depth you want to get and the number of exercises that you complete. DataCamp also has courses on spatial analysis using R, notably Spatial Analysis in R with sf and raster (4 hours). It covers some of the material from Geomputation in R in less depth, and gives you fewer tools for hooking into the multiple geospatial data systems available in R. Take this if you are more time-limited and are going to do less in terms of spatial statistics. Data Carpentry has a course on using R for spatial data. Like other *Carpentry lessons its designed as a workshop lesson plan but can be self-taught. It presumes very little R knowledge at all, and includes stuff like setting a project in RStudio. This is a good place to start people or students with little R experience to get them making maps right away. If you just want to get a quick feel for R spatial data types, jump into Chapter 3. Making Maps with R is a quick-start guide to mapping with ggplot2. It also introduces the gmap, maps, and mapdata packages for providing basemaps on which to overlay your spatial data. It is good for getting a map together quickly but if you are going to be doing things on a regular basis we suggest the resources above, which give you a better foundation on geographic data. Leaflet for R is a manual on the use of the R leaflet package to harness Leaflet, an open-source JS library for creating interactive maps. Leaflet maps particularly useful for exploring and visualizing spatial data, and are easily embedded into R Markdown documents. You should take a course or have knowledge of R Markdown prior to taking this course. "],["20-help.html", "20 Getting Help 20.1 Minimal reproducible examples - helping others help you", " 20 Getting Help How do I solve this problem? How do I get my skills up to snuff? We have an #data-sci-discuss channel on Slack to ask questions and also news about useful resources and packages. We prefer that you ask question on this channel rather than privately. This way you draw on the group’s knowledge and everyone can learn from the conversation. In general, if you spend 20 minutes banging your head against your screen trying to figure something out, it’s time to ask someone. Some good questions for the Slack room: Which package should I use for something? Anyone have a good reference or tutorial for package, method? What does this error mean? Our technology team are a tremendous resource for a number of computing topics (especially web technologies and development operations), but remember that they are our collaborators, not IT support. (We do have straight IT support, mostly for office network issues, through Profound Cloud) Also, outside EHA: There’s an almost-monthly NYC R Meetup and even rarer Data Visualization Meetup that EHA members sometimes attend. There’s also an R-Ladies NYC chapter that has regular meetups and a Slack Chat room. WiMLDS offers support to attend machine learning and data science conferences. Stack Overflow is a popular Q&amp;A site for computer programming that a lot of discussions about R. R-Weekly publishes a useful weekly newsletter on new R developments, packages, and publications. The #rstats hashtag on Twitter is a good place for news and short questions, and general ranting. If there’s a course, workshop, or conference you want to attend to improve these skills, speak with your supervisor, we can often support this. DataCamp courses are also potentially useful. If you feel they would match your learning style and needs, discuss EHA purchasing a subscription for you with your supervisor. 20.1 Minimal reproducible examples - helping others help you Minimal reproducible examples are small, self-contained code and data packets that will allow others to recreate the issue you are experiencing on their machine. The reprex package is really helpful for creating preformatted code for github, stackoverflow, or slack. This stackoverflow answer provides a succinct walk through of how to create a minimal reproducible example. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
