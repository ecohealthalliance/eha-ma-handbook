[["index.html", "EHA Modeling &amp; Analytics Handbook Introduction", " EHA Modeling &amp; Analytics Handbook “These aren’t rules, just some things that we figured out.” – Michael Reno Harrel Last edit 2024-01-09 by GitHub Introduction This handbook describes best practices and guidelines for project management, organization, modeling and programming we aim for at EHA. This is a living document. To make changes, just click the edit button () at the top of the page. It will take you to the source editor for the chapter on GitHub, where you can make edits and submit your changes. Be sure to commit major contributions to a new branch and open a pull request. In general, we aim to produce sound analyses that are: Self-contained Well-documented and communicated Easily reproducible Sharable Version controlled Together, these attributes help assure that our work is correct, can be built off of and extended, meets requirements for sharing and publication, and can be continued through staff turnover. The tools we use to accomplish this are mostly, but not exclusively, based around the R programming language and git version control. Other teams at EHA use other tools (e.g., the technology team mostly uses Python and Javascript, much of our work with partners is MS Office based). The guidelines in the document represent an ideal we aim for but to not always attain. Remember: Best practices are always evolving. Don’t let the perfect be the enemy of the good. Other teams and external partners have different workflows and we adjust as neccessary to collaborate. Our goal is to do good science to advance conservation and human health, not be slick programmers. You can find some slides from a previous presentation on this topic here. The philosophy and guidelines in this document owe an enormous amount to the work of the Software and Data Carpentry and rOpenSci organizations, and the work of Hadley Wickham and Jenny Bryan. You’ll find many links to their work in this handbook. "],["1-Contributing.html", "1 Contributing 1.1 Anatomy of a chapter 1.2 Workflow Summary 1.3 Git based workflows 1.4 Working with rmarkdown in the bookdown framework 1.5 Review 1.6 Modifying chapters 1.7 Additional resources", " 1 Contributing How do I add or modify a chapter in the EHA M&amp;A Handbook? Identify a gap in the handbook, create a branch, write the chapter and then create a pull request. This handbook is created using the bookdown package. Bookdown allows you to publish a set of Rmarkdown documents into a book. Each document constitutes a chapter in the book. This chapter assumes an introductory level understanding of git. See the chapter on git to learn more. 1.1 Anatomy of a chapter Motivation - What question is the chapter trying to answer? Summary - Short paragraph on the topic and/or bulleted list of workflow Main Content - Longer form answer to the question Additional Resources - Where can someone go to learn more about the topic? 1.2 Workflow Summary 1.2.1 For quick edits Click the edit button () at the top of the page Modify the chapter Commit the changes to master These changes will be immediately reflected in the handbook!. If you would like someone else to review them, commit the changes to a branch then open a pull request. 1.2.2 Adding a chapter Clone the repo Create a branch off of main/master for your chapter Start a new Rmarkdown document with rmarkdown::draft(\"my-chapter.Rmd\",template = \"EHA-MA-Chapter\",package = \"ehastyle\") Add content to RMD Add chapter to _bookdown.yml Preview chapter with bookdown::preview_chapter(\"my-chapter.Rmd\") Create a pull request into main/master Revise and resubmit 1.2.3 Major edits Clone the repo Create a branch off of main/master for your edits Edit Rmarkdown document Preview chapter with bookdown::preview_chapter(\"my-chapter.Rmd\") Create a pull request into main/master Revise and resubmit 1.3 Git based workflows This handbook is generated via github actions. Whenever a change is made to the master branch, the book is re-compiled and published to this webpage. Because of that, its important that we are creating new chapters using branches in git as any changes to master will be added before review. To create a branch use git checkout -b feature/my-chapter then use git push -u origin feature/my-chapter to track it on github. This will create a branch off of master and push it to the repository. 1.4 Working with rmarkdown in the bookdown framework The format of bookdown chapters and the workflow for previewing them is a little different from typical rmarkdown documents. Unlike typical rmarkdown documents, bookdown chapters do not start with a yaml section. Also, the file name for the chapter must be added to the _bookdown.yaml for it to knit as expected. Otherwise, the workflow is more or less the same. The ehaStyle package has a template for handbook chapters. remotes::install_github(&quot;ecohealthalliance/ehastyle&quot;) # if you are installing the package you may need to close and reopen # Rstudio to be able to access this template rmarkdown::draft(&quot;my-chapter.Rmd&quot;,template = &quot;EHA-MA-Chapter&quot;,package = &quot;ehastyle&quot;) This will generate a markdown document with a rough outline of a chapter structure and an overview of the workflow. After you have created the rmd, you can add it to the _bookdown.yaml. The chapters are rendered in order based on the _bookdown.yaml file so try to place your chapter near related content. You can now use bookdown::preview_chapter(\"my-chapter.Rmd\") to make sure that your chapter has rendered properly. By specifying your chapter, you avoid rendering the entire book. 1.5 Review When you’re happy with your chapter, open up a pull request and assign a reviewer. The reviewer will read the chapter and check that the contents are in-line with the overall context of the handbook and sufficient to get someone started on the topic. When your pull request is accepted and merged into master, the github actions workflow will kick in and rebuild the book with your new material. 1.6 Modifying chapters If you are making more than minor edits to the handbook, it is best to follow a branch based workflow as described above. When editing, you do not need to create a new rmd file or add the file name to _bookdown.yaml. 1.7 Additional resources Bookdown - Documentation for bookdown Rmarkdown: The definitive guide - Higher level overview of bookdown ROpenSci Bookdown materials - articles from ROpenSci regarding bookdown Bookdown cheatsheet - Quick reference from happy git with R for bookdown Happy Git With R: Branches - Overview of how to work with branches in GIT Git strategies - an overview of trunk based git workflows "],["2-quickstart-for-computing.html", "2 Quickstart for computing 2.1 Optional", " 2 Quickstart for computing In addition to the standard computer setup provided by office admin (Email, MS Office, etc), M&amp;A members should do the following: Download and install Slack. If you don’t alread have an invite, get one from Noam. Join the #data-sci-discuss and #journal-club channels any others your supervisor suggests or are associated with projects you are assigned to. (Don’t forget #random and #out-of-office-fun!) If you don’t already have one, sign up for an account on GitHub. Provide the Data Librarian with your username so as to add you to the EcoHealth Alliance team. If you already have a GitHub account using a personal email, add your EHA email to your account and set up custom routing to direct EHA organization notifications to your EHA email Install R and then RStudio. Links and instructions can be found here Install git and link it to RStudio and your GitHub account. Instructions for all of this are found here, in Sections 7-15 of Happy Git with R. If you get stuck along the way don’t hesitate to ask for help (via the #data-sci-discuss channel on Slack!). Install Dropbox on your computer with your EHA account (note you can have separate personal and EHA Dropbox folders) Check that your EHA email gives you access to Google Drive. If you prefer it, or your supervisor specifies it, install it locally on your computer 2.1 Optional In addition, the following programs may be helpful to install (all for Mac users): Homebrew for general package management iTerm2 for a better shell interface. (brew cask install iterm2 or brew install --cask iterm2 using Homebrew in the shell) mosh for connecting to our high-performance servers (brew install mosh using Homebrew in the shell) hub for interacting with GitHub from the shell (brew install hub) "],["3-projects.html", "3 Project Management 3.1 Teams and Work Cycles 3.2 Setup and materials Organization", " 3 Project Management 3.1 Teams and Work Cycles At EHA projects are typically months-to-years-long workstreams centered around a main analytical or research problem. A program, grant or contract may have multiple projects, and a project may have multiple outputs such as reports, scientific publications, or apps. A project typically has a small (2-5 person team) with a project lead and possibly a project administrative point of contact (APOC). We organize projects into work cycles of 4-8 weeks. For each cycle, a team should define day-to-week scale tasks, assign tasks to members, determine the percentage of time team members will put towards the project in that cycle, given other workloads, and plan travel, reporting, collaboration, or other deadlines. Teams report out their progress at the end (and start) of each work cycle at our weekly M&amp;A meetings. Report-outs should include Progress on tasks assigned and completed in previous cycle Substantive report-out of results and products Draft plan for tasks and goals for the coming cycle Team assignments for the next cycle and level of involvement (high (&gt;50%), medium (25%-50%, low (&lt;25%)) of team members over the cycle. Any additional deadlines or reporting anticipated in that time frame, including plans for other internal presentations or feedback sessions. During report-outs, the M&amp;A group will provide feedback for the upcoming cycle and set a date for the next one. Teams track work cycle progress through various mechanisms based on team preferences. One option is GitHub Milestones (Example). Others use Google Spreadsheets, Air tables, or other systems. Teams may choose what they prefer as long as their system Shows current tasks, deadlines, and assignments Tracks past tasks, deadlines, and assignments Includes top-level summaries for a reporting period Is available in “real time” online rather than stored on individual machines and e-mailed Can be made accessible to other staff via a URL but kept private within EHA note: historically teams had used Asana 3.2 Setup and materials Organization An M&amp;A project lasting more than one work cycle should typically have a Slack channel for communication, a GitHub repo for data and analysis code, or Dropbox or Google Drive folder for documents or materials not appropriate for git-based version control. In addition, it may have a Paperpile folder for references. In general, one URL (often the GitHub README) should be the starting point from which one can reach all project materials. 3.2.1 Code organization In general, one should aim to set up the analysis portion of a project in a self-contained way, with clear separation between raw data, processed data, exploratory analyses, and final products. In organizing a project folder, ask If I copied this whole folder onto someone else’s computer, could they pick up the project? Are the folder organization and file naming clear? There are some exceptions for large data sets or rapidly changing data sets. In these cases, data can be organized as a separate folder or project, and large data sets can be stored in an Amazon Web Services S3 bucket. In many cases it is actually best for data to be organized as a separate resource from analysis. This allows multiple analysis projects to rely on the same upstream data project, avoiding multiple versions of data. Data may also not be best stored in a git repository but in a project database to be pulled for analyses. See EHA guidance on setting up data resources for a project here 3.2.1.1 RStudio Projects In general we also prefer that R analyses be set up as RStudio projects. Here’s a Software Carpentry Lesson on RStudio projects 3.2.1.2 targets We recommend that analysis projects be set up using the targets framework to define steps in the code. targets is a package for defining R project workflow and tracks your functions and objects to ensure everything is up-to-date. Here are some resources for getting started with targets Internal presentations on using targets at EHA: (Videos (password protected on internal AirTable), Slides Example Repository on GitHub Brief overview of using renv with `targets Repo for Branching in targets The targets manual, including a walkthrough A high-level overview paper outlining the concepts and benefits to workflow tools like targets. A good paper discussing how targets helps with research workflows A good intro talk(~90 mins with questions) by Miles McBain on using targets and getting a good workflow A great introductory video series (5 30-minute lessons as a YouTubePlaylist) 3.2.1.3 Package management with renv We strongly recommend that projects use the renv package to manage versions of R packages so that the project does not break when packages update or when run on machines that have different package versions installed. The renv getting started guide tells you most of what you need to know. Here is a short presentation on using renv at EHA: (Video (via password at internal AirTable), Slides, Example Repository on GitHub) "],["4-r.html", "4 R and Reproducible Analysis 4.1 Install 4.2 Learn 4.3 Additional Resources", " 4 R and Reproducible Analysis Can everything be re-done easily if I change one data point in the inputs? At EHA R is our primary, though not exclusive, tool for analysis and modeling work. R is not just a piece of software for statistics and data manipulation but a computer language, meaning that our analyses are scripted. This means they thus can be automated, run again, built upon and extended. 4.1 Install R itself RStudio, the leading R development environment. 4.2 Learn Learning R is beyond the scope of this document, and you likely already have some experience in it, but some good starting points are: The Software Carpentry Lessons Swirl, a set of interactive lessons run right in R. The JHU Coursera Series R for Data Science by Hadley Wickam is a beginner/intermediate text that we highly recommend for getting up to speed with the particular workflows we recommend and the most recent packages that support them. Advanced R (Wickham) is very good for understanding how the language works. Efficient R by Colin Gillespie and Robin Lovelace is helpful for imporving workflows and speeding up code. R Packages (Wickham) is good for package development. Cheatsheets from RStudio are a useful references for a number of things. Dataquest courses are also potentially useful. If you feel they would match your learning style and needs, discuss EHA purchasing a subscription for you with your supervisor. These resources are largely about the mechanics of programming in R, rather than using it for statistical analyses. This is a far larger subject, but see the Statistical Methods section for a jumping-off point. 4.3 Additional Resources Trouble shooting your code: Getting Help. User groups/communities of practice: R Meetups Specific domains: Training Plans "],["5-eha-team-communication.html", "5 EHA Team Communication 5.1 Install", " 5 EHA Team Communication How do we work together and keep a useful record of our interactions? Slack is our office chat tool and is good for day-to-day communication. Slack does not have to be an instant communication tool - some people prefer to check it a few times a day. Check with your supervisor about your project/team preferences. Slack’s main purpose is to organize our communication by channels specific to a topic or project. It is good for keeping information from one project together in a way that can be referenced later by new team members, rather than being lost in various e-mail inboxes. A channel can be linked to many other tools (Dropbox/Google Drive Folder, GitHub Repository), so as to have a central hub for project management. E-mails can be forwarded to a channel. Slack also has voice-calling and, critically, screen-sharing capabilities that are useful for pair-debugging while programming. GitHub (see below) has a good issue-tracking system that accompanies each project and can be used for task management and general communication. This ties messages to a specific project and keeps a good long-term record, and can be connected to a slack channel or integrated with e-mail Remember that your Slack and GitHub communications are part of your project and are likely to be seen by both internal and external collaborators. 5.1 Install Download and install Slack. If you don’t alread have an invite, get one from Noam. Join the #data-sci-discuss and #journal-club channels and any others your supervisor suggests or are associated with projects you are assigned to. There’s also a mobile Slack app for iOS and Android, which may be helpful if you are traveling. "],["6-documentation-and-outputs.html", "6 Documentation and Outputs 6.1 Learn 6.2 Install 6.3 Library", " 6 Documentation and Outputs Will someone understand this thing when I hand it over? Documentation is essential to collaboration and continuity for our work. Your project should contain documentation to allow a project to be picked up by another user. Documentation includes the following: A README document in the top level of the project folder with a high-level explanation of project organization, purpose, and and contact info. Metadata for your data set showing its origin and the type of data in each field in a table. Comments in your computer code Written descriptions of your analyses. The primary medium for this should be R Markdown documents, which allow you to combine code, results, and descriptive text in an easy to update and modify form. Shorter ephermal results can be posted as plots to your project Slack rooms. 6.1 Learn R Markdown is pretty straightforward to learn. You can create your first document and get the basics by going to File &gt; New File &gt; R Markdown in the RStudio menu. When you have time, dive in a bit more with this great lesson on it with accompanying video. Here’s an RStudio Reference Sheet for R Markdown. 6.2 Install (Very optional unless you are asked): ehastyle is our internal EHA R package with R Markdown templates for some reports we produce. 6.3 Library Once your project has been published, you should include the output in our EHA Library. Simply submit through this google form. The library database, along with full texts of both media and science publications, is stored on DropBox. The purpose of the EHA Library is to store our work in a searchable, systematic database. A centralized location of EHA work is of use to our science and development teams. Additionaly, standardized analyses (stored in the eha-library github repository) create easy reporting to our board, our funding bodies, and the public. We have found this database particularly useful in the on-boarding of new staff. The current scope of the Library includes: Scientific publications Conference presentations Layman publications by EHA staff EHA-mentored academic work Media articles that highlight our work or our scientists "],["7-data-management.html", "7 Data Management 7.1 Data Management Plan 7.2 Notes on data management 7.3 Backups 7.4 Learn 7.5 Install", " 7 Data Management EcoHealth Alliance is committed to producing and promoting reliable and reproducible research. In order to achieve this, we have to provide data (and other research outputs) that non-team members can interpret and use; as well as promote best practices for data management among collaborators. Ideally, the framework for managing data laid out in this chapter will facilitate the creation of high quality, share-able research outputs. By focusing on Data Management Plans and the dmptool, we can build on well established workflows for producing high quality research outputs. 7.1 Data Management Plan Data Management Plans , also called Outputs Management Plans or Data Management and Sharing Plans, are living documents that help structure the creation and management of data throughout the lifecycle of a project. DMPs are flexible and do not force researchers to choose a particular technology set but rather ask probing questions about the mechanics and ethics of data use in research projects. Organizing data management in this way provides a common framework to think about data without requiring specific technologies be used in the research workflow. Furthermore, DMPs use stable identifiers (URIs) to connect components of the research workflow, making long term data access more reliable. The majority of funders require a DMP; however, each funder has specific expectations about what, when, and how research outputs should be shared. It is important you and your collaborators understand those expectations before submitting a DMP. Its equally important that all collaborators understand and agree to the obligations created when submitting a DMP. Early communication between collaborators is key to navigating differing expectations about data sharing from researchers in different contexts. Data management plan as hub in knowledge management system Important note on budgeting: Data management activities, but not necessarily infrastructure, are an allowable cost for most funding agencies (NIH, NSF, NASA). Gray areas include paying for hosting services and other infrastructure-like components of the DMP. Benefits of using a DMP: They provide a scaffold for you to conceptualize data management for your project What data do you need to answer your research question, where will it come from, what resources are needed throughout the project lifecycle, what are the mechanics of managing the data? They make it easier collaborate Defining responsibilities, Committing to using data standards, Documenting how the project works They make it easier for your data to be reused You get more citations, your effort contributes to knowledge creation in unexpected ways, your results become more reproducible They are a funder requirement and you want funding NIH, NSF, NASA, Wellcome Trust, etc. require a DMP be submitted with a proposal. Components of a DMP: Data Type - What will be collected or created during the project? Related Tools, Software, or Code - Whats needed to make your analysis run? Standards and Documentation - How will people/machines know what they are looking at? Where, when, and how will data be made accessible? Restrictions on data use - How will you abide by ethical standards or other restrictions on data reuse? Responsibilities - Who is supposed to do what? How will you monitor that? What do they have to do it with? EHA DMP Philosophy: Its never too late to write a DMP Data Management Plans are living documents that change with a project DMPs are created collaboratively and stored in DMPTool.org We ensure our DMPs meet EHA best practices for FAIR data and Reproducible Science Projects should have adequate resources (personnel time, infrastructure, time in project schedule) to implement the DMP Collaborators, especially those from outside institutions, are full participants in the DMP process 7.1.1 Expectations by project phase Proposal/Pre-Award Phase Look for funder requirements and use funder specific templates for DMPs. If no template exists, use the EHA Minimal Data Management Plan or create one based on funder requirements. Think about how you might make data Findable, Accessible, Interoperable and Reproducible (FAIR) use the re3data data repository catalog to find identify a potential archive for your data Establish expectations for data sharing and outputs with collaborators and PIs. These discussions should begin early at the same time as discussing project responsibilities and budget. Consider what tools you will use throughout the lifecycle of your data  Consider how data collection, analysis and management tasks will be divided among collaborators Incorporate data management activities into your project staffing, budget, and schedule Outline the ethical considerations for properly managing data in your project Ensure collaborators and PIs understand the commitments they are making via the DMP. Request and incorporate feedback from collaborators. Schedule a meeting with the Data Librarian, create a timeline for proposal submission, and have a notion of tools and standards to use Post-Award/Early Phase Review and update proposal DMP include updates from IRB or IACUC Refine roles and tech stack Provide more detail on data collection and storage Think about how measurements and primary data sets will be stored Think about how statistical models and derived products will be produced and stored Consider where data will be stored long term, how it will be accessed, and by whom Determine backup strategy and setup backups Think about how you will store artifacts of analysis Where will your code live? Who will be able to access it? If you’re using spreadsheets with formulas, proprietary software or other methods analyzing data, how will you make that workflow reproducible? Schedule a meeting with the Data Librarian Operational Phase Link the plan to the research artifacts (data sets, publications, code repositories, etc.) being created via URI’s. Review, revise, and update components of the DMP check that IRB/IACUC documents match DMP and protocols Make sure all SOPs and relevant data collection or analysis documents are accessible (linked) to the DMP  Check that data storage locations and methods are well described and linked  Check that necessary stakeholders are identified  Check that appropriate privacy and security measures are working as expected  Add any publications or research outputs are to DMP outputs DOI’s can be assigned to code, datasets, and published articles Check that your data products and code meet the needs of your proposed long term storage solution Schedule a meeting with the Data Librarian Publication and Archiving phase Review and revise your DMP Check that linked objects are accessible to the appropriate individuals Add DOI or stable identifier for research objects  DOI’s can be assigned to code, datasets, and published articles Submit materials to long term storage Ensure sharing and access are in agreement with requirements from IRB and/or research  Use EHA institutional tags where possible e.g. Zenodo Community Schedule a meeting with the Data Librarian 7.1.2 Using DMPTool to create prepare your proposal data Management plan Create an account on DMPTool.org associated with EcoHealth Alliance Identify Funder DMP requirements and Schedule a meeting with the Data Librarian Create a DMP using appropriate template given your funder. If no template is available or the funder has no requirements, use the EHA Minimal Data Management Plan. Add collaborators and complete as much of the plan as you can Principle Investigators and Project Partners explicitly agree to abide by the DMP. All collaborators should fully understand and agree with the data sharing components of the plan before approving it. Request feedback from the Data Librarian Work with the Data Librarian to incorporate feedback Export DMP for inclusion in grant 7.2 Notes on data management Can the data be shared and published, and easily re-used in other analyses? Create and maintain a data management plan Store data in simple, interoperable formats such as CSV files. Microsoft Excel can be a useful tool for data entry and organization, but limit its use to that, and organize your data in a way that can be easily exported. Metadata! Metadata! Document your data. For relational datasets you can create linked data on Airtable. For more information see 8 For data sets that cross multiple projects, create data-only project folders for the master version. When these data sets are finalized, they can be deposited in public or private data repositories such as figshare and zenodo. In some cases it makes sense for us to create data-only R packages for easily distributing data internally and externally. We aim to generally work in a tidy data framework. This approach to structuring data makes interoperability between tools easier. 7.3 Backups Cloud based storage solutions like google drive, dropbox, and aws S3 (used in airtable and ODK) are extremely reliable. Nevertheless, it is a good practice to have a backup of critical research objects like datasets and code. 7.3.1 What do we envision these backups being used for? Backups should protect against catastrophic loss of data. Catastrophic loss includes things like losing access to a service (either because the system is down or we cut ties), deletion of a dataset, or deletion of key tables. Backups may be cycled to save on space (e.g. backs are deleted after a certain period of time). In the event of catastrophic loss, it should be possible to restore or reconstitute a dataset from one of these backups. The snapshot and revision history in features in cloud storage should be sufficient for “time travel” type backups. 7.3.2 What counts as a back-up? A backup is any copy or representation of the data stored outside of the cloud storage system that allows users to recover the data stored in the system and allows the structure of the full dataset to be reconstructed (e.g. can restore relationships in airtable bases). Criteria: 1. Data are stored outside the service 2. Data are properly documented with a data dictionary and other metadata 3. Data can serve as a replacement in established research workflows Some groups may already have versioned backups of data outside of a given service that are not necessarily pulling the whole database but are capturing essential data. Ultimately, if this type of backup is sufficient to meet research goals that is fine. Some other groups may have a single source of truth that is split across multiple databases/workspaces. Priority should be given to the single source of truth. E.g. A central laboratory database consolidates data from several countries (USA, Canada, and Mexico). Country level mirrors of the database are created (e.g. just the data from the USA) to provide access to the data for that user group. Those country level mirrors do not need to be backed up unless they are being modified in a way that is not reflected in the central database. 7.4 Learn Watch M3 on Data Management Plans Read California Digital Library guidance on Data Management Plans Data Management Plan Skill Building from DataOne NIH Data Sharing Guidance NIH Data Sharing learning Resources Condensed NIH DMSP Guidance Resources NSF Bio DMP Guidance EHA Repo with additional DMP Resources Read Hadley Wickham’s tidy data paper for the general concept. Note the packages in this paper are out of date, but the structures and concepts apply. R For Data Science is a great online book to read and reference for working in this framework, and gives guidance for the most up-to-date packages (tidyr being the latest analogue of reshape and reshape2). Data Carpentry has a Lesson on spreadsheet organization for when you need to do some work in Excel but make it compatible with R. Nine simple ways to make it easier to (re)use your data rounds some things out in terms of data sharing. This post is nice, too. 7.5 Install Get the tidyverse package for R using install.packages(\"tidyverse\"). This will install several other relevant packages. "],["8-airtable.html", "8 Airtable 8.1 Key Terms 8.2 What is Airtable? 8.3 When should I use Airtable? 8.4 Security, Access Control, and Data Durability 8.5 Data in Airtable 8.6 Base Design 8.7 Automating Airtable 8.8 Using the REST API 8.9 Data Management", " 8 Airtable This chapter will provide an overview of Airtable and good practices for designing data models in relational databases. For an EHA specific overview of Airtable Enterprise, see this document Request access to the EHA shared workspaces by contacting the Data Librarian or Infrastructure Lead. 8.1 Key Terms Workspace - A collection of bases Base - A database. Each is identified by the Airtable API via a base id Table - A tabular data set within a base. Each table is identified by the Airtable API via the name of the table e.g. “Demo Table” Record - An individual cell within a table. Each record is identified by the Airtable API via a record id Field - A property of data in a table Views - A specific way of displaying a table. Default is grid. Entity - Something that either physically or logically exists whose properties are typically stored in a table and composed of data elements. Element - an attribute of a Entity (a field) 8.2 What is Airtable? Airtable provides a cloud hosted spreadsheet-database hybrid platform for collaborative data curation. The platform is designed to provide features of a relational database (SQL-Like) linked records with an easy-to-use spreadsheet interface. What is more, each database provides a data service that can be accessed via a secure application programming interface (API) by authorized users. This combination of features make Airtable an appropriate platform for people conducting data entry as well as people building technical infrastructure. 8.3 When should I use Airtable? Use Airtable when you would have multiple tabs in a spreadsheet, when you have hierarchical data, and/or when you want to create collaborative automated workflows in a low code environment. While Airtable is good for many things, it is not a good solution for big data. Consider using other systems if you plan to have more than 250,000 records of data in a base. Project Management Because Airtable is extremely useful and user friendly, there are a plethora of pre-built templates for nearly an project management type task. Airtable Templates User Contributed Templates For example, the computational sciences team has had success using Airtable for candidate searches. Airtable allows us to efficiently, independently, and consistently review applications then compare our ratings. We can then sort candidates by average score and reach a consensus on who to follow up with. Research Data Airtable shines when you have multiple data entities that are linked via common data elements. For example, if you are sampling bats then conducting antibody screening, running multiplex PCR, and keeping physical samples of blood and urine from those bats, you have multiple data entities (bats, antibody test results, PCR test results, and physical samples) that are all linked via common data elements like site and collector. What is more, the test results depend on the physical samples and the physical samples depend on the bats. Those common data elements only need to be entered into a single entity (or table) then shared across the other entities in your base using linked elements. Once you have linked elements you no longer have to worry about updating the same piece of information in multiple locations and you can quickly navigate between entities and understand hierarchical relationships. Figure 8.1: Data model for bat sampling example. 8.4 Security, Access Control, and Data Durability Airtable maintains physical and technological security as part of its ISO IEC 27001:2013 and SOC 2 compliance measures. Data are 256-bit encrypted when storing on the server and also when transferring data over the internet. To find vulnerabilities in their software, they run daily, weekly, and monthly scans on different components of their system and regularly commission external penetration tests. They also run a bug bounty program to help identify issues. Their data centers have fire detection and suppression systems, redundant power systems, and strict control for physical access. Because Airtable relies on Amazon Web Services (AWS) for its cloud infrastructure (the same providers used by previous EHA projects), data are geo-redundantly replicated in backups across multiple zones to increase data durability. They also have a team monitoring services at all times. Airtable employees are thoroughly vetted before hiring and continually trained on data protection best practices. Their workstations are secured by using full-disk encryption, automatic locking, and strong password requirements. 8.4.1 User- and Administrator Security Features 8.4.1.1 Access Controls Airtable provides database (referred to as “base”) and workspace administrators with granular controls over who can view, edit, comment, or otherwise modify data at the base, table, and field levels. There are four levels of Airtable user permissions: Owner/Creator: Full administrative control of base Editor: Sees full base, create and modify records and views, create and modify view share links Commenter: Sees full base, comment on records Reader: Sees full base Direct access to a base or workspace is granted or removed by base owners and creators to Airtable users. Base owners and creators can control who has access to a base and can control any “share” links created for that base. They may also restrict editing of tables or fields within a base. Any collaborator given direct access to a base at any permission level will be able to duplicate that base and share that data further. It is important that direct access to the base is limited to individuals with a need to curate or analyze the data. 8.4.2 Share Links and Interfaces To further restrict access to a base, users can be given indirect access via revocable share links or interfaces. Share links can be customized to prevent users from seeing the full base, prevent duplicating the base, and prevent copying data from the base. The ability to use the link can be password protected, restricted to people with certain email domains, and may be revoked at any time. If there are concerns about data leaks via base or table duplication, inviting people with a need to view the data via share links constrains their ability to extract data from the base. Interfaces are dynamic dashboards built on a limited set of data in an airtable base. Users can explore or even edit data based on the permissions provided by the interface creator. Access can be further tuned by setting up a “current-user filter”. See this guide for more information. 8.5 Data in Airtable See Airtable plan comparison for more information on the size of bases and features available. Information in this section pertains to all plans unless specified. 8.5.1 Workspaces, Bases, Tables, Fields, Records Airtable uses workspaces, bases, tables, and fields to manage data. A workspace generally pertains to a particular project and contains all bases relevant to that project. Sharing a workspace with someone allows them to see all bases within that workspace. Bases are equivalent to databases. They consist of a set of tables that can be linked and allow you to perform some task (e.g. IRB tracking, capturing research data, etc.). Bases can be duplicated and shared across your workspaces and you can share bases with other users. Tables are where most of the action happens. Data is entered in tables, tables can be transformed via views into calendars, dashboards, or galleries, and tables can be manipulated via the API. They describe a data entity and are composed of fields. In the bat sampling example, each circle would be a table in the bat sampling base. Figure 8.2: Data model for bat sampling example. Fields represent properties of a data entity. In a spreadsheet view, fields are columns. Airtable allows you to control field types (date, number, text, file attachments, logical, etc.) and paid plans allows you to control who can edit fields. Fields can be linked between tables creating links. Records are the individual data points in a table. In a spreadsheet they would be the rows. Records are shaped by the structure you have created in tables and fields. Each record has a URL that can be used to access it programatically or share it. 8.5.2 Views In Airtable, tables can be displayed in different views to emphasis different components of the data. Views are great for creating concise presentations of data, especially in sprawling tables. The default view in an table is the grid (spreadsheet) view. All other views will derive from the data entered in this view. For more on views, see the guide to views. 8.5.3 Internal Backups: Record History, Base Snapshots and Duplication Airtable has system for tracking changes to a base. They provide revision history for individual records (how long those histories are stored varies by plan). Any comments made in the revision history will be stored for the life of the record (on any plan). The current state of an entire base may be captured through snapshots. Should a systemic issue arise, the base can be restored to a snapshot at a later date. Restoring from a snapshot will remove the revision history for a record but comments on that record will be maintained. As revision histories are maintained, “deleted” data may be retrieved. In the event of the need to permanently remove data, the revision history of the base may be removed. Additionally, you can duplicate a base. This is similar to taking a snapshot except that if the original base is deleted, the duplicated base will persist. See the external backups section for saving data outside airtable. 8.5.4 Importing datad Data can be imported to Airtable from a number of sources including CSV files, excel, Google sheets, XML, and via copy paste. Airtable will guess what the most appropriate field type is, so make sure the field type is appropriate for the data (e.g. convert from text to date type fields). Certain sources, like Google sheets, can be imported as bases. For more on importing data see: Importing and Adding data 8.6 Base Design Having a good base design will make using your data easier. Generally, the process looks like this: Describe what the database will do and collect use cases Determine the roles of various stakeholders List out the entities in the database and define their properties Map out how the entities fit together (which properties link them) Check that the mapping meets the use cases Build base in Airtable Check that the base meets the use cases If you are migrating from spreadsheets, you likely already have an idea of what you need the base to do and a collection of data properties. It is still a good idea to follow the steps outlined above for mapping out entities. You may find that entire sheets can be replaced by views or that the data in one sheet should actually be stored in two different tables. Feel free to reach out to the data librarian for questions about base design. 8.7 Automating Airtable Airtable has five main routes for automating processes. Automations - a drag and drop visual programming tool Extensions - pre-built applications that perform some task Scripting - use JavaScript to automate tasks within Airtable Blocks - use JavaScript to create custom applications REST API - use whatever programming language you like to automate processes 8.7.1 Automations with Drag and Drop Programing (pro and above) The automations feature within Airtable allows you to visually program routines. Each automation has three basic components: Status - controls whether or not the automation will run when the trigger condition is met Triggers - condition for automation to run: the creation or change of a record, a scheduled time, or some other external action Actions - what the automation will do Automations are commonly used to augment the synced tables feature, send notifications, check data quality, and manipulate or create records. This automation creates a weekly summary of applicants who applied for a position. Its trigger is time based, then it finds records that match a condition, and finally it generates an email from those records and sends to the appropriate recipients. airtable weekly summary 8.7.2 Scripting and Blocks (pro plan and above) Scripting uses JavaScript to manipulate a base from within an application in Airtable. Scripting is flexible but has a steep learning curve. The scripting environment Airtable provides can be helpful as it provides code linting, direct access to documentation, and example scripts to build from. The major drawback to scripting is that scripts live in the base and files are not version controlled. There are a LOT of pre-written scripts, in a shared “marketplace” to perform automatic actions. Search the marketplace before you start writing bespoke code. Blocks are custom applications built in JavaScript and node.js that add to base functionality. They are created in a development environment outside of Airtable then brought back into platform. There are number of tutorials for getting started with blocks. 8.8 Using the REST API All Airtable bases are automatically accessible to authorized users via a REST API. The list of API accessible bases you have access to can be found here: https://airtable.com/api. By clicking on a base you will be able to see the full API documentation for that base. 8.8.1 Scoped Tokens ** Remember to save the token in a secure place. Do not store unencrypted tokens on the web (e.g. pushing them to github). Airtable is moving to a scoped tokens based approach to api access. Scoped personal access tokens allow you to create a token for a specific base with specific permissions - e.g. token has read-only access to a bat sampling base. Using scoped tokens in this way means that if the token is compromised (leaked, stolen, accidentally committed unencrypted to a github repo, etc), you can delete that token to remove any access it might have had and the limited scope means that you know exactly what a person would have been able to access. To create a personal access token go here: https://airtable.com/create/tokens Airtable has also deployed oauth tokens for all users. 8.8.2 Airtable and R The Airtable REST API can be used via R with the airtabler package. EHA has started a fork of the package that has additional functionality so it is recommended to use that version. The original package design works well for exploring the data. Our extension adds additional functionality to help use airtabler in automations via continuous integration systems like GitHub Actions. remotes::install_github(&quot;ecohealthalliance/airtabler&quot;) The Airtable API serves up data as JSON, which has a hierarchical structure similar to a list in R. To handle JSON, airtabler uses the jsonlite package. Its helpful to understand how jsonlite handles different JSON structures when working with more complicated Airtable data. See the jsonlite quick-start guide for a basic overview. The purr package is extremely helpful when dealing with data objects derived from JSON because it facilitates navigating nested data structures. The airtabler package provides instructions for setting up access to the Airtable API. You will need to follow those instructions for the following examples to work. One to One join library(airtabler) table1 &lt;- fetch_all(base = &quot;app49bbyLczZxX9PM&quot;,table_name = &quot;One To One&quot;) table2 &lt;- fetch_all(base = &quot;app49bbyLczZxX9PM&quot;,table_name = &quot;Table 2&quot;) ## Linked records are stored as JSON arrays so they become lists ## when part of a data frame. Because each array has a length of 1 ## we can safely unlist the arrays and add them back as to the ## data frame. table1$LinkedRec &lt;- unlist(table1$LinkedRec) joinedTables &lt;- dplyr::left_join(table1,table2, by = c(&quot;LinkedRec&quot;=&quot;id&quot;)) recordKey &lt;- joinedTables[c(&quot;Name&quot;,&quot;LinkedRec&quot;,&quot;Number&quot;)] recordKey One to Many Join library(airtabler) library(dplyr) library(purrr) oneToMany &lt;- fetch_all(base = &quot;app49bbyLczZxX9PM&quot;,table_name = &quot;One To Many&quot;) table2 &lt;- fetch_all(base = &quot;app49bbyLczZxX9PM&quot;,table_name = &quot;Table 2&quot;) # Depending on how you want to work with the data joins are a # little trickier here. ## to replace the values but keep the structure oneToMany$LinkedRecReplace &lt;- purrr::map(oneToMany$LinkedRec, function(x){ table2 %&gt;% filter(id %in% x) %&gt;% select(c(&quot;id&quot;,&quot;Number&quot;)) %&gt;% pull(&quot;Number&quot;) }) data.frame( id = unlist(oneToMany$LinkedRec), label = unlist(oneToMany$LinkedRecReplace) ) 8.9 Data Management Because of its flexibility and ease of use, it is extremely important that data management for airtable be taken seriously. Unlike most other relational databases, the fundamental properties of your base can be changed easily by multiple users without warning! Documenting the structure and purpose of your base, as well as creating regular external backups could save you from catastrophe. 8.9.1 Metadata Structural metadata - Information about a resource that tells you how its put together - e.g. the relationship between a table, field, and automation. With the exception of users on the enterprise plan, you must be deliberate about creating and maintaining structural metadata for your base. Example of Airtable Structural Metadata: Descriptive metadata - Information about a resource that makes it easier to find and attribute data. This includes things attributes like author, title, description, keywords, etc. This table becomes especially important when data are transitioned out of airtable, at the end of a project, or if the base will be shared broadly with collaborators. Example of Airtable Descriptive Metadata: To see the template base, follow this link 8.9.1.1 Metadata for automations and extensions Automations and extensions live entirely inside airtable. As of August 2022, Airtable introduced a “Manage Fields” tab that provides metadata for a specific table, including any automation dependencies. It is not possible to export those dependency tables or the code used in automations automatically. This makes it extremely important to document any automations and extensions outside your base. 8.9.2 External Backups It is a good idea to create regular external backups of airtable data in the event that something catastrophic happens to your base or you simply decide you no longer wish to use airtable as your data store.Unfortunately, airtable does not provide an off the shelf solution for this. Below are three options for extracting all data from your base. 8.9.2.1 Using airtabler Using airtabler::air_dump() and airtabler::air_dump_to_csv() functions, you can export all tables to R then create a versioned folder of CSVs. See the vignette in the airtabler package for generating metadata and creating backups. 8.9.2.2 Manually downloading CSVs Airtable allows you to manually download data as a CSV from each table. When downloading data, make sure you are in the right view. At this time (24 August 2023) there is no “export base to CSVs” feature built into airtable so all tables must be downloaded separately. 8.9.2.3 Other Open Source Projects UnlyEd - use this template to push airtable backups to AWS S3. https://github.com/UnlyEd/airtable-backups-boilerplate 8.9.2.4 Paid services Sequin - replicate airtable to postGres database - https://www.sequin.io/sources/airtable "],["9-versioning.html", "9 Version Control, Git and Github 9.1 Learn 9.2 Install", " 9 Version Control, Git and Github Can I go back to before I made that mistake? Can others see changes others have made to the project and can I see theirs? Version control is essential to long-term project management and collaboration. We primarily use git for this - we recommend it for any project with more than one file of code. It has a steep learning curve but is very powerful. GitHub is a web service for sharing git-versioned projects that has many great tools for collaboration. We have an organizational GitHub account so we can have private repositories and work in teams with shared projects. For projects with little code-based work, there are other options, as well: Google Docs/Word Track Changes are limited to single documents Dropbox can track all files in a shared project/folder Allows one to view/revert to any previous version of a file in the folder Easily sharable Does not travel well - history is lost when project moves elsewhere File histories are independent - does not track interrelated changes. Avoid filename-based version control: 9.1 Learn Git has a steep learning curve and we recommend you spend some time learning rather than only trying to pick it up as you go along. Work through the online book Happy Git with R, which will help you get git and GitHub setup and working with R and RStudio, and teach you some basic workflows. Take the Software Carpentry course Version Control with Git (free) to reinforce some key concepts and learn how work with versions on a day-to-day basis. 9.2 Install Go through the installation steps Happy Git with R’s “Installation” and “Connect” chapters and Appendix B Note when setting up your GitHub account that one account can have multiple e-mail addresses associated with it, so you can split your work and personal stuff without needing multiple accounts (see here). Give Noam (Modeling &amp; Analytics),the Data Librarian or the Infrastructure Lead your GitHub username so they can make you a member of the organizational EHA account and be given access to the appropriate teams. If you are using a GitHub account you previously created with another e-mail, be sure to add your EHA e-mail under Email Settings and set “Custom Routing” under your notification settings so that notifications related to the EHA organization go to your EHA e-mail. Install Dropbox on your computer with your EHA account (note you can have separate personal and EHA Dropbox folders) Check that your EHA email gives you access to Google Drive. If you prefer it, or your supervisor specifies it, install it locally on your computer ##. Personal Access Tokens Github requires you to use personal access tokens (PATs) when authenticating (signing in). These tokens provide specific levels of access to repositories and a user’s github account. The usethis package has a good overview of managing your PATs and some helpful functions that will be highlighted below. Good practices for managing GITHUB PATS: 1) Set an expiration date - github will notify you about expiration 2) Give them an informative name 3) Give minimal permissions - tokens should be able to do no more than necessary 4) Describe the token’s purpose in the notes field 5) Save the PAT in an encrypted file storage system - password managers like 1Password or Bitwarden are a good option 9.2.1 Creating Tokens There are a number of ways to create tokens, see github docs for a comprehensive review. You can create tokens using the function usethis::create_github_token(). This function will open the token management page on github and pre-select a sensible set of permissions. The next easiest way to generate tokens is to navigate to the tokens management page and click generate token &gt;&gt; classic token. There you will be presented with a number of options for permissions. You will probably want to select the “repo” option so that you can read and write to a repository. Historically, Github PATs have been stored in the .Renviron/.env file with the variable GITHUB_PAT. The usethis, gert,and gh packages are moving towards a more secure method for storing credentials. See storing git credentials for more information. 9.2.2 Regenerating Tokens When your token is about to expire, you will get a notification from github. That’s your cue to go to the tokens management page, select the appropriate token, and then click “regenerate token”. Copy and paste that token into your secure storage location and then update your git credentials in R. See PAT maintenance 9.2.3 Useful References Atlassian GIT Tutorials - Nice diagrams and plan language descriptions GIT CLI Cheatsheet GIT Ignore reference Github Learning Resources Git Tower cheat sheet Git basics overview "],["10-reviewing-analyses-and-code.html", "10 Reviewing Analyses and Code 10.1 Learn", " 10 Reviewing Analyses and Code Has my work recieved feedback? Has a second set of eyes checked it for correctness? Have I learned from my colleagues’ work? Just like any piece of writing that you do, your analysis code should be reviewed by a peer or supervisor. There are generally two types of code reviews we engage in: Unit reviews are reviews of discrete, small parts of a project. This might be an analysis that you took a few days or a couple of weeks to complete, and consists of 1-2 files or a few dozen to hundred lines of code. When you complete such a discrete unit, you should solicit feedback. Project reviews are reviews of a whole project as it wraps up, such as prior to the submission of a manuscript. These reviews aim to check that the project is complete, understandable and reproducible. Reviews can be either In person reviews where you go over your code with your team or at our informal science meetings. ScreenHero can also be used for this. Written reviews where a peers place comments in your code or use the commenting and reviewing features on GitHub. or both. 10.1 Learn Check out Fernando Perez’s tips for code review in the lab. Read the Mozilla Guide to Code Review in the Lab Check out some rOpenSci package review examples to look at one kind of code review in action. Best practices for this are evolving. Check out a recent conversation among scientists on Twitter on the topic "],["11-testing.html", "11 Testing 11.1 Learn 11.2 Install", " 11 Testing Is this code doing what I think its doing? Is this data correct? Most code should be accompanied by some form of testing. This scales with the size and type of project. Your work should generally accompanied with testing code or outputs that show that your models behave appropriately, are statisically sound, that your code is running as you expect and your data is checked for quality. 11.1 Learn Test driven data analysis is a neat blog on this subject. There’s a testing chapter in the R Packages book. The vingettes and README files of the packages below are useful. 11.2 Install R packages: assertr or validate for testing that data meets criteria visdat for visually inspecting tabular data. (though there are many ways to plot your data for inspection). testthat for functions and R packages. "],["12-high-performance-servers.html", "12 High-Performance Servers", " 12 High-Performance Servers How can I make this giant beast of a model run faster? EHA maintains internal high-performance servers for Modeling and Analytics work. For security reasons, details on these can now be found in this private repository. "],["13-odk.html", "13 Open Data Kit 13.1 ODK Central 13.2 ODK Collect 13.3 Other useful ODK Collect configurations 13.4 XLSForm", " 13 Open Data Kit This chapter covers documentation on the use and deployment of Open Data Kit(ODK) for data collection. This chapter is divided into three sections: A section on ODK Central, the ODK server type used by EHA for aggregating data collected from forms deployed on mobile devices and/or on the web (using Enketo Express); A section on ODK Collect, the Android-based mobile application used for offline data collection which interfaces with ODK Central; and, A section on XLSForm, a form standard created to help simplify the authoring of forms in Microsoft Excel and/or in any other spreadsheet applications. Most of the material presented in this chapter are summarised from/based on ODK’s official documentation. Wherever appropriate, links to a specific topic on the official documentation are provided as additional resource. For further details on what has been discussed here, please refer to this resource as it is updated regularly with the latest updates and features of ODK. A working group at EHA has pulled together this Tips and Tricks Document. It provides helpful information for working with the EHA ODK system and advice for trouble shooting common issues. 13.1 ODK Central 13.1.1 Getting an EHA ODK Central account ODK Central is the ODK server. It manages user accounts and permissions, stores form definitions, and allows data collection clients like ODK Collect to connect to it for form download and submission upload. EHA hosts its own ODK Central server. To gain access, request for an ODK Central account from the Infrastructure Lead or Data Librarian. Once you have an EHA ODK Central account, you will then have to be associated with either an existing ODK Central project or a specific project will have to be created for which your account will be associated. Be ready to let the Infrastructure Lead or Data Librarian know which project to associate your ODK Central account with or the name of the new project to create. Even if you do not have a new project to start or an existing project that you are involved in, it is still possible to have an ODK Central account so that this account can be easily added to a project later on. However, only an ODK Central account that is associated with a project that can perform some or all of the ODK Central tasks described here. 13.1.2 Using ODK Central The ODK official documentation provides extensive and detailed information on how to use ODK Central - https://docs.getodk.org/central-using/. 13.1.3 Setting up a project Only ODK Central administrators may create projects. To setup a project in EHA’s ODK Central, request one of the admins Infrastructure Lead or Data Librarian) to create a project for you. For them to be able to setup your project, they will need information on: the name you want to give your project; and, the list of team members, along with their respective project roles (either Project Manager, Project Viewer, Data Collector; you will find a detailed breakdown of user roles here, you want added to the project. It is important to remember that the team members that you want added to your ODK Central project will need to be added as ODK Central users (that is, they have ODK Central accounts). Do request the administrator/s for an ODK Central account for team members you want added to the project who do not have ODK Central accounts yet. You will also need to identify at least one of the team members you assign to the project to take on the Project Manager role. 13.1.4 Monitoring the progress of projects ODK Central provides a user interface for project team members (project managers and project viewer roles) to monitor progress of their project/s. When logged on to ODK Central, a project manager or a project viewer will be able to see all the projects they are members of. To view a specific project, a project manager or project viewer can click on that project’s name on the list of projects. A list of all the forms used within that specific project can then be viewed. To view the progress of data collection for a specific form, a project manager or project viewer can click on a specific form’s name on the list of forms. An overview of that specific form can then be viewed. In addition, a project manager or project viewer will be able to view all the submissions that has been sent to ODK Central for this specific form by clicking on the Submissions tab. A project manager or project viewer will then see all the submissions in table format. From this view, the project manager or project viewer will be able to perform specific project monitoring functions. A project manager will be able to comment, review, edit and download data while a project viewer will only be able to comment and download data. The comment, review, and edit capabilities of ODK Central has been added in v1.3. This allows a project manager to monitor data submission by reviewing each row of data available on the ODK Central server for a form, giving it an appropriate label (either approved, has issues, or rejected), adding a comment to explain the label assigned and highlight any problems identified in the specific row of data, and editing the specific row of data accordingly based on review. A project viewer, on the other hand, will only be able to comment on each row of data to provide their feedback and/or to address any comment made by a fellow project viewer or by a project manager. A presentation on these review capabilities of ODK Central can be viewed here. 13.1.5 Retrieving Data Data submissions can be retrieved from ODK Central in two ways: through a manual download via the ODK Central web interface; or, via API interface using a corresponding client. Manual download via the ODK Central web interface can be initiated in the same process described above for accessing form submissions. Once a project manager and/or a project viewer is in the Submissions view, a blue download button is available on the top right corner. Clicking on this button will download all the current form submissions which can then be read or viewed in the appropriate software (e.g., spreadsheet software or read into R). The API interface to ODK Central can be done through any client that can perform OData and/or REST API access. In R, the {ruODK} package is a client designed to access and parse data from ODK Central. A basic workflow for accessing data from ODK Central using {ruODK} is shown below: ################################################################################ # # Example R script for accessing ODK Central data using ruODK package. This # uses a sample form from the EHA ODK Central server for an anthropometric # survey of 6-59 month old children. # ################################################################################ ## Load libraries -------------------------------------------------------------- if (!require(remotes)) install.packages(&quot;remotes&quot;) if (!require(ruODK)) remotes::install_github(&quot;ropensci/ruODK&quot;) if (!require(dplyr)) install.packages(&quot;dplyr&quot;) if (!require(zscorer)) install.packages(&quot;zscorer&quot;) if (!require(nutricheckr)) remotes::install_github(&quot;nutriverse/nutrichecker&quot;) ## Setup connection with ODK Central and the anthropometry form ---------------- ru_setup( svc = &quot;https://odk.eha.io/v1/projects/1/forms/anthropometry.svc&quot;, un = &quot;YOUR_USERNAME_HERE&quot;, ## replace with your EHA ODK username pw = &quot;YOUR_PASSWORD_HERE&quot;, ## replace with your EHA ODK password tz = &quot;GMT&quot;, odkc_version = &quot;1.3&quot; ) ## Retrieve anthropometric data using ruODK ------------------------------------ anthro &lt;- odata_submission_get() ## Calculate anthropometric z-scores ------------------------------------------- anthro_zscores &lt;- anthro %&gt;% mutate(age_days = anthropometry_age * (365.25 / 12)) %&gt;% addWGSR( sex = &quot;anthropometry_sex&quot;, firstPart = &quot;anthropometry_weight&quot;, secondPart = &quot;age_days&quot;, index = &quot;wfa&quot; ) %&gt;% addWGSR( sex = &quot;anthropometry_sex&quot;, firstPart = &quot;anthropometry_height&quot;, secondPart = &quot;age_days&quot;, index = &quot;hfa&quot; ) %&gt;% addWGSR( sex = &quot;anthropometry_sex&quot;, firstPart = &quot;anthropometry_weight&quot;, secondPart = &quot;anthropometry_height&quot;, index = &quot;wfh&quot; ) ## Flag z-scores using WHO criteria -------------------------------------------- anthro_flags &lt;- anthro_zscores %&gt;% flag_who(hlaz = &quot;hfaz&quot;, waz = &quot;wfaz&quot;, whlz = &quot;wfhz&quot;) ## Get a list of rows of data with flags --------------------------------------- anthro_for_checking &lt;- anthro_flags %&gt;% filter(flag != 0) 13.2 ODK Collect ODK Collect is an open source Android app that replaces paper forms used in survey-based data gathering. It supports a wide range of question and answer types, and is designed to work well without network connectivity. ODK Collect renders forms into a sequence of input prompts that apply form logic, entry constraints, and repeating sub-structures. Users work through the prompts and can save the submission at any point. Finalized submissions can be sent to (and new forms downloaded from) a server such as the ODK Central. Collect supports location, audio, images, video, barcodes, signatures, multiple-choice, free text, and numeric answers. It can even accept answers from other apps on your device. 13.2.1 Installing ODK Collect Installing applications such as ODK Collect on to mobile devices such as mobile phones and tablets is a straightforward process. This tutorial describes how to install applications onto Android-powered devices using the Google Play Store. Specifically, this tutorial shows how to install ODK Collect from the Google Play Store on to your mobile device. This tutorial is aimed at IT administrators and/or survey managers or coordinators who are tasked to setup the mobile devices in preparation for data collection. This tutorial assumes that: You have internet connection; You have a Google account; and, You have access to the Google Play Store. Open Google Play Store Search for ODK Collect Install ODK Collect There are situations in which access to the Google Play Store might be limited for a variety reasons. You may not have a Google account which is required to be able to access the Google Play Store; There may be region or country-specific limitations to accessing either your Google account or Google Play Store or both; or, There is simply no internet connectivity in the area you are located in. Such situations would limit your ability to install applications including ODK Collect. This is a problem when the mobile devices you are going to use for your data collection are not yet configured and ODK Collect not yet installed. This tutorial addresses the problem described in situation A and B above where access to Google account and/or Google Play Store is limited but access to internet is not. For situation C, steps 2 to 4 of this tutorial will still apply but it would require that you have already done step 1 ahead of time where access to internet is available which will allow you to download a copy of the application to be installed. This tutorial shows how to install ODK Collect without using Google Play Store (and without requiring a Google account). Instead, the Open Data Kit application package is downloaded onto a computer and then transferred to the tablet for installation. This process requires a computer and a USB to micro-USB cable to connect the tablet to the computer. Download Open Data Kit APK First you need to download the Open Data Kit APK on your computer. APK is short for Android application package which is the package file format used by the Android operating system for distribution and installation of mobile applications. Here are the steps to download the Open Data Kit APK onto your computer. Transfer the ODK Collect APK to your tablet You will now need to transfer the ODK Collect APK to your tablet. This can be done by following these steps. Set-up tablet device to allow installation of applications outside of the Google Play Store You will now need to set-up your tablet device to allow installation of application outside of Google Play Store. Following are the steps you need to take. Install ODK Collect on your tablet Now, you need to install ODK Collect on your tablet using the ODK Collect APK that you just transferred to your tablet. This can be done by following these steps. 13.2.2 Connecting to a server With ODK Central, connecting ODK Collect has become simpler through its use of a QR Code that can be scanned using the mobile device to be used for data collection. Once scanned, ODK Collect will now be configured to communicate with the ODK Central server for retrieving and submitting forms. A detailed description of connecting with ODK Central using QR codes can be found here. There maybe occasions where you are unable to use the QR code functionality to connect ODK Collect with ODK Central. If you are in such a situation, you can connect ODK Collect with and ODK server by manually setting the server settings for ODK Collect as shown here. 13.3 Other useful ODK Collect configurations Once ODK Collect has been installed on the mobile device/s that you will be using for data collection, you may want to configure it’s settings to fully maximize its use and also to make it as easy as possible to use by your enumerators. Depending on how involved you want your enumerators to be in the management of the data collection process, you may want to make available or limit certain settings or functions of ODK Collect. This tutorial describes how to setup the various configuration options of ODK Collect with the aim of minimizing possible unintended interference by enumerators on the various settings that might be detrimental to your data collection campaign. These include the options to load blank forms and change the settings of the server platform (if you are using any). This guide is aimed at IT administrators and/or survey coordinators/managers who will be setting up ODK Collect prior to distribution of the mobile devices to enumerators. This tutorial assumes that ODK Collect is already installed on your mobile devices. The following tutorial shows how to install ODK Collect on mobile devices. 1. Open ODK Collect Find the ODK Collect app icon on your device and tap. You will see the following on your screen. This is the default configuration of ODK Collect when it is first installed on a device and the configurations have not been changed yet. 2. Tap on the ODK Collect settings menu On the upper right hand corner, you will see three square dots on top of each other (1). Click on this and a dropdown menu will appear showing options for ‘General Settings’ (2) and for ‘Admin Settings’ (3). There are three main ‘General Settings’ categories that can be configured: Server Settings (1), Auto Send (2), and User Interface (3). 2.1 Configure Server settings In ‘Server Settings’, there are three main settings that will need configuring: Platform (1), Platform settings (2) and Google account (3). First, tap on Platform (1). You will then be presented with various platform options that you can select depending on the type of server that you are using for your data collection campaign. ODK Aggregate is the default platform. For anything other than the default ODK Aggregate platform that is provided on ODK Collect by default, you will most likely need to select the Other option so that you can specify your own platform for your data collection server. Next, tap on ‘Configure platform settings’ (2). You will then see the following options on your screen. Here you can specify the settings of the server used by your platform specifically the URL of your server (1), the username (2) and password (3) use to login to your server. If you do not know these settings, you should ask your IT administrator or whoever setup your server platform to provide you with these settings. For most service providers offering servers for use by ODK, these settings are made explicit within your account so all you might need to do is to login to your account and find these settings. Next, configure your ‘Google account’ if you have one. This is not absolutely necessary and should be configured if you want to connect your ODK Collect with your Google account (if you have one). The advantage of doing this is that your data can be backed up to your Google account. 2.2 Configure Auto Send settings (not recommended) Now you can configure Auto Send settings. There are two settings to setup: ‘Auto send with Wi-Fi’ (1) and ‘Auto send with network’ (2). By default, these options are unchecked. It is recommended that you keep the default (unchecked) settings in place. This would mean that completed and finalized forms will be saved onto the device memory even when the device is connected on Wi-Fi and will not be sent unless the user specifically sends the forms for submission over the internet. It is good to have it this way because this allows for some type of checking of forms on the tablets themselves before they are sent to the server. This is also good when there is no reliable Wi-Fi and/or network connection or if no server has been setup for the data collection campaign. The option to send the forms by Wi-Fi or over the network can be done later on when there is reliable network and/or when a server has been setup. Otherwise, manual sending of finalized forms via direct mobile device to computer connection can be done. 2.3 Configure User Interface settings Now you can configure the ‘User Interface’ settings. Following are the settings that you can configure or change: Constraint processing behavior (1) - this is the option that determines how ODK Collect processes the checks and constraints that have been specified in your form. There are two options here. The first option is for ODK Collect to process the constraints at the end of the form when you are about to finalize the form. The second option is for ODK Collect to process the constraints every time you move to the next question in the form. The default behavior is to process the constraints after every question. It is recommended that the default option be kept (Validate upon forward swipe). Navigation (2) - this is the option that determines how the user can move from one question to the next in a form. The default behavior is to use horizontal swipes. It is recommended that the option for use of forward/backward buttons be selected instead. Swiping is very user dependent and not all find it easy to use swiping gestures to move to the next question. Buttons, however, are universally acceptable and easy to train enumerators on using. Text font size (3) - this is the option to change the font size used in ODK Collect. By default, this is set to Medium. This default is more than adequate and can be kept as is. Default to finalized (4) - this is the option of whether all forms are by default finalized when you reach the end of the questionnaire. This is checked by default. When you reach the end of your questionnaire, you will see a prompt asking whether you want to mark the form as finalized. This option will be checked automatically because of this setting. It is recommended to keep this setting as is. Delete after send (5) - this is the option of whether forms should be deleted after it has been sent to the server. By default this is unchecked. It is recommended that this option be kept unchecked as it is useful to have the forms kept on the devices as backup. Forms can be retrieved later on manually via mobile device to computer connection. This is especially important in settings where internet is unreliable and a backup of the data on the device is crucial. Enable hi-res video (6) - this is the option of whether hi-res video playback is required. Unless you require video for your forms, it is recommended to uncheck this option. Show splash screen (7) - this option determines if a splash screen on startup of ODK Collect should be shown. This option by default is turned off. It is recommended that this default is kept as is as there is no need for a splash screen and this only delays the startup of the ODK Collect application. 3. Tap on Admin Settings Now, you should configure the ‘Admin Settings’ (3). The ‘Admin Settings’ allow you to control what kinds of items are made available to the user of ODK Collect. 3.1 Setting an Admin password An Admin password restricts access to the ‘Admin Settings’ on ODK Collect. Setting a password blocks access to ‘Admin Settings’ to those users who do not have the password. 3.2 Form processing logic ‘Form processing logic’ settings should be kept as default. 3.3 Configure User Can Access Main Menu Items settings Now to configure the ‘User Can Access Main Menu Items’ settings. Edit Saved Form (1) - this is checked by default. Keep checked. Send Finalized Form (2) - this is checked by default. Keep checked. Get Blank Form (3) - this is checked by default. Uncheck this option. Unchecking this will hide it from the Main Menu. This will prevent unintended pulling of forms from the server that is not needed for your current data collection campaign. Delete Saved Form (4) - this is checked by default. Uncheck this option. Unchecking this will hide it from the Main Menu. This will prevent unintended deletion of forms from ODK Collect. 3.4 Configure User Can Access Change Settings Items settings Now, configure the ‘User Can Access Change Settings Items’ settings. The first five settings are shown here (1-5). It is recommended that all of these be unchecked to avoid unintended changing of settings. The next set of settings are shown here (6-14). All these settings should be unchecked except for ‘Text font size’. This will avoid unintended changing of settings. 3.5 Configure User Can Access Form Entry Items settings Now you can configure ‘User Can Access Form Entry Items’ settings. The settings are shown below (1-6). All options should be kept checked except for ‘Name this Form’ which should be unchecked. 4. Important Notes on Order of Operations You should configure the General Settings first before you change the Admin Settings. This is because the menu items needed to configure General Settings will not be available to you anymore one you have changed the Admin Settings as per recommendation above. This means you should also get the blank forms loaded onto ODK Collect already before changing the Admin Settings because you will not have the option to get blank forms once you have changed the Admin Settings. However, if you need to load forms onto ODK Collect, you can still do so by transferring the XForms version of the form straight into the mobile device from your computer using a mobile device to computer connection. Or you can activate the Get Blank Forms option again so that you can pull the blank forms from your server. 13.3.1 Language settings ODK Collect supports forms written in multiple languages. XLSForm or XForms standard used for creating forms usable in ODK Collect also provide functionality that encodes multiple language support within the same single form being created (for more information, see multiple language support in XLSForm standard). If a form has been created to support multiple languages, ODK Collect is able to provide options for users to change the language of the form as needed. This tutorial describes how to change the ODK Collect language settings. This tutorial assumes that: You have ODK Collect installed on your mobile device. See here for instructions on installing ODK Collect onto your mobile device. You already have created a form using XLSForm standard that has multiple languages encoded. See here for a guide on how to create forms using XLSForm and here for a guide on how to included multiple language support on your form. You have already uploaded your form that supports multiple languages onto ODK Collect. This tutorial is aimed at IT administrators and/or survey managers or coordinators to allow them to change language settings on ODK Collect in time for data collection. 1. Open a blank form that supports multiple languages Tap on ODK Collect then tap on ‘Fill Blank Form’. You will then be shown a list of forms that are uploaded onto your ODK Collect. Tap on the form that you want to use in ODK Collect. After tapping on the form you want to use in ODK Collect, you will be presented with the ‘cover page’ of the form that shows: The form logo if it has been provided by the creator of the form and if the logo has been uploaded into ODK Collect as well. In the example below the form logo has been provided (1). An introductory message (2) saying that you are at the start of the form that you selected and instructing you to tap the arrow buttons below to go backward and forward on the forms or to swipe backward and forward depending on how ODK has been configured (see here regarding ODK Collect settings and configuration). Tap on the forward button (or do a forward swipe) to see the first page of the form and to have an idea of what the form’s default language is (3). You now get to see the first page (1) of the form that you have selected. ODK Collect will always display the form in the default language set for the form. Hence, the language that you see being used in the form in the first page is the default language of the form. Now, tap on the backward button (2) to get back to the ‘cover page’ of the form. 2. Go to ODK Collect’s Change Language settings Once you are back to the ‘cover page’, look at the top right corner of the screen. Here you will see an icon with three square dots on top of each other (1). This is the button that allows you to change the settings of ODK Collect including the ‘Change Language’ settings (2). Once you have located this, tap on ‘Change Language’. 3. Select language you want to use for your form You will then see the following dialog box giving options of the languages supported by the form. The selected language is the language that is currently being used by the form. Choose the language that you want to use for the form. 4. Check the language now used by the form You can now check the language being used by the form by tapping on the forward button again from the ‘cover page’. You will now see that the form is displayed in the language that you have chosen. 13.3.2 Localizing ODK Collect The ODK Collect application for devices running Android can be localized into several different languages. Currently, the application has been fully translated into 11 different languages and is partly translated into 41 other languages. ODK Collect language localization is dependent on the language localization of the mobile device to which ODK Collect is installed in. The language to which the mobile device is set will be the language to which ODK Collect will localize to. This tutorial shows how to change the language settings of a mobile device running Android. 1. Go to settings On your tablet device, tap on the ‘Settings’. You will then see the following on your tablet screen. There are four (4) tabs on top of the screen named 1) Connections; 2) Device; 3) Controls; and 4) General. These are the four settings categories that can be configured as needed. 2. Tap on ‘Controls’ To change language settings, tap ‘Controls’ (1), then tap on ‘Language and input’ (2) and then tap on ‘Language’ (3) You will then see the following Language options on your screen. 3. Select language to set device to Change the language to the language you want to set the device to. Setting the default language of your device to a specific language will affect applications installed in your mobile device that also have language localization features. For these applications, their default language will change to the language of the mobile device. This will be the case for ODK Collect. Since you have changed the default language of your device, ODK Collect will now use that language as the default language for the application. This means that the menus and the text in ODK Collect will now all be in the language you chose. It should be noted, however, that this language localization only affects the text in the ODK Collect application. This language localization will not affect the default language that is set in the forms that are loaded into ODK Collect. So, for example, if a form that is created in English is loaded onto ODK Collect that is localized in Arabic, the form will still show in English and not in Arabic. In order for this form to be localized into Arabic, the form should have been authored in both English and Arabic and then the language settings for the form is set into Arabic. 13.4 XLSForm XLSForm is a form standard created to help simplify the authoring of forms in Excel. Authoring is done in a human readable format using a familiar tool that almost everyone knows - Excel. XLSForms provide a practical standard for sharing and collaborating on authoring forms. They are simple to get started with but allow for the authoring of complex forms by someone familiar with the syntax described here. The XLSForm is then converted to an ODK XForm, a popular open form standard, that allows you to author a form with complex functionality like skip logic in a consistent way across a number of web and mobile data collection platforms. Extensive documentation on the XLSForm standard can be found here. Here is a list of some XLSForms that provide good examples of specific approaches to form authoring for particular uses/applications: A basic XLSForm that shows most of the useful parameters for authoring and shows basic question types - The XLSForm can be viewed here. An XLSForm that demonstrates the various question types available using this form standard - The XLSForm can be viewed here An XLSForm that performs randomization of options for select_one or select_multiple types of questions - The XLSForm can be viewed here An XLSForm that demonstrates the use of the rank widget which allows the ranking of responses to select_multiple question types - The XLSForm an be viewed here Grid style layout that mimics an actual paper form. This XLSForm theme is specifically for use on a web client rather than on ODK Collect and presents the form as if it was a paper-based form with questions or fields laid out in a grid - The XLSForm can be viewed here. "],["14-cloud-computing-services.html", "14 Cloud Computing Services 14.1 Setting up Amazon Credentials 14.2 Using Amazon S3 Storage from R", " 14 Cloud Computing Services What if my data files are too big to hold in Github? Where is the latest IUCN/MODIS/LANDSAT shapefile? What if I need more CPUs than even our servers have? EHA has an organizational Amazon Web Services and Microsoft Azure accounts that may be useful for some projects. AWS S3 storage or Azure Blog Storage are useful for hosting large data files, especially if they are shared across projects. AWS EC2 cloud computers or Azure Virtual Machines may be appropriate for a hosting a web app or database, automating regular processes, or other analytical projects. If you think you need cloud resources for your project, contact Noam or Robert Young for access to the AWS account, to discuss what services or other providers may be useful. If you are using AWS, remember: All resources used on AWS need to be tagged with a project:PROJECTNAME tag in order to assign costs to the appropriate EHA projects. Be judicious with AWS service usage. It is easy to run up costs. 14.1 Setting up Amazon Credentials To connect to AWS via program like R, you will net to set up an access access credientials. Once you’ve gotten your credentials, log on to ehatek.signin.aws.amazon.com/console/. ehatek should be already entered in the “Account ID or alias” field. You will be asked to change your password after the first time you log on. 14.1.1 Generate an access key Navigate to your IAM (Identity and Access Management) page to generate an access key. You should see a ✅ next to “rotate your access keys.” Click “Manage User Access Keys” after expanding the access key subheading. (If you see an ❌, ask whoever created your account if you have the appropriate S3 privileges.) Click the “Security credentials” panel and scroll down to the “create access key” button. Your access key will have a key ID - a long string of letters and numbers - and a *~*secret*~* access key - a longer string of letters and numbers. Press “show” and hang onto it somewhere until you are sure your ~/.aws/credentials file is working as intended. Don’t share your secret key with anyone. If you lose it before you use it, you can delete your and make another one. Deleting a key is different from making a key inactive - you might reach your access key limit pretty quickly, so you’ll probably have to do the former if you have keys that you don’t use. 14.1.2 Create a credentials file Open a blank text file in a text editor and paste your key ID and secret key as follows. It will look like this: [default] aws_access_key_id = your_key_id aws_secret_access_key = your_secret_access_key aws_default_region = us-east-1&quot; Replace your_key_id and your_secret_access_key with the values you see in the AWS browser. Create a folder called ~/.aws, and put the credentials file in it. (YES, currently, you need that trailing \" after us-east-1 if you want to include the AWS_DEFAULT_REGION in your credentials file, because the aws.signature is using a strange parser.) If you get errors related to the AWS default region, you can delete that line and set it locally inside your R environment like this: Sys.setenv(&quot;AWS_DEFAULT_REGION&quot; = &quot;us-east-1&quot;) 14.1.3 Test your Key Open an R session to make sure your credentials file works. # install the AWS packages if you need them if (!require(aws.s3)) devtools::install_github(&quot;cloudyr/aws.s3&quot;) if (!require(aws.signature)) devtools::install_github(&quot;cloudyr/aws.signature&quot;) library(aws.s3) # load credentials from your credentials file aws.signature::use_credentials() # check list of aws S3 buckets head(aws.s3::bucketlist()) If you get a ‘403 Forbidden’ error, your credentials aren’t working. 14.1.4 Alternate credentialing method If you have Done Your Googles and are still having trouble setting your AWS credentials, you can set them in your .Renviron file. The advantage of using an ~/.aws/credentials file is that your key will then work across all platforms - R, Python, the command line, etc. However, if you’re only connecting to AWS through R, having your key in your .Renviron will be enough. # If you don&#39;t know how to find your .Renviron file, do the following to open it: if(!require(usethis)) install.packages(&quot;usethis&quot;) usethis::edit_r_environ() # add the following to your .Renviron, replacing your_key_id and your_secret_access_key with their actual values. Save your .Renviron and restart R. &quot;AWS_ACCESS_KEY_ID&quot; = your_key_id &quot;AWS_SECRET_ACCESS_KEY&quot; = your_secret_access_key &quot;AWS_DEFAULT_REGION&quot; = us-east-1 After you restart R, test that your credentials are working with aws.s3::bucketlist(). 14.2 Using Amazon S3 Storage from R Above, you used bucketlist above to list all the buckets associated with your account. To look at everything inside a specific bucket, use get_bucket. Note that Amazon charges per Gigabyte for downloading and uploading files. Use things as needed for your project, but don’t get crazy and put a file download inside a for loop that you’re running a thousand times a day. To test privacy privileges are working are intended, we’ll use a very small private bucket with ~4 KB in it - Two small subsets of the Social Security Administration’s baby names dataset from the babynames package. # check list of aws S3 buckets bucketlist() # aws.s3::bucketlist() pb &lt;- &quot;eha-ma-practice-bucket&quot; b &lt;- get_bucket(pb) 14.2.1 Save objects from S3 to your machine One of the main reasons we use Amazon S3 is to make sure everyone is working with the same large data files. Usually, you’ll download the large files you need one time and keep them on your local machine for analyses. This can be done with save_object. Wrapping save_object in an if() clause checks to see if the dataset exists first before downloading. # make a data folder if this is the first dataset you&#39;re downloading into this project if(!dir.exists(&quot;data&quot;)) { dir.create(&quot;data&quot;) } # save the babynames_subset.csv object (b[[1]]) into local memory if(!file.exists(&quot;data/babynames_subset.csv&quot;)) { save_object(object = b[[1]], bucket = pb, file = &quot;data/babynames_subset.csv&quot;) } dir(&quot;data&quot;) babynames_subset.csv is now in your data directory.You can then read it into memory as you would normally. 14.2.2 Upload objects from your machine to S3 To put a file from your computer back into the bucket, use put_object. This will be less common than downloading and saving objects from S3 - as a general rule, we want everyone working with the same versions of large datasets. If you’ve made a large rasterfile that your colleagues will use, you can upload it, In this example, we’ll make a change to the babynames_subset and save it as a new file (babynames_subset2). babynames_subset2 &lt;- babynames_subset %&gt;% group_by(name) %&gt;% summarize(n = sum(n)) %&gt;% filter(n &lt; 100) # save to csv write.csv(babynames_subset2, file = &quot;data/babynames_subset2.csv&quot;, row.names = FALSE) # check whether an object with this name is in the bucket, and if it isn&#39;t, put it in there if(!(&quot;babynames_subset2.csv&quot; %in% dplyr::pull(get_bucket_df(pb), &quot;Key&quot;) ) { put_object(file = &quot;data/babynames_subset2.csv&quot;, object = &quot;babynames_subset2.csv&quot;, bucket = pb, acl = &quot;private&quot;) } So now we have a data frame in memory called babynames_subset2 and a .csv in the AWS eha-ma-practice-bucket called babynames_subset2.csv. We’ll pull the AWS csv back into memory to make sure they’re the same. To do this, we have to first copy over babynames_subset2 to a new object in memory and remove it so that when we load the version from AWS, it doesn’t overwrite it. babynames_subset_R &lt;- babynames_subset2 rm(babynames_subset2) # save the .csv object using &quot;save_object&quot; tmp = tempdir() file = paste0(tmp, &quot;/babynames_subset2.csv&quot;) save_object(&quot;babynames_subset2.csv&quot;, bucket = pb, file = file) # read the csv file into memory babynames_subset2 &lt;- read_csv(file) Finally, we can check that the object in memory and the .csv in AWS are the same. assertthat::are_equal(babynames_subset_R, babynames_subset2) "],["15-google-authorization-and-r.html", "15 Google Authorization and R 15.1 The basic overview: 15.2 Key Terms 15.3 Before we start: 15.4 Setup encryption tools on your machine 15.5 Enable git-crypt on your repository 15.6 Setting up non-interactive authentication for Google sheets 15.7 General approach to securely managing keys 15.8 Additional Resources", " 15 Google Authorization and R IMPORTANT DO NOT ADD SENSITIVE FILES TO A GITHUB REPO UNTIL THEY ARE ENCRYPTED! EcoHealth Alliance sometimes uses Google Drive, and Google Sheets in particular, to store and collaborate on data. Working with Google Drive-based files in R is relatively painless thanks to the googledrive, googlesheets4, and gargle packages. What is less straightforward is working with drive based files without having to manually authenticate your identity. In this chapter, we will walk through the process of creating credentials with API access that can be used in your R project or package. Ultimately, this could allow you to fully automate your Google-centric data pipelines. 15.1 The basic overview: Create or store something (sheet, csv, doc, etc.) in Google drive Create a Google Cloud project to manage Google services Enable the appropriate APIs for the project so it can access things like Drive and Sheets. Create a service account so that you can access the APIs via credentials from R Encrypt credentials then add them to your R project so that you can still use git-based workflows without leaking access to your service account Share Google-based resources with the service account and check that credentials work as expected from R Add an encryption key to Github as an environment variable so that R can access the resources in automated workflows 15.2 Key Terms Authentication - Confirms the identity of an entity Authorization - Permits an entity to do something Auth - shorthand for authentication or authorization Key or Token- Computer-generated credentials that allow for authorization and authentication. In the R-Google universe key and token are synonyms, though not all services use this way, at at times okens and keys are communicated over the web using different method. You will see these terms used interchangeably in tutorials. Symmetric encryption - A type of encryption that uses a single key to encrypt and decrypt an object Asymmetric encryption - A type of encryption that uses public and private keys to encrypt and decrypt an object Environment variable - A value stored in a computer’s system environment. In R, this generally means values stored in the .Renviron file, which can be brought into your project using Sys.getenv(\"Variable_Name\") and are very useful for storing sensitive information like tokens and keys. Service - Functionality provided by another system i.e. serving data via an API. GCP - Google cloud platform. Web services from Google. 15.3 Before we start: Note: The preferred method for adding encryption to projects is via git-crypt. See chapter 16 for more on git-crypt. Credit: This chapter largely follows the non-interactive auth vignette from the Gargle R package, but diverges for package and non-package focused projects. What about Billing?: Good question. This is not an issue for Google Sheets or Drive APIs but you do need a linked billing account for BigQuery and Maps APIs. If you’re new to GCP as of 29 Sept 2021 you get $300 of credits in the Free Tier. If you use the $300 in credits GCP will ask for consent before billing. Check with the Data Librarian about using and billing arrangements beyond this. 15.4 Setup encryption tools on your machine See chapter 16. This process will take ~30 mins and involves using the command line. 15.5 Enable git-crypt on your repository Enabling git-crypt requires your code to be stored in a git-backed repository. See chapter 9 for setting up git repositories. Enabling git-crypt happens from the command line. You can access the command line directly in Rstudio. If you use rstudio projects and the command line in Rstudio, then the terminal should open in the repo you want to encrypt. In the command line run: # check that you are in the directory with the repo pwd ## /path/to/repo-i-want-to-encrypt # if you&#39;re in the wrong directory, use cd to navigate to the correct repo # cd /path/to/repo-i-want-to-encrypt git-crypt init Next you want to tell git-crypt which files should be encrypted. To do this, create a file in the top level directory of your repo called .gitattributes. Here you will list the files and folders you would like to encrypt. Each item should be placed a on separate line. To learn more about pattern matching in the .gitattributes file, see the git-crypt read.me and gitignore manual Your .gitattributes file might look something like this: .env filter=git-crypt diff=git-crypt auth/** filter=git-crypt diff=git-crypt .gitattributes !filter !diff The .env file will be used to store environment variables and the auth/ folder will be used to store keys. Do NOT encrypt the .gitattributes file. It maybe a good idea to add your google auth key explicitly to the .gitattributes file using this format **/mySecretKey.json so that the key is encrypted independent of the directory it is in. Next add yourself and other users who will require access to the encrypted files to the repo. The encryption chapter encryption in the handbook details how to add contributors to the repo. Finally, you will have to set up a symmetric key for github actions to use. This key can always be regenerated so there is no reason to store it. Additional detail can be found in the encryption chapter here. First, add the git-crypt key to your .gitignore so you don’t accidentally commit it to the repo. .Rproj.user .Rhistory .Rdata .httr-oauth .DS_Store inst/.DS_Store git_crypt_key.key In the command line run: # create the symmetic key git-crypt export-key git_crypt_key.key # convert it from binary to bas64 so github can use it # the file&#39;s contents can now be pasted into a github secret # environment variable cat git_crypt_key.key | base64 | pbcopy Paste the key into Github’s secret environment variable field as GIT_CRYPT_KEY64. Now delete the key file: # doesn&#39;t have to be done in terminal but you&#39;ve already got it open rm -i git_crypt_key.key You have now setup asymmetric encryption for human users and symmetric encryption for automations. The next steps involve getting the files you want to encrypt. 15.6 Setting up non-interactive authentication for Google sheets In this section, we will walk through setting up credentials that can be used in R to access Google sheets without manual authentication. To achieve non-interactive authorization, we want to either provide a token directly to a service or make a token discoverable for a service. A token is essentially a long password, designed to be exchanged by machines but too long and complexly formatted to be used by people, and often time-limited. Remember that tokens, secrets, and API keys should be stored in a secure fashion (NOT stored in the text of your code or in unencrypted files). We are going to follow the recommended (as of 29 September 2021) strategy of providing a service account key directly to handle authorization. A newer approach called “workload identity federation” exists as of writing but is not fully implemented in the gargle package. 15.6.1 Create a Google cloud platform project “Google Cloud projects form the basis for creating, enabling, and using all Google Cloud services including managing APIs, enabling billing, adding and removing collaborators, and managing permissions for Google Cloud resources.” - GCP Docs We will use a GCP project to access the Google sheets API via a service account. You do not need a profound understanding of GCP projects to setup a service account. Setup/view your Google cloud account Create a project on Google cloud to hold your credentials The GCP console is your destination for monitoring and modifying your projects 15.6.2 Enable APIs GCP Projects are centered on the idea that a single project will contain a single application. In our case, the application we are creating relies on the Google Sheets API. You can enable API’s for our application to access via the APIs &amp; Services menu item. In the left side menu, navigate to APIs &amp; Services &gt; Library Choose your api of interest. For this example it is Google sheets. Enable the api of interest. If you need to enable more API’s later you can always come back. 15.6.3 Create a Service Account Service accounts allow applications, like the GCP project we make, to access certain resources they need via authorized API calls. The service account’s access can be limited such that it can only access specific resources in a certain way. Importantly, service accounts are not part of the EHA workspace domain. You have to manually share resources like Google sheets with a service account even if you have provided domain-level access. GCP Service Account Docs Navigate back to your project homepage In the left sidebar go to IAM &amp; Admin &gt; Service Accounts Click create service account Give it a good name and description For Google sheets, we do not need to assign our account service a role Roles can be established to perform tests and otherwise manage the service but are not necessary Also not necessary to grant user access for this example You may have a need for this with more complicated services It may also be a good idea to get some redundancy in your workflow 15.6.4 Create a Key for your service account Keys for Google service accounts are stored in JSON files. Remember that this key will hold very sensitive information and we should treat it like a username and password combo. Click on the appropriate item in the service accounts table. Notice that it says no key. Click on the keys tab, then click on ADD KEY Select create new key and download the JSON file. WARNING: Do not store this unencrypted key in a shared location (Dropbox, Google Drive, folder connected to a git repository). If you are using the git-crypt workflow, add the file to the “./auth” folder in your project’s working directory. You should now see that there is an active key associated with your service account in the GCP project. 15.6.5 Share the Google drive resources of interest with the service account Make sure your sheet is shared with the service account Remember that service accounts do not belong the EHA domain so there will be extra prompts when sharing resources. 15.7 General approach to securely managing keys This workflow uses the encryption practices laid out in the encryption chapter. Make sure you have setup your computer with encryption tools before proceeding. Reach out to the Data Librarian with questions. Unencrypted files storing keys for the Google service account should NOT be stored in shared or public locations (Drive, Dropbox, Github Repo) If possible store in an encrypted volume. Keybase, Bitwarden, or other credential management storage systems generally allow you to store files in an encrypted manner. Files storing keys for the Google service account only ever enter the project working directory after being encrypted or after git-crypt has been initiated and the file is included in the .gitattributes file or stored in an encrypted folder like ./auth. See the [encryption chapter][Set up encryption for a repo that did not previously use git-crypt.] for more details 15.7.1 Provide a service account key for projects Now you have a secret key for the service account and need to securely access it in an R project. If you setup git-crypt, make sure the file is saved in an appropriate location (e.g. ./auth) and that all the users who need to use the encrypted file are added to the repo. 15.7.1.1 Run your code using encrypted keys For git-crypt users, the file will already be decrypted on your machine. If it isn’t, run git-crypt unlock in the terminal. If that does not work double check that your public key has been added to the repo. jsonChar &lt;- readr::read_file(&quot;./auth/myKey.json&quot;) googlesheets4::gs4_auth(path = jsonChar) 15.7.1.2 Read in your sheet Make sure your sheet is shared with the service account if it hasn’t been already googlesheets4::read_sheet(&quot;https://docs.Google.com/spreadsheets/that/i/definitely/shared/with/the/service-account&quot;) 15.7.1.3 Add Environment Variable to other services Non-interactive authentication allows us to automate workflows that involve Google sheets on services like Github Actions. In either case, it is relatively straight forward to securely add our SERVICE_ACCOUNT_PASSWORD environment variables to those services. See documentation here: - Add secrets to Github Actions 15.8 Additional Resources 15.8.1 Google Drive The googledrive package allows you to interact with files stored in Google drive from R. You can download, share, delete, copy, publish and otherwise manipulate files on your drive using this package. Package vignettes can be found here 15.8.2 Google Sheets The googlesheets4 package allows you to directly interact with Google sheets in R. You can read, write, reformat, create formulas, and otherwise manipulate the sheet of interest. Package vignettes can be found here "],["16-encryption.html", "16 Encryption 16.1 Set up Keybase 16.2 Install gpg and git-crypt 16.3 Create your Keybase keys 16.4 Import your keys to your local keychain 16.5 Configure gpg 16.6 Backing up and Recovering GPG Keys 16.7 Use git-crypt to unlock a repository 16.8 Managing which users can decrypt files in a repository 16.9 Use a symmetric key for automated processes 16.10 Removing sensitive files from git history", " 16 Encryption How do we share data in git repositories that needs to be secure? Sometimes we need to store and share secure information, such as passwords or API keys, to online service accounts. One of our methods of choice for this is to keep these files stored in git/GitHub repositories, but to encrypt them. We do this using PGP (Pretty Good Privacy) encryption, implemented by the program git-crypt. It takes a bit to set up but once activated makes sharing secure and seamless. The PGP encryption scheme involves making a public key that you share and a private key that you use to decrypt data encrypted with your public key. We also use Keybase, a service that helps you publish and verify a public key for this purpose. Instructions for setting this up are below. 16.1 Set up Keybase Sign up for Keybase, and follow the instructions for installing it on your computer. Note that if you switch computers, you need to have another device associated with the keybase account and/or a copy of your paper key in order to recover your key. A password manager like 1Password) - EHA provides account, (Bitwarden would be helpful for securely storing a paper key. 16.1.1 Installing Keybase on Linux For installing on linux, first identify the distribution by entering the following command into a terminal and noting down the Distributor ID. lsb_release -a Next identify the architecture via, arch Follow the instructions to install Keybase on Linux, available here, making sure to use the section relevant to the architecture and distribution information identified above. 16.1.2 Installing Keybase on macOS and Windows For installing on Windows and macOS, the easiest way is to download and install the graphical user interface available from the Keybase download page. This will also install the necessary command line tools. You may also use homebrew to install keybase on macOS 16.2 Install gpg and git-crypt gpg is the program that implements encryption, and git-crypt sets up git repos for encrypted sharing using gpg. These are already installed on EHA servers. 16.2.1 Install gpg and git-crypt on macOS Use homebrew to install gpg, git-crypt. You should also install pinentry, which is a helper program for entering passwords securely. Run the following in the terminal: brew install gpg brew install pinentry brew install git-crypt Homebrew automatically updates when you run it so if you haven’t used it in a while there may be a somewhat lengthy update 16.2.2 Install gpg and git-crypt on Windows For Windows there are two alternative approaches. 16.2.2.1 Using Windows Subsystem for Linux (WSL) The first approach is to utilize the Windows Subsystem for Linux (WSL). This method requires Windows 10 or higher. You must first install WSL. Do so following this guide: https://docs.microsoft.com/en-us/windows/wsl/install. Once WSL is set up, the necessary packages can be installed through the WSL command line shell. Run the following in the shell: sudo apt update sudo apt install keybase gpg git-crypt 16.2.2.2 Using Windows: install binaries The second method is to install pre-compiled Windows binaries for GPG and git-crypt. First download a Windows-compatible binary for GPG which can be found (here)https://gnupg.org/download/. The ‘Simple installer for the current GnuPG’ binary on that page is the recommended choice. Then install git-crypt by via the following steps: Downloading git-crypt-*.exe from https://github.com/AGWA/git-crypt/releases. This may generate the warning “git-crypt-*.exe is not commonly downloaded and may be dangerous”. Click the up arrow next to ‘Discard’ and select ‘Keep’. Even after this the download may fail with the message ‘Failed - Virus detected’. Do not worry this is a false positive. If this occurs, search for ‘Virus &amp; thread protection’ in the task bar and click on ‘Manage settings’ under ‘Virus &amp; thread protection settings’. Once there, turn off “real-time protection” and try downloading again. Please make sure to turn it back on again when done. Once downloaded, rename the file to gpg-crypt.exe. Move the resulting gpg-crypt.exe into a folder recognized by the Windows PATH environment variable. A convenient location is C:\\Program Files\\Git\\cmd\\. Once Keybase and GPG are installed, the terminal commands related to exporting keys from Keybase into GPG are the same regardless of operating system. 16.3 Create your Keybase keys If you are just starting to use Keybase, you can generate new keys for use on your computer using this guide: https://github.com/pstadler/keybase-gpg-github. That guide also helps set up using your key to sign GitHub commits, which you should do for added security. Create a password associated with your keys when asked. You can store this in your password manager such as 1Password or BitWarden. Once your keys are created, visit your Keybase account at and verify your keys via as many other services, devices, or online identities as you want. We suggest at least three. It is also a good idea to generate a physical ‘paper key’ and store it in a secure location. 16.4 Import your keys to your local keychain If you have a Keybase account set and keys already generated, you can now import your Keybase keys to use. Instruction are found at https://blog.scottlowe.org/2017/09/06/using-keybase-gpg-macos/. When followng those instructions, set your keys to maximum trust level. 16.5 Configure gpg 16.5.1 Configure gpg on macOS (and Linux) A common source of errors for macOS (and Linux) users is that the text entry for gpg isn’t set properly. This means that gpg and your terminal aren’t speaking the same language. You can fix this by setting the GPG_TTY environment variable in your shell configuration. export GPG_TTY=$(tty) Adding this to your .profile, .bashrc, .zshrc or other settings files prevents having to run the command when you use git-crypt or sign commits. U Use a text editor to modify the settings file for your shell. These are set in one of the files ~/.profile, ~/.bashrc, ~/.zshrc. For most macOS users, it is ~/.zshrc. Here are instructions using nano, an editor available on most machines. In the terminal, run: nano ~/.zshrc Then, in the nano text editor that comes up, add the following line to the file: export GPG_TTY=$(tty) In the nano editor, press Ctrl-O to write (“write-Out”) to save your changes, then press Ctrl-X to exit. Alternatively, run the following line in the terminal to change your .zshrc file without using nano or any other editor: echo &quot;export GPG_TTY=$(tty)&quot; &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc 16.5.2 Configure gpg on Windows Windows needs to inform git of the location of the gpg executable. This can be done by opening cmd or PowerShell and entering the following command: git config --global gpg.program &quot;C:\\Program Files (x86)\\GnuPG\\bin\\gpg.exe&quot;` Note that if GnuPG is installed in a different location the command should be altered to reflect this change. 16.6 Backing up and Recovering GPG Keys GPG private keys are stored locally and vulnerable to loss in the event of a problem with a user’s local machine. There are two potential ways to do this, and a belt-and-suspenders approach is best: Use KeyBase to host an encrypted version of the key. host private key on keybase Store a copy of your private key in 1Password 16.6.1 How do I save a key? First, do not commit the private key to github. Consider adding prv.key to the .gitignore file. Run the following commands to export public and private keys to file: # Get list of keys on your machine. Replace USER NAME with the name associated with a listed key. gpg --list-keys # Save private key to a file -- DO NOT COMMIT TO GITHUB # You will prompted to enter your GPG password - its the same # password you would enter to sign commits gpg --export-secret-key -a USER NAME &gt; prv.key # Save public key to a file gpg --export -a USER NAME &gt; pub.key Move private key to encrypted storage locations (e.g. keybase and 1password) Delete the private key from your machine to avoid leaks 16.6.2 How do I recover my gpg key from a file? First download the key from one of your back up locations. Then run the following commands: # import the key gpg --import prv.key # list keys to get public {KEY} gpg --list-keys # set the trust level gpg --edit-key {KEY} trust quit # enter 5&lt;RETURN&gt; (I trust ultimately) # enter y&lt;RETURN&gt; (Really set this key to ultimate trust - Yes) # list keys to check that your key is trusted gpg --list-keys 16.7 Use git-crypt to unlock a repository The git-crypt README outlines the basics of using git-crypt to encrypted and decrypt files in a git repository. First be sure your public key has been added to the repository. To do this, check the .git-crypt/keys/default/0 folder in the github repo for your public key. If your key is present pull or clone the repo then run, git-crypt unlock from the terminal or command line in the repository folder. If all goes well, congrats! Encryption and decryption for pushing and pulling should now happen automatically. 16.7.1 Troubleshooting If git-crypt unlock fails, try the following steps: Verify that GPG has successfully imported your private key from Keybase by opening a shell and entering gpg --list-secret-keys. If no keys are found, follow the guide for importing keys. This might fail for mac and linux users if export GPG_TTY=$(tty) is not in your .bashrc or .zshrc. Make sure that your GPG private key has actually been added to the repository. Navigate to the ~/.git-crypt/keys/default/0/ directory on github and look for a file that matches your public key. Once your key has been added, pull from the remote to make sure the key is available to your local repository. Windows users need to let git know where to find the gpg executable. Double check that the gpg.exe really is in the folder you specified above. If not find it’s location or try re-installing gpg. Make sure that GIT knows about your signing key 16.8 Managing which users can decrypt files in a repository If you’ve made it this far and you only need to unlock a repository that has already been set up you’re done! The instructions below will help you go further by outlining how to grant access to encrypted files on a repository, add your key to the EHA servers, and to initialize a new repository to use git-crypt. 16.8.1 Set up encryption for a repo that did not previously use git-crypt This will initialize the repository and add the default gpg key to git-crypt. Note: adding encryption to a repository will only encrypt files going forward. Any previous versions in the commit history will still be un-encrypted. Best practice is to set up git-crypt first, add relevant file or folder names to the .gitattributes file, and only then add any sensitive files to the repo. ## In the repo base directory open the terminal or command line and enter: git-crypt init ## To verify that files are being encrypted run: git-crypt status # add your key to git crypt git-crypt add-gpg-user YOUR_KEY_HERE # create .gitattributes file - will tell git crypt what should be encrypted touch .gitattributes # tell git crypt that .env should be encrypted echo &#39;**/.env filter=git-crypt diff=git-crypt&#39; &gt;&gt; .gitattributes # may also be a good idea to add **/auth if you&#39;re going to be # using non-interactive processes with dropbox and google drive echo &#39;auth/** filter=git-crypt diff=git-crypt&#39; &gt;&gt; .gitattributes To learn more about pattern matching in the .gitattributes file, see the git-crypt read.me and gitignore manual Your .gitattributes file might look something like this: .env filter=git-crypt diff=git-crypt auth/** filter=git-crypt diff=git-crypt .gitattributes !filter !diff 16.8.2 Allow contributors access to encrypted files First add their public key to your keychain. Visit their Keybase profiles (e.g., https://keybase.io/noamross) and click on the key - it will show several ways to import the keys. Two methods are shown below, # curl + gpg pro tip: import noamross&#39;s keys curl https://keybase.io/noamross/pgp_keys.asc | gpg --import # the Keybase app can push to gpg keychain, too keybase pgp pull noamross Next edit the key so that it has sufficient trust levels as described in this guide. # in terminal gpg --list-keys ## Copy the key that matches the individual you want to allow access to ecnrypted files. gpg --edit-key &lt;keyID&gt; # At the gpg&gt; prompt, use uid X to select the user ID you want to mark as trusted, # then use the trust command to set the trust level. # Use save to exit when you’re done. . Edit that key so that it has sufficient trust levels as described in the above Add the key to your git repo (make sure you’re in the right directory) ## this will automatically commit changes. Note: replace [key] below with the appropriate gpg key. git crypt add-gpg-user [key] push changes to remote (github) git push remind the added individual to pull these changes down before they try to unlock the repo. The individual who was just added will have to unlock the repo before they can access encrypted files. git crypt unlock 16.8.3 Import your keys to the EHA server You will probably want the ability to decrypt files when working remotely, so place them on the EHA server. You can do this by copying over your whole gpg keys directory to the server, like so: scp -rp ~/.gnupg url.of.server: Note that the main EHA analysis servers share user file systems so you only have to do this on one of them. You may have to edit the file ~/.gnupg/gpg-agent.conf on the server. If its first line is pinentry-program /usr/local/bin/pinentry-mac, change it to pinentry-program /usr/bin/pinentry. Note that it may not exist at all, which is fine - it means the program is just using default behavior. 16.9 Use a symmetric key for automated processes If you are using continuous integration on a repository with encrypted files, you’ll need to provide a way for the CI system to unlock them. An easy, but not most secure way is to provide a symmetric key. You can generate this by running this in your project directory. This key can always be regenerated so do NOT commit it to your repository. git-crypt export-key git_crypt_key.key git_crypt_key.key can now be used to decrypt the repository, and you can provide it to the CI system as an environment variable. However, since it is binary data, you’ll need to convert it to base64 first. So run something like: cat git_crypt_key.key | base64 | pbcopy to convert this file to base64 data, then paste it in your CI system’s environment variable field as something like GIT_CRYPT_KEY64. The key can now be removed from your system. rm git_crypt_key.key To use the key later, you’ll need (1) git-crypt and gpg installed in the CI system image, and (2) to run these commands after the CI clones your repository: echo $GIT_ENCRYPT_KEY64 &gt; git_crypt_key.key64 &amp;&amp; base64 -d git_crypt_key.key64 &gt; git_crypt_key.key &amp;&amp; git-crypt unlock git_crypt_key.key 16.10 Removing sensitive files from git history AKA What to do if you accidentally committed sensitive files (data, keys, etc.) to your repository either before encryption or our outside the scope of your .gitattributes file. Take a breath. Make sure your sensitive files are included either in the .gitattributes file if you want them encrypted or delete them then add them to your .gitignore file. You can add a filter for particular files any where in your repo with **/MySuperSensitiveFile see gitignore documentation Install git-filter-repo Navigate to your repo’s working directory cd my/project/folder Back up your .git/config file as certain elements will be removed when we run git filter-repo Run git filter-repo --invert-paths --path PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA to remove a sensitive file. Make sure you’ve removed everything you want from the repository’s history use git log -- path/to/file to see git history for a particular file. Note If you’ve successfully removed the file there won’t be any history to display. see git log for more information. 9)Check in with collaborators to make sure no one has work on branches in the repo they would like to keep. Next you will use push --force to overwrite all the history on github and all the branches on your repo. Push your changes to the remote using git push origin --force --all. You may have to add the origin back into your .git/config file Use git push origin --force --tags to remove sensitive files from any of your tagged releases. Follow the instructions here for contacting github to make sure there aren’t any caches of the files on github. Tell your collaborators to rebase,NOT MERGE, any branches created off the old repo history. Wait a little bit to make sure that your file remove didn’t have unintended side effects then run the following code: git for-each-ref --format=&quot;delete %(refname)&quot; refs/original | git update-ref --stdin git reflog expire --expire=now --all git gc --prune=now Breathe a sigh of relief. "],["17-dependencies.html", "17 Dependency Management 17.1 GCC and mac silicon 17.2 ", " 17 Dependency Management How do I make sure that all my software and configurations needed for a projects are portable? renv captures package versions and sources as well as R version. renv also records python versions and virtual environments for projects that used both languages. The capsule package can be useful when you want version control/depedency management but not the full weight of renv. Sometimes, renv can be an impediment for smaller and highly collaborative projects, that is where capsule can be useful. Ultimately, capsule will allow to setup renv for a project when it gets to a stable state. Docker allows you to create virtual machines, meaning you can capture all system dependencies in addition to the dependencies captured in renv. It works for essentially any programming language and is extremely flexible. UseR 2022 Docker workshop A lesson in user Docker for an R project Makefiles can automate a complex, multipart project. Here’s a lesson on them from Software Carpentry R packages can be a useful project output. We have some in-house R packages to provide access to internal data and generate reports, and may be developing more for external audiences. Hadley Wickham’s R Packages Book provides guidance for these, and we expect our packages to be up to rOpenSci standards. 17.1 GCC and mac silicon R package installation can be especially tricky on Mac computers with Apple Silicon. Below are some steps to get package install set ups. Install the R compilation tool chain https://mac.thecoatlessprofessor.com/macrtools/. If any issues with FORTRAN, follow instructions for downloading FORTRAN compiler here for Apple silicon Macs: https://mac.r-project.org/tools/ Install gcc brew install gcc. This requires having brew set up. Make sure homebrew is in terminal path. Add it by export PATH=$PATH:/opt/homebrew/bin. Create ~/.R/Makevars and insert the following. This will need to be updated depending on gcc version. CXX14FLAGS += -O3 -arch arm64 -ftemplate-depth-256 FC = /opt/homebrew/Cellar/gcc/13.2.0/bin/gfortran F77 = /opt/homebrew/Cellar/gcc/13.2.0/bin/gfortran FLIBS = -L/opt/homebrew/Cellar/gcc/13.2.0/lib/gcc/13 RStudio may have a bug in accessing the correct terminal path. The workaround is to copy your terminal path (echo $PATH) into .Renviron per instructions here: https://community.rstudio.com/t/how-to-get-rstudio-ide-to-use-the-correct-terminal-path-in-mac-os-x/131528/6 Some packages may need additional help for R to find the correct paths. For units: remotes::install_version(package = \"units\", version = \"0.8-5\", configure.args = \"--with-udunits2-lib=/opt/homebrew/Cellar/udunits/2.2.28/lib --with-udunits2-include=/opt/homebrew/Cellar/udunits/2.2.28/include\") 17.2 Historically some projects used Packrat or checkpoint to fix R package versions. These systems have been superceded by renv. "],["18-statistical-methods.html", "18 Statistical Methods 18.1 Multivariate Regression 18.2 Networks 18.3 Bioinformatics and Sequence Analysis 18.4 Species Distribution Modeling 18.5 Epidemic Simulation and Fitting 18.6 Phylogenetics", " 18 Statistical Methods Statistical methods are a far larger subject than can be covered in this handbook. Our team uses a wide variety of methods that differ by project and change via advances in the field. Nonetheless, here are some methods that we favor when they are appropriate. 18.1 Multivariate Regression We like Generalized Linear Models. Many of us approach these from a Bayesian perspective, and build and fit these models using Stan. A great introduction to this type of modeling is found in Statistical Rethinking, by Richard McElreath which we have a couple of copies of. Richard also has accompanying lectures for this book online as YouTube videos. For nonlinear models, we like Generalized Additive Models, for which we use the mgcv R package. Noam has a bunch of materials on this, including slides and exercises from a workshop he co-teaches, and the seminal text is Generalized Additive Models in R, by Simon Wood. (Example - Host-Pathogen Phylogeny Project: Paper, GitHub) When a multivariate analysis is primarily about prediction, and less about variable inference, machine-learning methods such as boosted regression trees are useful. We make use of these through the dismo or xgboost packages. (Example - Hotspots 2: Paper, GitHub) 18.2 Networks We like networks for both descriptive visualizations and quantitative analyses. R is full of packages to achieve both of these goals, check out ggnetwork and bipartite. We assess individual components within networks looking for high-impact individuals due to their high degree, centrality, or ability to bridge different communities. For a tutorial on basic metrics check out Randi Griffin’s tutorial on primate social networks which utilizes the igraph package. Often, our networks are bipartite, meaning the networks shows interactions between two type of nodes. For us, these are usually viruses and hosts. You can look at example networks in two PREDICT publications: Figure 4 in Anthony et al. 2017 and Figure 3 in Willoughby et al. 2017. To assess overall network structure and identify communities, we measure the modularity. The modularity of a network model is a type of network partitioning into subgroups or modules. This is an alternative to clustering alogrithms. For bipartite networks, we like the Barber’s modularity (Q), calculated through the lpbrim package. Alternatives modularity algorithms include NetCarto which uses a simulated annealing (SA) algorithm or using a map equation (ME) algorithm. 18.3 Bioinformatics and Sequence Analysis Bioconductor is a comprehensive ecosystem of R packages focused on sequence analyses. Typing your general topic of interest into their searchable software table should at least provide an introduction to relevant software. In general, best practices in bioinformatics are changing rapidly, so it is difficult to recommend particular procedures. However, the journal Nature Protocols covers cutting-edge methods, ranging from the lab to the laptop, in great depth. Many articles include step-by-step instructions to complete a given analysis. For a scattershot introduction to high-quality bioinformatics software and relevant applications, it might be worth checking out the Bedford, Pachter, and Patro lab websites. Note that many contemporary bioinformatics tools are accessed through the command line rather than R. For a general overview of RNA sequencing analysis (other bioinformatics pipelines have strong similarities), the Simple Fool’s Guide from the Palumbi lab is a great learning resource. 18.4 Species Distribution Modeling 18.5 Epidemic Simulation and Fitting 18.6 Phylogenetics "],["19-quality-control.html", "19 Quality control", " 19 Quality control 19.0.1 What is quality control? Quality control is a determinant step that impacts the accuracy of the results obtained from NGS data. This chapter presents the motivations and tools involved in the quality control of NGS data. 19.0.2 Why is a quality control step necessary in NGS data? Simple and paired-end reads from NGS technologies are sequenced through complex processes that occur at the atomic scale. In particular, the sequences of the Illumina technology are obtained through a technique called sequencing by synthesis. As a consequence of this process, reads could carry complete sequences or fragments of adapters. In addition, the quality of the bases represented by the PHRED score is not homogeneous throughout the sequence, reducing towards the ends of the reads, in particular the end 3’. So a step is necessary that removes the technical sequences (adapters), but also the low-quality regions from the reads. This process is called trimming. 19.0.3 Why is a quality control step important in the processing of NGS data? Improves the quality of the results. The positive impact of the read trimming stage has been demonstrated in the study of genetic expression, variant calling, genome assembly, metagenomics and in general in all areas of knowledge where NGS data are used. Trimming the reads increases the number of sequences that are aligned against the reference sequences or decreases the complexity of the assembly graphs saving time and computational resources. 19.0.4 How does the trimming process work? Here we describe in a superficial way how the trimming process is performed by the trimmomatic tool. There are two ways to remove adapters from reads. In the simple mode, the adapter sequences are aligned with an allowed mistmach number against the reads. The region in the 5’ region of the alignment is trimming. The second type is the Palindrome mode. The palindrome mode is specifically optimized for the detection of ‘adapter read-through’. When ‘read-through’ occurs, both reads in a pair will consist of an equal number of valid bases, followed by contaminating sequence from the ‘opposite’ adapters (figure below). Adapter read-through detection is done by aligning reverse read against forward read of each pair. The region aligned between the adapters represents valid sequences, the remaining sequences are discarded. To remove regions of low quality trimmomatic uses a sliding window. This works by scanning from the 5’ end of the read, and removes the 3’ end of the read when the average quality of a group of bases drops below a specified threshold. image 19.0.5 Quality control with trimmomatic 19.0.5.1 Documentation Github Manual Publication 19.0.5.2 Installation An easy way to install trimmomatic is with conda conda install -c bioconda trimmomatic We are also going to install fastqc conda install -c bioconda fastqc 19.0.5.3 Execution The following command line removes adapters and trimming #’ regions of low quality pair-end reads in fastq files trimmomatic PE -threads 8 R1.fastq R2.fastq R1_clean.fastq R1_clean_unpaired.fastq R2_clean.fastq R2_clean_unpaired.fastq ILLUMINACLIP:TruSeq2-PE.fa:2:30:10 LEADING:6 TRAILING:6 SLIDINGWINDOW:4:20 MINLEN:80 2&gt; {log} Input files: R1.fastq forward reads R2.fastq reverse reads Output files: R1_clean.fastq trimmed forward reads R1_clean_unpaired.fastq trimmed forward reads (dropped pair) R2_clean.fastq trimmed reverse reads R2_clean_unpaired.fastq trimmed reverse reads (dropped pair) #### Parameters: - threads number of threats that could be used to execute the command - ILLUMINACLIP:TruSeq2-PE.fa:2:30:10 fasta file containing adapters TruSeq2-PE; the maximum number of mismatches allowed is 2 in the seed alignment; minimum 30 nucleotides must align for PE palindrome read alignment;minimum 10 nucleotides must align between a sequence and an adapter. - LEADING:6 The minimum quality to retain a base a base is 6 (5&#39;) - TRAILING:6 The minimum quality to retain a base a base is 6 (3&#39;) - SLIDINGWINDOW:4:20 The sliding window size is 4 and and the average quality must be a minimum of 20. - MINLEN: Only reads with a minimum length of 80 nucleotides after trimming are kept. 19.0.6 Visualization of the quality of the reads After trimming the reads, it is important to check the quality of the reads. A popular tool at this step is fastqc which produces easy to interpret plots about different read quality parameters. 19.0.6.1 Execution fastqc --threads 8 R1_clean.fastq --outdir=QC Input: R1_clean.fastq trimmed reads Output: QC: directory; in this directory fastqc produces html files with the main results of the quality analysis. "],["20-assembly-of-high-throughput-data.html", "20 Assembly of high-throughput data", " 20 Assembly of high-throughput data In this chapter, the two main approaches to the assembly of biological sequences will be presented. Some introductory documentation - A bit of de Bruijn graph history - MEGAHIT publication 20.0.1 The problem Figure 1 presents the challenges faced by assembly algorithms. Current second and third-generation sequencing technologies produce fragments of genomes, called reads that can vary from a few hundred to kilobases, depending on the technology. Ideally, these reads could cover the complete genome sequence. However, both technical problems and the complexity of the biological data lead to gaps and reductions in the overall size of the assembly. Among the biological factors, the heterozygous diploid genomes, the abundance and extension of the repeats; the nucleotide variation hinder the assembly process. However, the main problem is to recover most of the genome from millions of reads with a reasonable use of time and resources. An elegant and efficient solution to this problem particularly suitable for short reads, is the Bruijn graph. 20.0.2 Graphs A solution to the assembly problem is represented by graphs. Graphs are a mathematical notion that is widely applied to various biological, ecological, and evolutionary problems. Formally, a graph is a mathematical structure composed of nodes and vertices. The nodes are related to each other by the vertices. Figure 2 is an example of a graph; in this example, numbers are found at the nodes, while the vertices represent relationships between these nodes. image Figure 2. Graph https://en.wikipedia.org/wiki/Graph_theory 20.0.2.1 Why are graphs a good strategy to assemble biological sequences? Figure 3. Overlap between reads. Nucleotides shared between reads are in bold. 20.0.3 Bruijn graph There are different types of graphs with particular properties. The de Bruijn graph presents some properties that guarantee always finding the genomic sequence from the reads. Initially, we will make an intuitive introduction to de Bruijn graphs before doing a more formal presentation. Before introducing the de Bruijn graph, we will present an essential concept of this type of graph, the k-mer. The k-mer are words of size K observed in a sequence identified through a sliding window (with a step of 1),(Figure 4). Figure 4. K-mer example. All K-mers of size 6 of a small genome are shown. 20.0.3.1 How to build a de Bruijn graph? A de Bruijn graph for a fixed integer K: Nodes = all k-mers (substrings of length k) present in the reads. There is an edge between x and y if the (k-1)-mer prefix of y matches exactly the (k-1)-mer suffix of x. 20.0.3.2 Examples Genome : ACTG K-mers for K=3: ACT, CTG Bruijn graph : ACT -&gt; CTG Why? A second example with several reads Exercises What are the de Bruijn graphs of the following reads, you can use a k-mer of 3: ACTG, CTGC, CTGA, TGCC AGCCTGA, AGCATGA 20.0.4 Contigs Different factors can complicate the construction of de Bruijn graphs; Biological factors include sequence variations that may occur in some samples, for example in virus sequences or heterozygosity or polyploidy in plants. As we mentioned before, the frequency and length of the repetitions is another factor that has significant effects on the assembly process. In addition, sequencing errors reduce the chances of obtaining whole genomes. The previous exercises showed that these factors produce branching paths in the de Bruijn graph. Most assembly software does not face the dilemma of choosing between different paths and they cut the graph into disjoint components. These partial genome sequences are called contigs. Contigs are defined as unique paths in the de Bruijn graph that do not share nodes with other paths. The following figure is an example of contigs, or disjoint paths, in a de Bruijn graph: This figure shows that none of the contigs share nodes and represent partial sequences of the same genome. 20.0.5 A formal presentation of de Bruijn graphs The de Bruijn graphs represent an important advance in the assembly of genomes. The first bacterial genomes and the human genome were assembled with a different strategy. In this strategy, the nodes represented the reads and the edges represented the overlap (shared sequence) between the nodes. This strategy was developed for the relatively long sequences obtained with the Sanger technology. In this approach, the genomes were assembled by finding a Hamiltonian path in the graph. A Hamiltonian path is defined by visiting each of the nodes once, returning to the starting point or initial node. This method involves aligning each of the reads against all other reads. This last point is the main limitation of this strategy; from the computational point of view, this is an NP-Complete problem. This means that the time required to align all the reads in an assembly is polynomially related to the number of reads. Therefore the assembly of genomes from millions of short reads with the Hamiltonian cycle is not tractable. In response to this major limitation, in the early 2000s, Pevzner proposed de Bruijn graphs as a practical solution to assembling genomes from millions of short reads. So far an intuitive introduction to de Bruijn graphs has been made, now some formalization is necessary. We will assume that our de Bruijn graph is directed and connected, this means that the edges have a direction when relating two different nodes and that each node could be reached by another node, respectively (Figure below). We will also assume that our graph is balanced, this means that the number of edges that arrive at a node (indegree) is equal to the number of edges that left this same node (outdegree), and that this is true for all the nodes of the graph. Euler showed that there is a unique solution (unique path), in which each of the edges of a graph is visited once from an initial point. This unique solution represents the genome. The de Bruijn graph represents a major advance in the assembly of genomes because, unlike the Hamiltonian solution, the Eulerian path does not imply the alignment of the reads. Although a plausible solution, the Eulerian cycle presents certain limitations, of which the most important is the memory size required for the storage of the graph. In addition, sequencing errors and biological variability and repeats prevent, in many cases, obtaining a single genomic sequence. Currently, there are different software that apply the algorithmic concepts of the Eulerian cycle to assemble genomes from different types of high-throughput sequences. Megahit, which is an example of this kind of software, will be presented below. But first, we will discuss a bit about the importance of the size of the k-mer. image A de Bruijn graph, directed, connected, and balanced 20.0.6 MEGAHIT Github The tool that we will use for de novo assembly of genomes is MEGAHIT. In our experience, MEGAHIT has presented the best balance in the assembly process of genomic sequences from metagenomic data. MEGAHIT presents several advantages; this software works under the standard resources of most of the scientific computing clusters; MEGAHIT assembles sequences in a reasonable amount of time. MEGAHIT makes use of succinct de Bruijn graphs which is a compressed representation of Brujin graphs. This compression is based on the Burrows–Wheeler transformation which reduces the memory space needed to store the graph; an in-depth technical explanation of succinct de Bruijn graphs is given in this link. Before we address the installation and execution of MEGAHIT, we will highlight some of the options that we feel are important for running this software. The first one of these options is the minimum multiplicity of a k-mer for filtering (–min-count). This option specified the minimum multiplicity that a k-mer must have to be used in the assembly process. The idea behind this parameter is to remove k-mer with errors, once sequencing errors are rare events. However, it is possible that using this option, k-mer of species or sequences with low abundances are removed. For this reason, MEGAHIT uses a heuristic that recovers some of these k-mers called mercy-kmer, which can be disabled from the command line (–no-mercy). Another set of options of major implication in the assembly is related to the k-mer size. A small k-mer size is favorable for filtering erroneous edges and filling gaps in low-coverage regions, a large k-mer size is useful for resolving repeats. It is for this reason that MEGAHIT uses k-mers of different sizes eliminating the different biases. Some of the parameters that control the size of k-mer in particular for metagenomic data can be specified with the –presets option. 20.0.6.1 Installation To install MEGAHIT we will use Conda conda install -c bioconda megahit 20.0.6.2 Running MEGAHIT megahit --presets meta-large --min-contig-len 200 -t 20 -1 R1.fastq -2 R2.fastq -r R1.unpaired.fastq,R2.unpaired.fastq Options: --presets meta-large : &#39;--k-min 27 --k-max 127 --k-step 10&#39; --min-contig-len : minimum length of contigs to output -t : number of CPU threads -1 : comma-separated list of fasta/q paired-end #1 files -2 : comma-separated list of fasta/q paired-end #2 files -r : comma-separated list of fasta/q single-end files "],["21-seqs_aln.html", "21 Sequences alignment 21.1 Heredity 21.2 Orthologs and paralogs 21.3 BLAST 21.4 Install BLAST+ with Conda 21.5 Install database reference sequences 21.6 Pairwise sequence alignment 21.7 BLAST with high-throughput data", " 21 Sequences alignment 21.1 Heredity Today, we are capable of reconstructing kinship relationships between organisms through bioinformatics analysis of biological sequences, particularly proteins, and DNA. This is possible thanks to the principle of heredity. The biological sequences are transmitted from one generation to the next through sexual and asexual reproduction. From one generation to the next, the sequences accumulate variation as a result of stochastic or selective processes. Despite this variation, sequences from two closely related organisms are expected to be more similar to each other than sequences from two distantly related organisms. Between two organisms with a distant relationship, more time has elapsed since the last shared ancestor, accumulating more variation. This is the principle on which the search for homology between sequences is based, two DNA or protein sequences are homologous if they share ancestry. This means that the similarity between these sequences is the product of sharing the same ancestor placed somewhere in the phylogenetic tree of life. The higher the sequence similarity, the more recent the common ancestor between the sequences is expected to be. However, the identification of a hit with a high similarity is not enough to determine the homology of a sequence; it is also necessary to model the evolutionary process through phylogenetic tools. This chapter will address the methodological principles of pairwise alignments, the alignment between two biological sequences. Multiple sequence alignment is the topic of the next chapter. Before introducing pairwise alignment, I would like to introduce some homology concepts that are essential to the application and interpretation of alignments and phylogenies. 21.2 Orthologs and paralogs Homology relationships between sequences are defined by two important events in the evolution of species: duplication and speciation. Two sequences (genes or proteins) are orthologous if they are the product of speciation. Two sequences are paralogs if they are the products of duplication events. When two different organisms present sequences that are the product of duplication events prior to speciation, these sequences are out-paralogs. When duplication occurs after the speciation process, the sequences are called in-paralogs. 21.3 BLAST 21.3.1 Conda Installation One of the easy ways to install bioinformatics software is with Conda. Download miniconda for ubuntu (If you have another OS choose the appropriate file) https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Set the correct authorizations chmod +x Miniconda3-latest-Linux-x86_64.sh Install bash Miniconda3-latest-Linux-x86_64.sh Export path export PATH=&quot;$HOME/miniconda3/bin:$PATH&quot; Reinitialize the session. 21.4 Install BLAST+ with Conda conda install -c bioconda blast=2.12.0 21.5 Install database reference sequences The first step is to create the directories where the databases will be installed. For example, in your home directory: mkdir NCBI cd NCBI Create two subdirectories, one for nucleic acid sequences and one for protein sequences. mkdir nt mkdir nr 21.5.1 There are different ways to install the pre-formatted BLAST databases: 21.5.1.1 The ‘wget’ command Download NCBI nucleotide sequence database cd nt wget ftp://ftp.ncbi.nlm.nih.gov/blast/db/nt.??.tar.gz Uncompress the database files. You will find the uncompress.sh script in the scripts directory of this repository ./uncompress.sh Download NCBI amino acid sequence database (In the nr directory) wget ftp://ftp.ncbi.nlm.nih.gov/blast/db/nr.??.tar.gz ./uncompress.sh 21.5.1.2 The update_blastdb.pl script. BLAST+ offers the update_blastdb.pl script that allows you to download the NCBI databases. This script uncompress the files for you. Obtain information from the available databases to download update_blastdb.pl --showall [*] Download the nucleotide sequence database (NCBI_DB/nt directory) update_blastdb.pl --decompress nt [*] Download the amino acid database (NCBI_DB/nr directory) update_blastdb.pl --decompress nr [*] 21.6 Pairwise sequence alignment 21.6.1 BLASTn Here is an example of alignment with BLASTn. Remember that BLAST offers different softwares to perform alignments according to the type of sequence and the reference database (BLAST software). BLASTn involves nucleotide-to-nucleotide alignments, this means that both the query sequences and the reference database are nucleotide sequences. blastn -db /home/alexarmero2022/shared/NCBI/nt/nt -evalue 0.0000000001 -num_threads 10 -outfmt &#39;6 std qcovs slen&#39; -query test.fasta &gt; testout.csv In the last command line there are several important points to keep in mind: Specify the full directory path to the NCBI database. In this example I’m using my path (/home/alexarmero2022/shared/NCBI/nt), replace it with the appropriate directory path on your system. The -db option expects the name of a database. It is for this reason that in addition to the path, it is also necessary to give the name of the database in the previous example nt. The -num_threads option refers to the number of cores or logic CPUs that the user wants to assign to the task. Specify it according to availability in your system. The results of an alignment can be reported in different types of formats (-outfmt). In the example, the tabular format (6) was used; this format is easy to manipulate. In this example the standard alignment information (std) was requested to be printed, but also some other information such as the alignment coverage in the query sequence (qcovs) and the length of the target sequence (slen). Tabular format options. 21.6.2 BLASTp Here an example of protein-protein alignment with BLASTp. Notice that the path to the database changed; now it points to the nr directory, where the amino acid sequences were installed. In the same way the name of the database also changed (nr). The sequences in the test.fasta file must necessarily be amino acid sequences. blastp -db /home/alexarmero2022/shared/NCBI/nr/nr -evalue 0.0000000001 -num_threads 10 -outfmt &#39;6 std qcovs slen&#39; -query test.fasta &gt; testout.csv 21.6.3 Configuration files and environmental variables BLAST+ allows you to configure the blast runs. Configuration can be done through a .ncbirc file. In the scripts directory of this directory, there is an example of this file. Once the variables of interest have been modified, place this file in your home directory. For example, you could indicate where your NCBI database is located: ; Start the section for BLAST configuration [BLAST] ; Specifies the path where BLAST databases are installed BLASTDB=/home/alexarmero2022/shared/NCBI/ In this last example we modify the variable BLASTDB and we assign the directory path to the NCBI databases. Note, that is the path we use in the blastn and blastp alignment examples. With this modification the blastn command line would subtly change: blastn -db nt/nt -evalue 0.0000000001 -num_threads 10 -outfmt &#39;6 std qcovs slen&#39; -query test.fasta &gt; testout.csv You could also use environmental variables. For example from your command line, you could write: BLASTDB=/home/alexarmero2022/shared/NCBI/ There are several other variables that can modify. This last line is equivalent to using the .ncbirc file. Note. The .ncbirc is a hidden file. To be able to observe it in your directory, execute the following line: ls -ad .* 21.7 BLAST with high-throughput data BLAST is perhaps the best software in the identification of homology relationships between biological sequences. The sensitivity of this tool has a cost both in the use of physical computing resources and in processing time. These limitations are particularly evident when are used NCBI databases. Several efforts have been made to use high performance computing (HPC) resources to efficiently run BLAST on large datasets. Here is one such tool: Crocroblast. This software is presented as an example, and a starting point for higher performance strategies. Crocroblast determines what is the optimal file size to perform the alignment according to the HPC physical capabilities. In this way this software divides a large fasta file into files with sizes optimized to the available system resources. This is an easy to use tool; we recommend a good reading of the CrocroBLAST documentation. The following example shows how to perform a CrocoBLAST blastn alignment. 21.7.1 Installation Get the right version for your operating system. Install the software in an appropriate directory mkdir Crocroblast cd Crocroblast unzip CrocoBLAST_linux_64.zip chmod +x crocoblast 21.7.2 Add sequence database In order to use CrocroBLAST you need to add the sequence database. Please adapt the following line according to the directory path where your database is located: ./crocoblast -add_database --formatted_db /home/alexarmero2022/shared/NCBI/nt/nt.00.nsq 21.7.3 Add the job to the queue CrocroBLAST handles job queues; this software has different functions to stop, pause, delete and run a job. The next line adds a job to the queue: ./crocoblast -add_to_queue blastn nt data/test.fa output_test_4 --blast_options -outfmt 6 The following line will allow you to check if the job was added to the queue ./crocoblast -status 21.7.4 Run the job ./crocoblast -run --num_threads 4 CrocoBLAST shows messages with the time remaining to complete the process. We hope that this example motivates you to carefully read the CrocoBLAST wiki to efficiently exploit the different possibilities of this nice tool. "],["22-data-sources.html", "22 Data Sources", " 22 Data Sources Here are some key data sources we have built and use across many projects: awesome-parasite is our curated list of sources for host/parasite/pathogen associations and associated taxonomies. HP3Extended in an internal EHA database of host-virus associations. It is hosted on AirTable and requires an EHA login. lemis and cites are R packages that host data on wildlife trade. EIDR is our emerging infectious disease repository and hosts data on first emergence effects of disease. FLIRT, the Flight Risk Tracker, contains information on airline travel patterns as well as simulations of human travel using those patterns. EIDITH, the Emerging Infectious Diseases Technology Hub, is the shared database for the PREDICT project. The eidith R package is how we interface with it for analysis. "],["23-references.html", "23 References 23.1 Converting Paperpile Citations in Google Docs to Endnote Citations in MS Word", " 23 References Paperpile is an excellent tool for sharing and keeping track of research papers, and it is fully integrated into Google Docs – which means you can easily add citations on shared documents. Many teams and projects use Paperpile to organize and share references. If you need a paperpile account, request a key from Megan Walsh. Be sure to associate the Paperpile with your EHA Google account, not a personal one. Many people in the office use EndNote which can be offline on your desktop or online. A nice feature of Endnote is the traveling libraries export from word documents. You will need a license for this, so contact Megan Walsh. 23.1 Converting Paperpile Citations in Google Docs to Endnote Citations in MS Word There are some situations where you may need to convert a Google Doc with Paperpile citations into a Microsoft Word Document with Endnote citations. Luckily, Paperpile has a detailed guide to walk you through the process. But first, you must properly install the Paperpile Google Docs Add-on! Note: You may already have the Google Chrome Paperpile Extension installed and be using it to create citations in your Google Doc, but you need the Google Docs Add-on for proper exporting. To install this addon, follow the links from Paperpile’s website dedicated to Google Docs. Now, when you open a document in Google Docs, you should see both menus for Addons and Paperpile. In order to begin Step 1 of Paperpile’s Endnote Guide, you need to access the add-on sidebar: Go to Addons \\(\\rightarrow\\) Paperpile \\(\\rightarrow\\) Manage Citations and the Paperpile side-bar will appear Don’t try to export or mess around with the Paperpile menu option (or P symbol) – they won’t help you! Once you have the add-on sidebar open, you can follow Paperpile’s detailed guide. Note: depending on your operating system and your edition of Microsoft Word, some of the Word menus may appear slightly different than the ones depicted in Paperpile’s guide. However, the Paperpile images included in the guide should look exactly the same. "],["24-training-resources-and-plans.html", "24 Training Resources and Plans 24.1 Training plan components 24.2 Training Plans 24.3 Metagenomics", " 24 Training Resources and Plans This section under revision as we consider alternate resources. If you are planning on spending significant time improving your data science and modeling skills, you will want to create a training plan. 24.1 Training plan components A description of your goals for the training plans and to which EHA projects and activities you will apply your skills A list of courses/tutorials your plan to complete The total time the courses will take to complete. The time frame over which you expect to complete them The name of a peer learner. You should have a peer learner at EcoHealth who will be a partner over the course of your training. This may be someone working on a similar training plan or someone with knowledge of the material already. They should play some or all of these roles: Accountability: Your peer learner should know about your training plan and its time frame, and check in on how you are doing. Co-learning: Your learning peer and you may want to schedule times to watch course videos and complete exercises together Review: Especially for materials without automating, your peer learner should be able to look at your work and provide feedback Motivation: Your peer learner should make your training fun and tell you that you rock. When your supervisor signs off on your training plan, contact Megan Walsh to provide you with any subscriptions for the period you need them. 24.2 Training Plans These are some suggestions for assembling resources and courses into training plans. This is of course a small fraction of the many learning and teaching resources available. Consult your peers, supervisor, and the #data-sci-discuss Slack channel to find courses or resources on the topics you require. If you use a new resource or course, please add to to this page so others can learn from your impressions of it! 24.2.1 Introductory Programming materials Hands-On Programming with R: An introduction to R for non-programmers with a focus on project based learning. Introduction to R: This introduction is designed to get you familiar with R quickly. It covers the basics and explains how to work with common data types in R. Eloquent Javascript A project based book that will take you from the basics to creating websites. For R users, the chapter on data structures is especially helpful for understanding JSON and jsonlite. 24.2.2 Better Managing Data Johns Hopkins Introduction to the Tidyverse This chapter focuses on the foundational concept of tidy data. That is rectangular data where one row = one observation, all variables are store in their own column, and all cells have a value, even if that value is NA. If you are mostly working in spreadsheets but collaborating with R or Python users, or just trying to organize a lot of spreadsheet data for your projects, work through the Data Carpentry lesson on spreadsheet organization (~2 hours) and read Hadley Wickham’s paper on tidy data (~1 hour) Importing Data provides a good overview of basic data imports while R4DS: Import demonstrates how to properly import more complex data types. AIRTABLE DATA - see chapter on Airtable ODK DATA - see chapter on ODK Reading and writing spatial data with terra This chapter goes over ingesting spatial data with the terra package in R. Geographic Data in R This chapter discusses geographic data classes in R and the sf, sp, and terra packages. 24.2.3 Version Control and Git Read through and work through the examples in Happy Git with R. Atlassian has a nice collection of git tutorials for beginners and advanced users. 24.2.4 Reproducible reporting Reports with R Markdown covers making the reproducible reports from the basics to parameterization. This chapter provides an overview of using targets in R Markdown. 24.2.5 Improving Your Statistical Fundamentals Probability, Statistics, and Data: A fresh approach using R A breadth first approach to probability and statistics that assumes minimal familiarity with calculus. The book includes data sets that require cleaning and wrangling before they can be used for analysis and relies on simulations to demonstrate important concepts. An Introduction to Probability and Simulation This book covers concepts in Probability and Simulation. It encourages users to use code notebooks (google colab, Jupyter, etc) to work through problems. Foundations of Probability in R. A short self guided course on probability and distributions. Statistical Rethinking: A Bayesian Course. This is an excellent book and video lecture series that gives builds great foundations for doing many types of modeling. Prerequisites are Intermediate R, some experience with linear regression and probability. This course has about 19 hours of video. We recommend 2-3 months for going through the book, video, and exercises. 24.2.6 Improving your data visualization Fundamentals of Data Visualization by Claus Wilke is an excellent guide to making high-quality figures, focusing more on design than mechanics of programming. R code is available for all of its examples. If you feel you have a solid grasp of ggplot2 but want to improve the quality of your figures, we recommend reading this e-book, and using the accompanying code in its GitHub repository to reproduce figures. ### R Programming Advanced R: Functions “In this chapter, you’ll learn how to turn informal, working knowledge [of functions] into more rigorous, theoretical understanding.” 24.2.7 Map-making and geospatial analysis in R Geocomputation in R is a comprehensive guide for understanding geographic data, mapping, and conducting spatial analysis in R. Likely, the most relevant chapters for your purposes are 1-8, 10-11. A chapter might take you 1-3 hours to work through, depending on how in depth you want to get and the number of exercises that you complete. Data Carpentry has a course on using R for spatial data. Like other *Carpentry lessons its designed as a workshop lesson plan but can be self-taught. It presumes very little R knowledge at all, and includes stuff like setting a project in RStudio. This is a good place to start people or students with little R experience to get them making maps right away. If you just want to get a quick feel for R spatial data types, jump into Chapter 3. Making Maps with R is a quick-start guide to mapping with ggplot2. It also introduces the gmap, maps, and mapdata packages for providing basemaps on which to overlay your spatial data. It is good for getting a map together quickly but if you are going to be doing things on a regular basis we suggest the resources above, which give you a better foundation on geographic data. Leaflet for R is a manual on the use of the R leaflet package to harness Leaflet, an open-source JS library for creating interactive maps. Leaflet maps particularly useful for exploring and visualizing spatial data, and are easily embedded into R Markdown documents. You should take a course or have knowledge of R Markdown prior to taking this course. 24.2.8 Bioinformatics Conceptual and practical introduction to some of the main topics in Bioinformatics. 24.3 Metagenomics Quality control Assembly Files and formats in high-throughput sequencing (In development) Pairwise sequence alignment Read alignment (In development) Multiple Alignment (In development) Variant Identification (In development) "],["25-help.html", "25 Getting Help 25.1 Minimal reproducible examples - helping others help you", " 25 Getting Help How do I solve this problem? How do I get my skills up to snuff? We have an #data-sci-discuss channel on Slack to ask questions and also news about useful resources and packages. We prefer that you ask question on this channel rather than privately. This way you draw on the group’s knowledge and everyone can learn from the conversation. In general, if you spend 20 minutes banging your head against your screen trying to figure something out, it’s time to ask someone. Some good questions for the Slack room: Which package should I use for something? Anyone have a good reference or tutorial for package, method? What does this error mean? Our technology team are a tremendous resource for a number of computing topics (especially web technologies and development operations), but remember that they are our collaborators, not IT support. (We do have straight IT support, mostly for office network issues, through Profound Cloud) Also, outside EHA: There’s an almost-monthly NYC R Meetup and even rarer Data Visualization Meetup that EHA members sometimes attend. There’s also an R-Ladies NYC chapter that has regular meetups and a Slack Chat room. WiMLDS offers support to attend machine learning and data science conferences. Stack Overflow is a popular Q&amp;A site for computer programming that a lot of discussions about R. R-Weekly publishes a useful weekly newsletter on new R developments, packages, and publications. The #rstats hashtag on Mastodon and X (formerly Twitter) is a good place for news and short questions, and general ranting. The Big Book of R provides a comprehensive catalog of books on R (~350 different books across many topics). If there’s a course, workshop, or conference you want to attend to improve these skills, speak with your supervisor, we can often support this. Dataquest provides high quality courses that help build data science and statistical skill sets. If you feel they would match your learning style and needs, discuss EHA purchasing a subscription for you with your supervisor. 25.1 Minimal reproducible examples - helping others help you Minimal reproducible examples are small, self-contained code and data packets that will allow others to recreate the issue you are experiencing on their machine. The reprex package is really helpful for creating preformatted code for github, stackoverflow, or slack. This stackoverflow answer provides a succinct walk through of how to create a minimal reproducible example. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
