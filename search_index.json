[["index.html", "EHA Modeling &amp; Analytics Handbook Introduction", " EHA Modeling &amp; Analytics Handbook “These aren’t rules, just some things that we figured out.” – Michael Reno Harrel Last edit 2021-09-27 by GitHub Introduction This handbook describes best practices and guidelines for project management, organization, modeling and and programming we aim for our on the EHA Modeling &amp; Analytics team. This is a living document. To make changes, just click the edit button () at the top of the page. It will take you to the source editor for the chapter on GitHub, where you can make edits and submit your changes. In general, we aim to produce sound analyses that are: Self-contained Well-documented and communicated Easily reproducible Sharable Version controlled Together, these attributes help assure that our work is correct, can be built off of and extended, meets requirements for sharing and publication, and can be continued through staff turnover. The tools we use to accomplish this are mostly, but not exclusively, based around the R programming language and git version control. Other teams at EHA use other tools (e.g., the technology team mostly uses Python and Javascript, much of our work with partners is MS Office based). The guidelines in the document represent an ideal we aim for but to not always attain. Remember: Best practices are always evolving. Don’t let the perfect be the enemy of the good. Other teams and external partners have different workflows and we adjust as neccessary to collaborate. Our goal is to do good science to advance conservation and human health, not be slick programmers. You can find some slides from a previous presentation on this topic here. The philosophy and guidelines in this document owe an enormous amount to the work of the Software and Data Carpentry and rOpenSci organizations, and the work of Hadley Wickham and Jenny Bryan. You’ll find many links to their work in this handbook. "],["1-quickstart-for-computing.html", "1 Quickstart for computing 1.1 Optional", " 1 Quickstart for computing In addition to the standard computer setup provided by office admin (Email, MS Office, etc), M&amp;A members should do the following: Download and install Slack. If you don’t alread have an invite, get one from Noam. Join the #data-sci-discuss and #journal-club channels any others your supervisor suggests or are associated with projects you are assigned to. (Don’t forget #random and #out-of-office-fun!) If you don’t already have one, sign up for an account on GitHub. Provide Noam with your username so as to add you to the EcoHealth Alliance team. If you already have a GitHub account using a personal email, add your EHA email to your account and set up custom routing to direct EHA organization notifications to your EHA email Install R and then RStudio. Links and instructions can be found here Install git and link it to RStudio and your GitHub account. Instructions for all of this are found here, in Sections 7-15 of Happy Git with R. If you get stuck along the way don’t hesitate to ask for help (via the #data-sci-discuss channel on Slack!). Install Dropbox on your computer with your EHA account (note you can have separate personal and EHA Dropbox folders) Check that your EHA email gives you access to Google Drive. If you prefer it, or your supervisor specifies it, install it locally on your computer 1.1 Optional In addition, the following programs may be helpful to install (all for Mac users): Homebrew for general package management iTerm2 for a better shell interface. (brew cask install iterm2 or brew install --cask iterm2 using Homebrew in the shell) mosh for connecting to our high-performance servers (brew install mosh using Homebrew in the shell) hub for interacting with GitHub from the shell (brew install hub) "],["2-projects.html", "2 Project Management 2.1 Teams and Work Cycles 2.2 Setup and materials Organization", " 2 Project Management 2.1 Teams and Work Cycles At EHA projects are typically months-to-years-long workstreams centered around a main analytical or research problem. A program, grant or contract may have multiple projects, and a project may have multiple outputs such as reports, scientific publications, or apps. A project typically has a small (2-5 person team) with a project lead and possibly a project administrative point of contact (APOC). We organize projects into work cycles of 4-8 weeks. For each cycle, a team should define day-to-week scale tasks, assign tasks to members, determine the percentage of time team members will put towards the project in that cycle, given other workloads, and plan travel, reporting, collaboration, or other deadlines. Teams report out their progress at the end (and start) of each work cycle at our weekly M&amp;A meetings. Report-outs should include Progress on tasks assigned and completed in previous cycle Substantive report-out of results and products Draft plan for tasks and goals for the coming cycle Team assignments for the next cycle and level of involvement (high (&gt;50%), medium (25%-50%, low (&lt;25%)) of team memberss over the cycle. Any additional deadlines or reporting anticipated in that time frame, including plans for other internal presentations or feedback sessions. During report-outs, the M&amp;A group will provide feedback for the upcoming cycle and set a date for the next one. Teams track work cycle progress through various mechanisms based on team preferences. One option is GitHub Milestones (Example). Others use Google Spreadsheets, Air tables, or other systems. Teams may choose what they prefer as long as their system Shows current tasks, deadlines, and assignments Tracks past tasks, deadlines, and assignments Includes top-level summaries for a reporting period Is available in “real time” online rather than stored on individual machines and e-mailed Can be made accessible to other staff via a URL but kept private within EHA note: historically teams had used Asana 2.2 Setup and materials Organization An M&amp;A project lasting more than one work cycle should typically have a Slack channel for communication, a GitHub repo for data and analysis code, or Dropbox or Google Drive folder for documents or materials not appropriate for git-based version control. In addition, it may have a Paperpile folder for references. In general, one URL (often the GitHub README) should be the starting point from which one can reach all project materials. 2.2.1 Code organization In general, one should aim to set up the analysis portion of a project in a self-contained way, with clear separation between raw data, processed data, exploratory analyses, and final products. In organizing a project folder, ask If I copied this whole folder onto someone else’s computer, could they pick up the project? Are the folder organization and file naming clear? There are some exceptions for large data sets or rapidly changing data sets. In these cases, data can be organized as a separate folder or project, and large data sets can be stored in an Amazon Web Services S3 bucket. In many cases it is actually best for data to be organized as a separate repository from analysis. This allows multiple analysis projects to rely on the same upstream data project, avoiding multiple versions of data. In these cases the “data” project should include raw data, aggregation and cleaning, and its output will be cleaned but complete (not summarized data). Analysis projects can import this data as a first step. 2.2.1.1 Resources Here is a nice blog post about project structure and a few more alternatives. For anything using R, RStudio projects are a good idea for project organization. Here’s a Software Carpentry Lesson on RStudio projects. "],["3-r.html", "3 R and Reproducible Analysis 3.1 Install 3.2 Learn 3.3 Additional Resources", " 3 R and Reproducible Analysis Can everything be re-done easily if I change one data point in the inputs? At EHA R is our primary, though not exclusive, tool for analysis and modeling work. R is not just a piece of software for statistics and data manipulation but a computer language, meaning that our analyses are scripted. This means they thus can be automated, run again, built upon and extended. 3.1 Install R itself RStudio, the leading R development environment. 3.2 Learn Learning R is beyond the scope of this document, and you likely already have some experience in it, but some good starting points are: The Software Carpentry Lessons Swirl, a set of interactive lessons run right in R. The JHU Coursera Series R for Data Science by Hadley Wickam is a beginner/intermediate text that we highly recommend for getting up to speed with the particular workflows we recommend and the most recent packages that support them. Advanced R (Wickham) is very good for understanding how the language works. Efficient R by Colin Gillespie and Robin Lovelace is helpful for imporving workflows and speeding up code. R Packages (Wickham) is good for package development. Cheatsheets from RStudio are a useful references for a number of things. DataCamp courses are also potentially useful. If you feel they would match your learning style and needs, discuss EHA purchasing a subscription for you with your supervisor. These resources are largely about the mechanics of programming in R, rather than using it for statistical analyses. This is a far larger subject, but see the Statistical Methods section for a jumping-off point. 3.3 Additional Resources Trouble shooting your code: Getting Help. User groups/communities of practice: R Meetups Specific domains: Training Plans "],["4-eha-team-communication.html", "4 EHA Team Communication 4.1 Install", " 4 EHA Team Communication How do we work together and keep a useful record of our interactions? Slack is our office chat tool and is good for day-to-day communication. Slack does not have to be an instant communication tool - some people prefer to check it a few times a day. Check with your supervisor about your project/team preferences. Slack’s main purpose is to organize our communication by channels specific to a topic or project. It is good for keeping information from one project together in a way that can be referenced later by new team members, rather than being lost in various e-mail inboxes. A channel can be linked to many other tools (Dropbox/Google Drive Folder, GitHub Repository), so as to have a central hub for project management. E-mails can be forwarded to a channel. Slack also has voice-calling and, critically, screen-sharing capabilities that are useful for pair-debugging while programming. GitHub (see below) has a good issue-tracking system that accompanies each project and can be used for task management and general communication. This ties messages to a specific project and keeps a good long-term record, and can be connected to a slack channel or integrated with e-mail Remember that your Slack and GitHub communications are part of your project and are likely to be seen by both internal and external collaborators. 4.1 Install Download and install Slack. If you don’t alread have an invite, get one from Noam. Join the #data-sci-discuss and #journal-club channels and any others your supervisor suggests or are associated with projects you are assigned to. There’s also a mobile Slack app for iOS and Android, which may be helpful if you are traveling. "],["5-documentation-and-outputs.html", "5 Documentation and Outputs 5.1 Learn 5.2 Install 5.3 Library", " 5 Documentation and Outputs Will someone understand this thing when I hand it over? Documentation is essential to collaboration and continuity for our work. Your project should contain documentation to allow a project to be picked up by another user. Documentation includes the following: A README document in the top level of the project folder with a high-level explanation of project organization, purpose, and and contact info. Metadata for your data set showing its origin and the type of data in each field in a table. Comments in your computer code Written descriptions of your analyses. The primary medium for this should be R Markdown documents, which allow you to combine code, results, and descriptive text in an easy to update and modify form. Shorter ephermal results can be posted as plots to your project Slack rooms. 5.1 Learn R Markdown is pretty straightforward to learn. You can create your first document and get the basics by going to File &gt; New File &gt; R Markdown in the RStudio menu. When you have time, dive in a bit more with this great lesson on it with accompanying video. Here’s an RStudio Reference Sheet for R Markdown. 5.2 Install (Very optional unless you are asked): ehastyle is our internal EHA R package with R Markdown templates for some reports we produce. 5.3 Library Once your project has been published, you should include the output in our EHA Library. Simply submit through this google form. The library database, along with full texts of both media and science publications, is stored on DropBox. The purpose of the EHA Library is to store our work in a searchable, systematic database. A centralized location of EHA work is of use to our science and development teams. Additionaly, standardized analyses (stored in the eha-library github repository) create easy reporting to our board, our funding bodies, and the public. We have found this database particularly useful in the on-boarding of new staff. The current scope of the Library includes: Scientific publications Conference presentations Layman publications by EHA staff EHA-mentored academic work Media articles that highlight our work or our scientists "],["6-data-management.html", "6 Data Management 6.1 Learn 6.2 Install", " 6 Data Management Can the data be shared and published, and easily re-used in other analyses? Store data in simple, cross-compatible formats such as CSV files. Microsoft Excel can be a useful tool for data entry and organization, but limit its use to that, and organize your data in a way that can be easily exported. Metadata! Metadata! Document your data. For relational datasets you can create linked data on Airtable For data sets that cross multiple projects, create data-only project folders for the master version. When these data sets are finalized, they can be deposited in public or private data repositories such as figshare and zenodo. In some cases it makes sense for us to create data-only R packages for easily distributing data internally and externally. We aim to generally work in a tidy data framework. This approach to structuring data makes iteroperability between tools easier. 6.1 Learn Read Hadley Wickham’s tidy data paper for the general concept. Note the packages in this paper are out of date, but the structures and concepts apply. R For Data Science is a great online book to read and reference for working in this framework, and gives guidance for the most up-to-date packages (tidyr being the latest analogue of reshape and reshape2). Data Carpentry has a Lesson on spreadsheet organization for when you need to do some work in Excel but make it compatible with R. Nine simple ways to make it easier to (re)use your data rounds some things out in terms of data sharing. This post is nice, too. 6.2 Install Get the tidyverse package for R using install.packages(\"tidyverse\"). This will install several other relevant packages. "],["7-version-control-git-and-github.html", "7 Version Control, Git and Github 7.1 Learn 7.2 Install", " 7 Version Control, Git and Github Can I go back to before I made that mistake? Can others see changes others have made to the project and can I see theirs? Version control is essential to long-term project management and collaboration. We primarily use git for this - we recommend it for any project with more than one file of code. It has a steep learning curve but is very powerful. GitHub is a web service for sharing git-versioned projects that has many great tools for collaboration. We have an organizational GitHub account so we can have private repositories and work in teams with shared projects. For projects with little code-based work, there are other options, as well: Google Docs/Word Track Changes are limited to single documents Dropbox can track all files in a shared project/folder Allows one to view/revert to any previous version of a file in the folder Easily sharable Does not travel well - history is lost when project moves elsewhere File histories are independent - does not track interrelated changes. Avoid filename-based version control: 7.1 Learn Git has a steep learning curve and we recommend you spend some time learning rather than only trying to pick it up as you go along. Work through the online book Happy Git with R, which will help you get git and GitHub setup and working with R and RStudio, and teach you some basic workflows. Take the Data Camp course Introduction to Git for Data Science (free) to reinforce some key concepts and learn how work with versions on a day-to-day basis. Here is a useful git cheat sheet 7.2 Install Go through the installation steps Happy Git with R’s “Installation” and “Connect” chapters and Appendix B Note when setting up your GitHub account that one account can have multiple e-mail addresses associated with it, so you can split your work and personal stuff without needing multiple accounts (see here). Give Noam (Modeling &amp; Analytics) or Robert Young (Tech), your GitHub username so they can make you a member of the organizational EHA account and be given access to the appropriate teams. If you are using a GitHub account you previously created with another e-mail, be sure to add your EHA e-mail under Email Settings and set “Custom Routing” under your notification settings so that notifications related to the EHA organization go to your EHA e-mail. Install Dropbox on your computer with your EHA account (note you can have separate personal and EHA Dropbox folders) Check that your EHA email gives you access to Google Drive. If you prefer it, or your supervisor specifies it, install it locally on your computer "],["8-reviewing-analyses-and-code.html", "8 Reviewing Analyses and Code 8.1 Learn", " 8 Reviewing Analyses and Code Has my work recieved feedback? Has a second set of eyes checked it for correctness? Have I learned from my colleagues’ work? Just like any piece of writing that you do, your analysis code should be reviewed by a peer or supervisor. There are generally two types of code reviews we engage in: Unit reviews are reviews of discrete, small parts of a project. This might be an analysis that you took a few days or a couple of weeks to complete, and consists of 1-2 files or a few dozen to hundred lines of code. When you complete such a discrete unit, you should solicit feedback. Project reviews are reviews of a whole project as it wraps up, such as prior to the submission of a manuscript. These reviews aim to check that the project is complete, understandable and reproducible. Reviews can be either In person reviews where you go over your code with your team or at our informal science meetings. ScreenHero can also be used for this. Written reviews where a peers place comments in your code or use the commenting and reviewing features on GitHub. or both. 8.1 Learn Check out Fernando Perez’s tips for code review in the lab. Read the Mozilla Guide to Code Review in the Lab Check out some rOpenSci package review examples to look at one kind of code review in action. Best practices for this are evolving. Check out a recent conversation among scientists on Twitter on the topic "],["9-testing.html", "9 Testing 9.1 Learn 9.2 Install", " 9 Testing Is this code doing what I think its doing? Is this data correct? Most code should be accompanied by some form of testing. This scales with the size and type of project. Your work should generally accompanied with testing code or outputs that show that your models behave appropriately, are statisically sound, that your code is running as you expect and your data is checked for quality. 9.1 Learn Test driven data analysis is a neat blog on this subject. There’s a testing chapter in the R Packages book. The vingettes and README files of the packages below are useful. 9.2 Install R packages: assertr or validate for testing that data meets criteria visdat for visually inspecting tabular data. (though there are many ways to plot your data for inspection). testthat for functions and R packages. "],["10-high-performance-servers.html", "10 High-Performance Servers", " 10 High-Performance Servers How can I make this giant beast of a model run faster? EHA maintains internal high-performance servers for Modeling and Analytics work. For security reasons, details on these can now be found in this private repository. "],["11-cloud-computing-services.html", "11 Cloud Computing Services 11.1 Setting up Amazon Credentials 11.2 Using Amazon S3 Storage from R", " 11 Cloud Computing Services What if my data files are too big to hold in Github? Where is the latest IUCN/MODIS/LANDSAT shapefile? What if I need more CPUs than even our servers have? EHA has an organizational Amazon Web Services and Microsoft Azure accounts that may be useful for some projects. AWS S3 storage or Azure Blog Storage are useful for hosting large data files, especially if they are shared across projects. AWS EC2 cloud computers or Azure Virtual Machines may be appropriate for a hosting a web app or database, automating regular processes, or other analytical projects. If you think you need cloud resources for your project, contact Noam or Robert Young for access to the AWS account, to discuss what services or other providers may be useful. If you are using AWS, remember: All resources used on AWS need to be tagged with a project:PROJECTNAME tag in order to assign costs to the appropriate EHA projects. Be judicious with AWS service usage. It is easy to run up costs. 11.1 Setting up Amazon Credentials To connect to AWS via program like R, you will net to set up an access access credientials. Once you’ve gotten your credentials, log on to ehatek.signin.aws.amazon.com/console/. ehatek should be already entered in the “Account ID or alias” field. You will be asked to change your password after the first time you log on. 11.1.1 Generate an access key Navigate to your IAM (Identity and Access Management) page to generate an access key. You should see a ✅ next to “rotate your access keys.” Click “Manage User Access Keys” after expanding the access key subheading. (If you see an ❌, ask whoever created your account if you have the appropriate S3 privileges.) Click the “Security credentials” panel and scroll down to the “create access key” button. Your access key will have a key ID - a long string of letters and numbers - and a *~*secret*~* access key - a longer string of letters and numbers. Press “show” and hang onto it somewhere until you are sure your ~/.aws/credentials file is working as intended. Don’t share your secret key with anyone. If you lose it before you use it, you can delete your and make another one. Deleting a key is different from making a key inactive - you might reach your access key limit pretty quickly, so you’ll probably have to do the former if you have keys that you don’t use. 11.1.2 Create a credentials file Open a blank text file in a text editor and paste your key ID and secret key as follows. It will look like this: [default] aws_access_key_id = your_key_id aws_secret_access_key = your_secret_access_key aws_default_region = us-east-1&quot; Replace your_key_id and your_secret_access_key with the values you see in the AWS browser. Create a folder called ~/.aws, and put the credentials file in it. (YES, currently, you need that trailing \" after us-east-1 if you want to include the AWS_DEFAULT_REGION in your credentials file, because the aws.signature is using a strange parser.) If you get errors related to the AWS default region, you can delete that line and set it locally inside your R environment like this: Sys.setenv(&quot;AWS_DEFAULT_REGION&quot; = &quot;us-east-1&quot;) 11.1.3 Test your Key Open an R session to make sure your credentials file works. # install the AWS packages if you need them if (!require(aws.s3)) devtools::install_github(&quot;cloudyr/aws.s3&quot;) if (!require(aws.signature)) devtools::install_github(&quot;cloudyr/aws.signature&quot;) library(aws.s3) # load credentials from your credentials file aws.signature::use_credentials() # check list of aws S3 buckets head(aws.s3::bucketlist()) If you get a ‘403 Forbidden’ error, your credentials aren’t working. 11.1.4 Alternate credentialing method If you have Done Your Googles and are still having trouble setting your AWS credentials, you can set them in your .Renviron file. The advantage of using an ~/.aws/credentials file is that your key will then work across all platforms - R, Python, the command line, etc. However, if you’re only connecting to AWS through R, having your key in your .Renviron will be enough. # If you don&#39;t know how to find your .Renviron file, do the following to open it: if(!require(usethis)) install.packages(&quot;usethis&quot;) usethis::edit_r_environ() # add the following to your .Renviron, replacing your_key_id and your_secret_access_key with their actual values. Save your .Renviron and restart R. &quot;AWS_ACCESS_KEY_ID&quot; = your_key_id &quot;AWS_SECRET_ACCESS_KEY&quot; = your_secret_access_key &quot;AWS_DEFAULT_REGION&quot; = us-east-1 After you restart R, test that your credentials are working with aws.s3::bucketlist(). 11.2 Using Amazon S3 Storage from R Above, you used bucketlist above to list all the buckets associated with your account. To look at everything inside a specific bucket, use get_bucket. Note that Amazon charges per Gigabyte for downloading and uploading files. Use things as needed for your project, but don’t get crazy and put a file download inside a for loop that you’re running a thousand times a day. To test privacy privileges are working are intended, we’ll use a very small private bucket with ~4 KB in it - Two small subsets of the Social Security Administration’s baby names dataset from the babynames package. # check list of aws S3 buckets bucketlist() # aws.s3::bucketlist() pb &lt;- &quot;eha-ma-practice-bucket&quot; b &lt;- get_bucket(pb) 11.2.1 Save objects from S3 to your machine One of the main reasons we use Amazon S3 is to make sure everyone is working with the same large data files. Usually, you’ll download the large files you need one time and keep them on your local machine for analyses. This can be done with save_object. Wrapping save_object in an if() clause checks to see if the dataset exists first before downloading. # make a data folder if this is the first dataset you&#39;re downloading into this project if(!dir.exists(&quot;data&quot;)) { dir.create(&quot;data&quot;) } # save the babynames_subset.csv object (b[[1]]) into local memory if(!file.exists(&quot;data/babynames_subset.csv&quot;)) { save_object(object = b[[1]], bucket = pb, file = &quot;data/babynames_subset.csv&quot;) } dir(&quot;data&quot;) babynames_subset.csv is now in your data directory.You can then read it into memory as you would normally. 11.2.2 Upload objects from your machine to S3 To put a file from your computer back into the bucket, use put_object. This will be less common than downloading and saving objects from S3 - as a general rule, we want everyone working with the same versions of large datasets. If you’ve made a large rasterfile that your colleagues will use, you can upload it, In this example, we’ll make a change to the babynames_subset and save it as a new file (babynames_subset2). babynames_subset2 &lt;- babynames_subset %&gt;% group_by(name) %&gt;% summarize(n = sum(n)) %&gt;% filter(n &lt; 100) # save to csv write.csv(babynames_subset2, file = &quot;data/babynames_subset2.csv&quot;, row.names = FALSE) # check whether an object with this name is in the bucket, and if it isn&#39;t, put it in there if(!(&quot;babynames_subset2.csv&quot; %in% dplyr::pull(get_bucket_df(pb), &quot;Key&quot;) ) { put_object(file = &quot;data/babynames_subset2.csv&quot;, object = &quot;babynames_subset2.csv&quot;, bucket = pb, acl = &quot;private&quot;) } So now we have a data frame in memory called babynames_subset2 and a .csv in the AWS eha-ma-practice-bucket called babynames_subset2.csv. We’ll pull the AWS csv back into memory to make sure they’re the same. To do this, we have to first copy over babynames_subset2 to a new object in memory and remove it so that when we load the version from AWS, it doesn’t overwrite it. babynames_subset_R &lt;- babynames_subset2 rm(babynames_subset2) # save the .csv object using &quot;save_object&quot; tmp = tempdir() file = paste0(tmp, &quot;/babynames_subset2.csv&quot;) save_object(&quot;babynames_subset2.csv&quot;, bucket = pb, file = file) # read the csv file into memory babynames_subset2 &lt;- read_csv(file) Finally, we can check that the object in memory and the .csv in AWS are the same. assertthat::are_equal(babynames_subset_R, babynames_subset2) "],["12-encryption.html", "12 Encryption", " 12 Encryption How do we share data in git repositories that needs to be secure? Sometimes we need to store and share secure information, such as API keys to paid online service accounts. One of our methods of choice for this is to keep these files stored in git/GitHub repositories, but to encrypt them. We do this using PGP (Pretty Good Privacy) encryption, implemented by the program git-crypt. It takes a bit to set up but once activated makes sharing secure and seamless. The PGP encryption scheme involves making a public key that you share and a private key that you use do decrypt data encrypted with your public key. We like and use Keybase, a service that helps you publish and verify a public key for this purpose. Instructions for setting this up are below. They assume you are using MacOS for local work, but are mostly right for other cases, too. 12.0.1 Set up Keybase Sign up for Keybase, and follow the instructions for installing it on your laptop. 12.0.2 Install gpg and git-crypt gpg is the program that implements encryption, and git-crypt sets up git repos for encrypted sharing. These are aleady installed on EHA servers. To install them on a mac, use homebrew. You should also install pinentry. 12.0.3 Create your Keybase keys If you are just starting to use keybase, you can generate new keys for use on your computer using this guide. The guide also helps set up using your key to sign GitHub commits, which you should do for added security. Do create a password associated with your keys when asked. You can store this in a service such as LastPass if desired. At this point you can go to your Keybase account and verify your keys via as many other services, devices, or online identities as you want. We suggest at least three. 12.0.4 Import your keys to your local keychain If you have a Keybase account set and keys already generated, you can now import your Keybase keys to use. Instruction are found here. Do set your keys to maximum trust level. 12.0.5 Also import your keys to the EHA server You will probably want the ability to decrypt files when working remotely, so place them on the EHA server. You can do this by copying over your whole gpg keys directory to the server, like so: scp -rp ~/.gnupg url.of.server: Note that the main EHA analysis servers share user file systems so you only have to do this on one of them. You may have to edit the file ~/.gnupg/gpg-agent.conf on the server. If its first line is pinentry-program /usr/local/bin/pinentry-mac, change it to pinentry-program /usr/bin/pinentry. Note that it may not exist at all, which is fine - it means the program is just using default behavior. 12.0.6 Use git-crypt The git-crypt README outlines the basics of using git-crypt to add encrypted files to a git repository. If you are using an encrypted repository someone else set up, they should have added your public key to the repo, and all you need to do after pulling/cloning is run git-crypt unlock in the repository folder. After that encryption and decryption for pushing and pulling should happen automatically. One key area missing is how to import the public keys of your team members to your local keychain. Visit their Keybase profiles (e.g., https://keybase.io/noamross) and click on the key - it will show several ways to import the keys. Another common source of errors is that the text entry for gpg isn’t set properly. You can fix this by setting the GPG_TTY environment variable like so: export GPG_TTY=$(tty) Add this to your .profile, .bashrc, .zshrc or other settigns file. 12.0.7 Extra: Use a symmetric key for automated processes If you are using continuous integration on a repository with encrypted files, you’ll need to provide a way for the CI system to unlock them. An easy, but not most secure way is to provide a symmetric key. You can generate this by running this in your project directory. git-crypt export-key git_crypt_key.key git_crypt_key.key can now be used to decrypt the repository, and you can provide it to the CI system as an environment variable. However, since it is binary data, you’ll need to convert it to base64 first. So run something like: cat git_crypt_key.key | base64 | pbcopy to convert this file to base64 data, then paste it in your CI system’s environment variable field as something like GIT_CRYPT_KEY64. To use the key later, you’ll need (1) git-crypt and gpg installed in the CI system image, and (2) to run these commands after the CI clones your repository: echo $GIT_ENCRYPT_KEY64 &gt; git_crypt_key.key64 &amp;&amp; base64 -d git_crypt_key.key64 &gt; git_crypt_key.key &amp;&amp; git-crypt unlock git_crypt_key.key "],["13-dependencies.html", "13 Dependency Management", " 13 Dependency Management How do I make sure that all my software and configurations needed for a project are portable? Packrat or checkpoint to fix R package versions. Docker for everything A lesson in user Docker for an R project Makefiles can automate a complex, multipart project. Here’s a lesson on them from Software Carpentry R packages can be a useful project output. We have some in-house R packages to provide access to internal data and generate reports, and may be developing more for external audiences. Hadley Wickham’s R Packages Book provides guidance for these, and we expect our packages to be up to rOpenSci standards. "],["14-statistical-methods.html", "14 Statistical Methods 14.1 Multivariate Regression 14.2 Networks 14.3 Bioinformatics and Sequence Analysis 14.4 Species Distribution Modeling 14.5 Epidemic Simulation and Fitting 14.6 Phylogenetics", " 14 Statistical Methods Statistical methods are a far larger subject than can be covered in this handbook. Our team uses a wide variety of methods that differ by project and change via advances in the field. Nonetheless, here are some methods that we favor when they are appropriate. 14.1 Multivariate Regression We like Generalized Linear Models. Many of us approach these from a Bayesian perspective, and build and fit these models using Stan. A great introduction to this type of modeling is found in Statistical Rethinking, by Richard McElreath which we have a couple of copies of. Richard also has accompanying lectures for this book online as YouTube videos. For nonlinear models, we like Generalized Additive Models, for which we use the mgcv R package. Noam has a bunch of materials on this, including slides and exercises from a workshop he co-teaches, and the seminal text is Generalized Additive Models in R, by Simon Wood. (Example - Host-Pathogen Phylogeny Project: Paper, GitHub) When a multivariate analysis is primarily about prediction, and less about variable inference, machine-learning methods such as boosted regression trees are useful. We make use of these through the dismo or xgboost packages. (Example - Hotspots 2: Paper, GitHub) 14.2 Networks We like networks for both descriptive visualizations and quantitative analyses. R is full of packages to achieve both of these goals, check out ggnetwork and bipartite. We assess individual components within networks looking for high-impact individuals due to their high degree, centrality, or ability to bridge different communities. For a tutorial on basic metrics check out Randi Griffin’s tutorial on primate social networks which utilizes the igraph package. Often, our networks are bipartite, meaning the networks shows interactions between two type of nodes. For us, these are usually viruses and hosts. You can look at example networks in two PREDICT publications: Figure 4 in Anthony et al. 2017 and Figure 3 in Willoughby et al. 2017. To assess overall network structure and identify communities, we measure the modularity. The modularity of a network model is a type of network partitioning into subgroups or modules. This is an alternative to clustering alogrithms. For bipartite networks, we like the Barber’s modularity (Q), calculated through the lpbrim package. Alternatives modularity algorithms include NetCarto which uses a simulated annealing (SA) algorithm or using a map equation (ME) algorithm. 14.3 Bioinformatics and Sequence Analysis Bioconductor is a comprehensive ecosystem of R packages focused on sequence analyses. Typing your general topic of interest into their searchable software table should at least provide an introduction to relevant software. In general, best practices in bioinformatics are changing rapidly, so it is difficult to recommend particular procedures. However, the journal Nature Protocols covers cutting-edge methods, ranging from the lab to the laptop, in great depth. Many articles include step-by-step instructions to complete a given analysis. For a scattershot introduction to high-quality bioinformatics software and relevant applications, it might be worth checking out the Bedford, Pachter, and Patro lab websites. Note that many contemporary bioinformatics tools are accessed through the command line rather than R. For a general overview of RNA sequencing analysis (other bioinformatics pipelines have strong similarities), the Simple Fool’s Guide from the Palumbi lab is a great learning resource. 14.4 Species Distribution Modeling 14.5 Epidemic Simulation and Fitting 14.6 Phylogenetics "],["15-data-sources.html", "15 Data Sources", " 15 Data Sources Here are some key data sources we have built and use across many projects: awesome-parasite is our curated list of sources for host/parasite/pathogen associations and associated taxonomies. HP3Extended in an internal EHA database of host-virus associations. It is hosted on AirTable and requires an EHA login. lemis and cites are R packages that host data on wildlife trade. EIDR is our emerging infectious disease repository and hosts data on first emergence effects of disease. FLIRT, the Flight Risk Tracker, contains information on airline travel patterns as well as simulations of human travel using those patterns. EIDITH, the Emerging Infectious Diseases Technology Hub, is the shared database for the PREDICT project. The eidith R package is how we interface with it for analysis. "],["16-references.html", "16 References 16.1 Converting Paperpile Citations in Google Docs to Endnote Citations in MS Word", " 16 References Paperpile is an excellent tool for sharing and keeping track of research papers, and it is fully integrated into Google Docs – which means you can easily add citations on shared documents. Many teams and projects use Paperpile to organize and share references. If you need a paperpile account, request a key from Megan Walsh. Be sure to associate the Paperpile with your EHA Google account, not a personal one. Many people in the office use EndNote which can be offline on your desktop or online. A nice feature of Endnote is the traveling libraries export from word documents. You will need a license for this, so contact Megan Walsh. 16.1 Converting Paperpile Citations in Google Docs to Endnote Citations in MS Word There are some situations where you may need to convert a Google Doc with Paperpile citations into a Microsoft Word Document with Endnote citations. Luckily, Paperpile has a detailed guide to walk you through the process. But first, you must properly install the Paperpile Google Docs Add-on! Note: You may already have the Google Chrome Paperpile Extension installed and be using it to create citations in your Google Doc, but you need the Google Docs Add-on for proper exporting. To install this addon, follow the links from Paperpile’s website dedicated to Google Docs. Now, when you open a document in Google Docs, you should see both menus for Addons and Paperpile. In order to begin Step 1 of Paperpile’s Endnote Guide, you need to access the add-on sidebar: Go to Addons \\(\\rightarrow\\) Paperpile \\(\\rightarrow\\) Manage Citations and the Paperpile side-bar will appear Don’t try to export or mess around with the Paperpile menu option (or P symbol) – they won’t help you! Once you have the add-on sidebar open, you can follow Paperpile’s detailed guide. Note: depending on your operating system and your edition of Microsoft Word, some of the Word menus may appear slightly different than the ones depicted in Paperpile’s guide. However, the Paperpile images included in the guide should look exactly the same. "],["17-training-resources-and-plans.html", "17 Training Resources and Plans 17.1 Training plan components 17.2 Training Plans", " 17 Training Resources and Plans This section under revision as we consider alternate resources. If you are planning on spending significant time improving your data science and modeling skills, you will want to create a training plan. A training plan is required if you would like to use EHA’s Data Camp account. Data Camp provides excellent modular courses comprised of both video and interactive, automatically graded exercises to learn data science and coding. 17.1 Training plan components A description of your goals for the training plans and to which EHA projects and activities you will apply your skills A list of courses/tutorials your plan to complete The total time the courses will take to complete. We find the estimated times of Data Camp courses are roughly accurrate. The time frame over which you expect to complete them The name of a peer learner. You should have a peer learner at EcoHealth who will be a partner over the course of your training. This may be someone working on a similar training plan or someone with knowledge of the material already. They should play some or all of these roles: Accountability: Your peer learner should know about your training plan and its time frame, and check in on how you are doing. Co-learning: Your learning peer and you may want to schedule times to watch course videos and complete exercises together Review: Especially for materials without automating like Data Camp, your peer learner should be able to look at your work and provide feedback Motivation: Your peer learner should make your training fun and tell you that you rock. When your supervisor signs off on your training plan, contact Megan Walsh to provide you with a Data Camp subscription for the period you need it. 17.2 Training Plans These are some suggestions for assembling resources and courses into training plans. This is of course a small fraction of the many learning and teaching resources available, including many more Data Camp courses. Consult your peers, supervisor, and the #data-sci-discuss Slack channel to find courses or resources on the topics you require. If you use a new resource or course, please add to to this page so others can learn from your impressions of it! 17.2.1 Better Managing Data DataCamp: Introduction to the Tidyverse (4 hours) will get you started immediately working with data in R, and puts the following items in context. It’s a good introduction for everyone with little to no experience. If you have no experience, including no EHA workshop, take Introduction to R, as well (4 hours, though there will be some redundancy). If you are mostly working in spreadhseets but collaborating with R or Python users, or just trying to organize a lot of spreadsheet data for your projects, work through the Data Carpentry lesson on spreadsheet organization (~2 hours) and read Hadley Wickham’s paper on tidy data (~1 hour) Both DataCamp: Importing Data in R (Part 1), (3 hours) and DataCamp: Cleaning Data in R (4 hours) will be useful if you are doing a database aggregation or building project. 17.2.2 Version Control and Git Read through and work through the examples in Happy Git with R. There is a Data Camp course Introduction to Git for Data Science (4 hours). This is a more detailed introduction that teaches you more about how git works and features we use less in our workflow. It may be appropriate once you have jumped into a project that uses git more elaborately. 17.2.3 Reproducible reporting Reporting with R Markdown (3 hours) covers making the reproducible reports some teams like for rapid iteration. Take this if you are writing an Emerging Disease Insights report, and if you team is using R Markdown for communication. 17.2.4 Improving Your Statistical Fundamentals DataCamp: Introduction to Data (4 hours) is a good starter course you have not much or any training in statistics in school and want to get familiar with the language and concepts. DataCamp: Foundations of Probability in R (4 hours) covers many of the important concepts that those doing Bayesian model work at EHA discuss. Take this if you are just getting started interacting with or reading this work and you don’t have a probability background. Statistical Rethinking: A Bayesian Course. This is an excellent book and video lecture series that gives builds great foundations for doing many types of modeling. Prerequisites are Intermediate R, some experience with linear regression and probability (for which the DataCamp course above is a good basis). This course has about 19 hours of video. We recommend 2-3 months for going through the book, video, and exercises. 17.2.5 Improving your data visualization Data Visualization with ggplot2 (Parts 1 + 2) (5 + 5 hours). These will take you much further than the basic visualization you learn in Intro to the Tidyverse. Take them if you aren’t able to make the plots you need based on the knowledge you learn in that. Part 3 covers topics not generally needed. Fundamentals of Data Visualization by Claus Wilke is an excellent guide to making high-quality figures, focusing more on design than mechanics of programming. R code is available for all of its examples. If you feel you have a solid grasp of ggplot2 but want to improve the quality of your figures, we recommend reading this e-book, and using the accompanying code in its GitHub repository to reproduce figures. 17.2.6 R Programming DataCamp: Introduction to R is a good place to back up a bit to fill in concepts if you are missing things to help understand other course sequences. DataCamp: Intermediate R (6 hours) or DataCamp: Writing Functions in R/ (4 hours) are both good courses to improve your skills if you find yourself managing a more complex workflow or working on an R package with someone at EHA. 17.2.7 Map-making and geospatial analysis in R Geocomputation in R is a comprehensive guide for understanding geographic data, mapping, and conducting spatial analysis in R. Likely, the most relevant chapters for your purposes are 1-8, 10-11. A chapter might take you 1-3 hours to work through, depending on how in depth you want to get and the number of exercises that you complete. DataCamp also has courses on spatial analysis using R, notably Spatial Analysis in R with sf and raster (4 hours). It covers some of the material from Geomputation in R in less depth, and gives you fewer tools for hooking into the multiple geospatial data systems available in R. Take this if you are more time-limited and are going to do less in terms of spatial statistics. Data Carpentry has a course on using R for spatial data. Like other *Carpentry lessons its designed as a workshop lesson plan but can be self-taught. It presumes very little R knowledge at all, and includes stuff like setting a project in RStudio. This is a good place to start people or students with little R experience to get them making maps right away. If you just want to get a quick feel for R spatial data types, jump into Chapter 3. Making Maps with R is a quick-start guide to mapping with ggplot2. It also introduces the gmap, maps, and mapdata packages for providing basemaps on which to overlay your spatial data. It is good for getting a map together quickly but if you are going to be doing things on a regular basis we suggest the resources above, which give you a better foundation on geographic data. Leaflet for R is a manual on the use of the R leaflet package to harness Leaflet, an open-source JS library for creating interactive maps. Leaflet maps particularly useful for exploring and visualizing spatial data, and are easily embedded into R Markdown documents. You should take a course or have knowledge of R Markdown prior to taking this course. "],["18-help.html", "18 Getting Help 18.1 Minimal reproducible examples - helping others help you", " 18 Getting Help How do I solve this problem? How do I get my skills up to snuff? We have an #data-sci-discuss channel on Slack to ask questions and also news about useful resources and packages. We prefer that you ask question on this channel rather than privately. This way you draw on the group’s knowledge and everyone can learn from the conversation. In general, if you spend 20 minutes banging your head against your screen trying to figure something out, it’s time to ask someone. Some good questions for the Slack room: Which package should I use for something? Anyone have a good reference or tutorial for package, method? What does this error mean? Our technology team are a tremendous resource for a number of computing topics (especially web technologies and development operations), but remember that they are our collaborators, not IT support. (We do have straight IT support, mostly for office network issues, through Profound Cloud) Also, outside EHA: There’s an almost-monthly NYC R Meetup and even rarer Data Visualization Meetup that EHA members sometimes attend. There’s also an R-Ladies NYC chapter that has regular meetups and a Slack Chat room. WiMLDS offers support to attend machine learning and data science conferences. Stack Overflow is a popular Q&amp;A site for computer programming that a lot of discussions about R. R-Weekly publishes a useful weekly newsletter on new R developments, packages, and publications. The #rstats hashtag on Twitter is a good place for news and short questions, and general ranting. If there’s a course, workshop, or conference you want to attend to improve these skills, speak with your supervisor, we can often support this. DataCamp courses are also potentially useful. If you feel they would match your learning style and needs, discuss EHA purchasing a subscription for you with your supervisor. 18.1 Minimal reproducible examples - helping others help you Minimal reproducible examples are small, self-contained code and data packets that will allow others to recreate the issue you are experiencing on their machine. The reprex package is really helpful for creating preformatted code for github, stackoverflow, or slack. This stackoverflow answer provides a succinct walk through of how to create a minimal reproducible example. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
